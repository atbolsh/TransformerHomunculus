{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcd4aa-7533-4182-9e01-962a2ffc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the brain, using text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95177782-9045-4f95-841c-f6080981f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928dd60-7533-4516-b1a1-55ad1eefcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71488f80-d08b-4f75-9830-8ad80dc11d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # -- penguins.farm version\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # -- penguins.army version\n",
    "\n",
    "vocab_size = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f54e6b25-8b3e-454e-ac90-a5dc8b9b1b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fd6afc-496a-4c84-9844-be2e3cbde658",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DefaultAgentBrain().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d4196e-9aa9-486b-9468-b11670884524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 2, (4,))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1356acfc-f7c0-4326-bf90-ba3a91d1b89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 3, 2, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = torch.LongTensor([[0, 1, 2, 3, 4], [0, 1, 3, 2, 5]])\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cc74fd-f3a7-43f0-8abf-35b44f364bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 3, 2, 5],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd8564a-82ea-44ab-91e6-329f789250d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 3, 2, 5]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = vals.to(device)\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6246eca-7944-4d4e-bd5b-583f82d4a2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 3, 2, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vals.to('cpu')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97c3ea4-ae7c-478e-b7dc-2d4ca296381c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 3, 2, 5]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1773ac06-3515-41a6-8abf-9449265107d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5],\n",
       "        [0, 1, 3, 2, 5]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(batchsize, device = device):\n",
    "    v = vals.to(device)\n",
    "    inds = torch.randint(0, 2, (batchsize,), device=device)\n",
    "    return v[inds]\n",
    "\n",
    "get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05667868-c9d4-427c-a582-cf3263b95cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def get_loss(res, inputs):\n",
    "    return torch.sum(criterion(res[:, :, :-1], inputs[:, 1:]))\n",
    "\n",
    "optimizer = optim.Adam(brain.parameters(), lr=0.0001*256/256, eps=1e-9)#, #betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ff7ab3a-d550-4e2d-a33c-c60267941926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 100: 0.17730373039841651\n",
      "Average Training Loss at batch 200: 0.1762099702656269\n",
      "Average Training Loss at batch 300: 0.17520385324954987\n",
      "Average Training Loss at batch 400: 0.17463001534342765\n",
      "Average Training Loss at batch 500: 0.1745309340953827\n",
      "Average Training Loss at batch 600: 0.1748275390267372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m res \u001b[38;5;241m=\u001b[39m brain\u001b[38;5;241m.\u001b[39mget_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss(res, inputs)\n\u001b[0;32m---> 18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-florence/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-florence/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-florence/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "batches = 30000\n",
    "batchsize = 256\n",
    "\n",
    "train_loss = 0\n",
    "\n",
    "for batch in range(batches):\n",
    "    brain.train()\n",
    "    inputs = get_batch(batchsize)\n",
    "    src_attention_mask, src_key_padding_mask = brain.get_masks(inputs, use_masks=True)\n",
    "    text_encoding = brain.get_text_encoding(inputs, src_attention_mask, src_key_padding_mask)\n",
    "    img_context = torch.randn((batchsize, 256, 768), device=inputs.device) # easier for pretraining\n",
    "    res = brain.get_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full=True)\n",
    "    loss = get_loss(res, inputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss += loss.item()\n",
    "    if batch % 100 == 99:\n",
    "        avg_loss = train_loss / 100\n",
    "        train_loss = 0\n",
    "        print(f\"Average Training Loss at batch {batch + 1}: {avg_loss}\")\n",
    "        torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_DEBUG_POSITION.pth')\n",
    "        torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_DEBUG_POSITION.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea61b830-ed6a-4c1f-af57-7cc66c0b79d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 5, 4, 4, 5]], device='cuda:0'),\n",
       " tensor([[-5.8365e-04, -7.1559e-01, -6.7329e-04, -1.9073e-04, -3.6812e-04,\n",
       "          -1.3475e-03, -1.6108e-03, -5.1584e-03, -4.7136e-02, -1.4511e-02,\n",
       "          -9.2657e-02, -9.3384e-03, -4.6642e-02, -9.1591e-03, -2.2130e-02,\n",
       "          -1.7346e-02, -2.5759e-01, -1.1941e-01, -3.5244e+00, -1.2056e-01,\n",
       "          -1.7401e-02, -2.7031e-02, -3.2639e-02, -9.7146e-02, -2.6613e-01,\n",
       "          -2.6722e-02, -1.4102e-01, -1.0009e+00, -2.4345e-02, -1.9376e-01,\n",
       "          -7.1617e-01]], device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[0.0098, 0.6986, 0.0106, 0.0034, 0.0064, 0.0189, 0.0222, 0.0610, 0.3074,\n",
       "          0.1642, 0.4694, 0.0974, 0.3087, 0.0885, 0.1943, 0.2058, 0.8567, 0.6369,\n",
       "          0.3949, 0.6102, 0.1684, 0.2205, 0.3073, 0.5889, 1.0236, 0.2639, 0.7292,\n",
       "          1.4098, 0.2774, 1.1117, 1.3544]], device='cuda:0',\n",
       "        grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eddb043-17e9-4f01-be9b-73dfb414204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = vals[:1, :-1]\n",
    "seed2 = vals[1:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4affbe1-f4ad-4b43-b20a-b20659eb3ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6282d3c4-fc50-47ea-992f-a3323effbaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 3, 2]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3614245d-c6d8-4c72-a5f1-1050f6294113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4]], device='cuda:0'),\n",
       " tensor([-0.0004], device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor([0.0062], device='cuda:0', grad_fn=<NegBackward0>),\n",
       " tensor([False], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.extend(seed1, is_terminated = torch.tensor([False]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9914d2bd-59b1-48cf-9882-120602fbae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so it did learn to memorize these two things.\n",
    "# It knows about position; it correctly uses positional encoding. Good.\n",
    "# Done here. Successful test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3107205-59cd-411d-82c6-f75c37fc1c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
