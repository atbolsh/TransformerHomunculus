{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcd4aa-7533-4182-9e01-962a2ffc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the brain, using text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95177782-9045-4f95-841c-f6080981f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928dd60-7533-4516-b1a1-55ad1eefcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71488f80-d08b-4f75-9830-8ad80dc11d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # -- penguins.farm version\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # -- penguins.army version\n",
    "paths = [\"text_pretraining_data/eng_sentences_pruned-train.txt\"]\n",
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "## Dataset\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, seq_length = 32, evaluate: bool = False, tokenizer=None, device = None):\n",
    "        if device is None:\n",
    "            device = 'cpu'\n",
    "        self.device = device\n",
    "        self.seq_length = seq_length\n",
    "        if tokenizer is None:\n",
    "            tokenizer = ByteLevelBPETokenizer(\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-vocab.json\",\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-merges.txt\",\n",
    "            )   \n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )   \n",
    "        tokenizer.enable_truncation(max_length=self.seq_length)\n",
    "        tokenizer.enable_padding()#length=seq_length)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./text_pretraining_data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./text_pretraining_data/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i]).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fd6afc-496a-4c84-9844-be2e3cbde658",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DefaultAgentBrain().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773ac06-3515-41a6-8abf-9449265107d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "🔥 text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    }
   ],
   "source": [
    "sdt = SampleDataset(tokenizer=tokenizer)\n",
    "sdv = SampleDataset(tokenizer=tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206c4e99-fb84-433d-84c7-99f3625cb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 80 #32*4*2#8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(sdt, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05667868-c9d4-427c-a582-cf3263b95cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def get_loss(res, inputs):\n",
    "    return torch.sum(criterion(res[:, :, :-1], inputs[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7ab3a-d550-4e2d-a33c-c60267941926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   0%|                                                                                                                                                                     | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 9.316092491149902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   5%|███████▌                                                                                                                                                | 1000/20000 [04:29<1:25:08,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 4.725765901607471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  10%|███████████████▏                                                                                                                                        | 2000/20000 [09:00<1:20:48,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 4.40001886811988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  15%|██████████████████████▊                                                                                                                                 | 3000/20000 [13:31<1:16:24,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 4.2339056343763755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [18:02<1:11:56,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 4.127308309748601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  25%|██████████████████████████████████████                                                                                                                  | 5000/20000 [22:33<1:07:24,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 4.04621469686089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  30%|█████████████████████████████████████████████▌                                                                                                          | 6000/20000 [27:04<1:02:53,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 3.9802095581741854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  35%|█████████████████████████████████████████████████████▉                                                                                                    | 7000/20000 [31:35<58:19,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 3.9264698808422396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  40%|█████████████████████████████████████████████████████████████▌                                                                                            | 8000/20000 [36:06<53:51,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 3.881814607693663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  45%|█████████████████████████████████████████████████████████████████████▎                                                                                    | 9000/20000 [40:37<49:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 3.841321705262034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 10000/20000 [45:08<44:52,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 3.8063207524691256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  55%|████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 11000/20000 [49:40<40:31,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 3.77447204205808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  60%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 12000/20000 [54:12<35:56,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 3.7459280223471354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  65%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 13000/20000 [58:43<31:28,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 3.7190133005772616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 14000/20000 [1:03:14<27:02,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 3.6944150092661205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 15000/20000 [1:07:45<22:31,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 3.6709845734233815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 16000/20000 [1:12:17<17:58,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 3.6496240171847676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 17000/20000 [1:16:48<13:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 3.6288734807749927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 18000/20000 [1:21:19<08:59,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 3.6098645449307196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 19000/20000 [1:25:50<04:29,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 3.5923654388411475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:30:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 0: 3.5751865940213206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   0%|                                                                                                                                                                     | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 3.109647750854492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   5%|███████▌                                                                                                                                                | 1000/20000 [04:30<1:25:26,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 3.2007551610053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  10%|███████████████▏                                                                                                                                        | 2000/20000 [09:02<1:21:09,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 3.193572289225222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  15%|██████████████████████▊                                                                                                                                 | 3000/20000 [13:33<1:16:32,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 3.1835153334063713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [18:04<1:12:06,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 3.177908931961956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  25%|██████████████████████████████████████                                                                                                                  | 5000/20000 [22:35<1:07:19,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 3.172420911516244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  30%|█████████████████████████████████████████████▌                                                                                                          | 6000/20000 [27:07<1:03:06,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 3.166583309569293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  35%|█████████████████████████████████████████████████████▉                                                                                                    | 7000/20000 [31:38<58:35,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 3.1612249050050476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  40%|█████████████████████████████████████████████████████████████▌                                                                                            | 8000/20000 [36:09<53:51,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 3.155896200267781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  45%|█████████████████████████████████████████████████████████████████████▎                                                                                    | 9000/20000 [40:40<49:28,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 3.1501564208487247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 10000/20000 [45:11<44:56,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 3.145067584143914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  55%|████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 11000/20000 [49:43<40:25,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 3.1398323572892384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  60%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 12000/20000 [54:14<36:03,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 3.1342914981570265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  65%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 13000/20000 [58:45<31:29,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 3.129219017160112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 14000/20000 [1:03:16<27:02,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 3.1235915145808635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 15000/20000 [1:07:48<22:27,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 3.119052573908377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 16000/20000 [1:12:20<17:59,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 3.114170181550843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 17000/20000 [1:16:51<13:27,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 3.1096745298144524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 18000/20000 [1:21:22<08:58,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 3.1053871576948024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 19000/20000 [1:25:53<04:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 3.101329814044496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:30:25<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 1: 3.0971908952951432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   0%|                                                                                                                                                                     | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.7698516845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   5%|███████▌                                                                                                                                                | 1000/20000 [04:31<1:25:41,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.970735984367805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  10%|███████████████▏                                                                                                                                        | 2000/20000 [09:02<1:20:46,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.9645126723814226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  15%|██████████████████████▊                                                                                                                                 | 3000/20000 [13:33<1:16:31,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.9605469156288775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [18:05<1:12:05,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.959237880034615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  20%|██████████████████████████████▍                                                                                                                         | 4002/20000 [18:11<7:35:46,  1.71s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(brain.parameters(), lr=0.0001*80/256, eps=1e-9)#, #betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    brain.train()\n",
    "    train_loss = 0\n",
    "    i = -1\n",
    "    for inputs in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        i += 1\n",
    "        inputs = inputs.to(device)\n",
    "        img_context = torch.randn((batch_size, 256, 768), device=inputs.device) # easier for pretraining\n",
    "        src_attention_mask, src_key_padding_mask = brain.get_masks(inputs, use_masks=True)\n",
    "        text_encoding = brain.get_text_encoding(inputs, src_attention_mask, src_key_padding_mask)\n",
    "        res = brain.get_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full=True)\n",
    "        loss = get_loss(res, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            avg_train_loss = train_loss / (i + 1)\n",
    "            print(f\"Average Training Loss at batch {i + 1}: {avg_train_loss}\")\n",
    "            torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v3.pth')\n",
    "            torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v3.pth')\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss after epoch {epoch}: {avg_train_loss}\")\n",
    "    torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v3.pth')\n",
    "    torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b830-ed6a-4c1f-af57-7cc66c0b79d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
