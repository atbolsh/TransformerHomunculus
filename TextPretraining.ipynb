{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcd4aa-7533-4182-9e01-962a2ffc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the brain, using text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95177782-9045-4f95-841c-f6080981f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928dd60-7533-4516-b1a1-55ad1eefcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71488f80-d08b-4f75-9830-8ad80dc11d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # -- penguins.farm version\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # -- penguins.army version\n",
    "paths = [\"text_pretraining_data/eng_sentences_pruned-train.txt\"]\n",
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "## Dataset\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, seq_length = 32, evaluate: bool = False, tokenizer=None, device = None):\n",
    "        if device is None:\n",
    "            device = 'cpu'\n",
    "        self.device = device\n",
    "        self.seq_length = seq_length\n",
    "        if tokenizer is None:\n",
    "            tokenizer = ByteLevelBPETokenizer(\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-vocab.json\",\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-merges.txt\",\n",
    "            )   \n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )   \n",
    "        tokenizer.enable_truncation(max_length=self.seq_length)\n",
    "        tokenizer.enable_padding()#length=seq_length)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./text_pretraining_data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./text_pretraining_data/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i]).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fd6afc-496a-4c84-9844-be2e3cbde658",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DefaultAgentBrain().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773ac06-3515-41a6-8abf-9449265107d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "🔥 text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    }
   ],
   "source": [
    "sdt = SampleDataset(tokenizer=tokenizer)\n",
    "sdv = SampleDataset(tokenizer=tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206c4e99-fb84-433d-84c7-99f3625cb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 80 #32*4*2#8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(sdt, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05667868-c9d4-427c-a582-cf3263b95cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def get_loss(res, inputs):\n",
    "    return torch.sum(criterion(res[:, :, :-1], inputs[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7ab3a-d550-4e2d-a33c-c60267941926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.57485294342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:40<3:39:56,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.7464449901085395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:18<3:26:38,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.7476680111968474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:55<3:15:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.7470537762926326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:30<3:04:01,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.7457614956245338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [58:07<2:52:07,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.745810607413582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:42<2:41:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.7444192146265673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:21:18<2:29:24,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.7441621956323288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:54<2:17:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.7442622507174246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:44:30<2:06:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.743551623329906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:56:05<1:54:34,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.7431635051092593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:07:40<1:43:26,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.7430106039188544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:19:16<1:31:45,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.7434393701210844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:51<1:20:24,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.7433261110474354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:42:27<1:08:57,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.743612541470236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:54:03<57:28,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.743344100791052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:05:39<45:41,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.7431497149791997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:17:11<34:19,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.742952244890822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:28:44<22:53,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.7426422437650153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:40:16<11:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.7423734431530034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:51:49<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 0: 2.7420826733112333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.7697505950927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:33<3:36:48,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.6822283818171573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:05<3:26:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.682164951540839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:37<3:14:34,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.6854271879199345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:10<3:03:01,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.6843621767750085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:42<2:51:34,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.6865466605947153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:15<2:39:56,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.6873280816109175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:46<2:28:50,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.6875176108951075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:20<2:16:42,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.6889327716386373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:52<2:05:39,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.689944128051968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:24<1:54:14,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.6913956908294767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:06:57<1:43:05,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.6917087791421372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:30<1:31:11,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.692064023556267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:03<1:19:55,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.6921591690142113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:41:35<1:08:40,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.692319878424315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:07<57:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.6931721077991417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:04:40<45:48,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.6944201273006554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:16:12<34:09,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.694626669877838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:27:44<22:50,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.695031713733447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:39:17<11:24,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.695275155677713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:50:49<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 1: 2.6958542519330977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.678349494934082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:33<3:37:41,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.63840402399267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:05<3:25:42,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.6429334325471086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:38<3:14:27,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.646729375632037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:10<3:02:48,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.6474673706303533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:43<2:51:38,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.6490506531357454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:15<2:40:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.650761636251848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:47<2:28:35,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.6526091728461094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:19<2:17:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.653184524909688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:53<2:06:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.6548820276973433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:25<1:54:20,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.655866386389544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:07:06<1:43:43,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.6562396255951235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  57%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                      | 11302/20000 [2:10:41<1:40:22,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(brain.parameters(), lr=0.0001*80/256, eps=1e-9)#, #betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    brain.train()\n",
    "    train_loss = 0\n",
    "    i = -1\n",
    "    for inputs in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        i += 1\n",
    "        inputs = inputs.to(device)\n",
    "        img_context = torch.randn((batch_size, 256, 768), device=inputs.device) # easier for pretraining\n",
    "        src_attention_mask, src_key_padding_mask = brain.get_masks(inputs, use_masks=True)\n",
    "        text_encoding = brain.get_text_encoding(inputs, src_attention_mask, src_key_padding_mask)\n",
    "        res = brain.get_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full=True)\n",
    "        loss = get_loss(res, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            avg_train_loss = train_loss / (i + 1)\n",
    "            print(f\"Average Training Loss at batch {i + 1}: {avg_train_loss}\")\n",
    "            torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v5.pth')\n",
    "            torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v5.pth')\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss after epoch {epoch}: {avg_train_loss}\")\n",
    "    torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v5.pth')\n",
    "    torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea61b830-ed6a-4c1f-af57-7cc66c0b79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4 final score: 2.73\n",
    "# Relaunching v5 on top of v4, training further, but keeping v4 around as overfitting insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaf33f-3a95-4675-a619-8528b2016c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
