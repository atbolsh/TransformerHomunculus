{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcd4aa-7533-4182-9e01-962a2ffc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the brain, using text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95177782-9045-4f95-841c-f6080981f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928dd60-7533-4516-b1a1-55ad1eefcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71488f80-d08b-4f75-9830-8ad80dc11d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # -- penguins.farm version\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # -- penguins.army version\n",
    "paths = [\"text_pretraining_data/eng_sentences_pruned-train.txt\"]\n",
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "## Dataset\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, seq_length = 32, evaluate: bool = False, tokenizer=None, device = None):\n",
    "        if device is None:\n",
    "            device = 'cpu'\n",
    "        self.device = device\n",
    "        self.seq_length = seq_length\n",
    "        if tokenizer is None:\n",
    "            tokenizer = ByteLevelBPETokenizer(\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-vocab.json\",\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-merges.txt\",\n",
    "            )   \n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )   \n",
    "        tokenizer.enable_truncation(max_length=self.seq_length)\n",
    "        tokenizer.enable_padding()#length=seq_length)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./text_pretraining_data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./text_pretraining_data/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i]).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fd6afc-496a-4c84-9844-be2e3cbde658",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DefaultAgentBrain().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773ac06-3515-41a6-8abf-9449265107d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "🔥 text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    }
   ],
   "source": [
    "sdt = SampleDataset(tokenizer=tokenizer)\n",
    "sdv = SampleDataset(tokenizer=tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206c4e99-fb84-433d-84c7-99f3625cb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 80 #32*4*2#8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(sdt, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05667868-c9d4-427c-a582-cf3263b95cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def get_loss(res, inputs):\n",
    "    return torch.sum(criterion(res[:, :, :-1], inputs[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04392e9-d7ae-488b-9246-85e6fc781dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v6 continues after 3 epochs of v5, with the lr reduced by a factor of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff7ab3a-d550-4e2d-a33c-c60267941926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.701967239379883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:27<3:37:25,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.613007904170872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [22:59<3:26:06,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.606911308702262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:32<3:14:17,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.602032979779623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:05<3:03:07,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.599142922725835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:38<2:51:20,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.597411099635847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:10<2:39:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.5954671794425406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:43<2:28:36,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.5930177726808266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:16<2:17:11,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.592302918151056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:48<2:05:58,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.59140998871376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:21<1:54:23,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.5908935376136686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:06:54<1:43:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.590346380381484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:26<1:31:32,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.5901007513415464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:29:59<1:20:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.5894788651913463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:41:31<1:08:52,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.5890980420576812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:04<57:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.588980160620314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:04:36<45:38,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.588726804641014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:16:09<34:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.588324567709423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:27:42<22:54,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.5883240175541227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:39:14<11:24,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.5881092676984143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:50:47<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 0: 2.5876196774721145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.8024184703826904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:32<3:37:23,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.575273718152727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:05<3:25:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.5727388286876534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:37<3:14:20,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.5711466053731042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:10<3:03:21,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.5700751090460914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:42<2:51:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.5710708527201724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:14<2:39:52,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.5712164773322845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:47<2:28:25,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.5701297885468137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:20<2:17:23,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.5702223844519856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:52<2:06:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.5700709486839415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:25<1:54:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.5692415669874817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:06:57<1:42:52,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.568906441763004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:30<1:31:36,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.569072826575581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:02<1:20:09,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.569048276049166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:41:34<1:08:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.5686759267923756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:07<57:14,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.5689850064422535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:04:40<45:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.568980874318524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:16:13<34:21,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.5686474015562544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:27:46<22:54,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.5689827130308522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:39:19<11:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.568843936877504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:50:52<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 1: 2.568696899831295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.376535415649414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:33<3:37:15,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.5557827285000614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:05<3:25:38,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.5569706230030125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:38<3:14:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.5555499184731443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:11<3:03:04,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.5569423492834944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:43<2:51:27,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.5561086689560586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:15<2:39:40,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.556548072862299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:49<2:28:45,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.556695968010582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:21<2:17:07,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.5565424230065648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:54<2:05:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.5570258947759585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:27<1:54:22,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.5567862576764173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:07:00<1:43:10,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.5573247625290096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:32<1:31:48,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.5568879217259557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:05<1:20:13,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.5570505411017352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:41:38<1:08:35,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.5573203283398076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:10<57:13,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.5576663910170474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:04:43<45:51,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.557917996485765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:16:16<34:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.5579987666682773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:27:49<22:51,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.5583126482967007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:39:21<11:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.558553900306372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:50:54<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 2: 2.5588872188210487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.755262613296509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:32<3:37:49,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.5509459107786743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:04<3:26:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.547181782276853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:38<3:14:39,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.550966025511053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:10<3:02:42,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.5508566871639253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:46<2:52:09,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.549958018726455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:24<2:40:42,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.549728758870274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:21:01<2:29:29,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.5498429751195255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:38<2:18:03,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.5506841321093425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:44:15<2:06:10,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.5503467638324917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:52<1:55:09,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.5501139854123718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:07:29<1:43:39,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.5501438118026383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:19:06<1:31:59,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.5502749030743628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:43<1:20:42,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.5508008794173507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:42:21<1:09:05,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.5509208355552087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:59<57:31,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.55116530806452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:05:37<46:10,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.550994971462894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:17:14<34:30,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.5508447706780006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:28:49<22:56,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.550999877241067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:40:25<11:29,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.5513141645481006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:52:01<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 3: 2.5516063534498215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.660503625869751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:35<3:38:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.542998387739732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:11<3:27:11,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.540142658053488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:47<3:15:28,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.54031365611322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:22<3:03:54,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.5410137885631667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:59<2:52:19,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.5418408805192696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:34<2:40:59,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.5422746293287717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:21:12<2:28:29,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.543364351910364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:44<2:17:39,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.543596663723557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:44:17<2:05:41,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.5429471735291553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:49<1:54:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.543061582556535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:07:21<1:43:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.543276792525725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:53<1:31:30,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.544242361428072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 13000/20000 [2:30:26<1:20:12,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 2.544498895353633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 14000/20000 [2:41:57<1:08:28,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 2.544605754050312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 15000/20000 [2:53:30<57:13,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 2.544829524204688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 16000/20000 [3:05:02<45:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 2.5449748478802388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 17000/20000 [3:16:34<34:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 2.5447558859686072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 18000/20000 [3:28:07<22:53,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 2.5446926678599837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 19000/20000 [3:39:39<11:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 2.5447739589139666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [3:51:12<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 4: 2.5451262454748154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:   0%|                                                                                                                                                                                 | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.5313591957092285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:   5%|████████▏                                                                                                                                                           | 1000/20000 [11:31<3:37:23,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.538586293067132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  10%|████████████████▍                                                                                                                                                   | 2000/20000 [23:03<3:25:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.53388767514093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  15%|████████████████████████▌                                                                                                                                           | 3000/20000 [34:35<3:14:14,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.5357362308012807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  20%|████████████████████████████████▊                                                                                                                                   | 4000/20000 [46:07<3:02:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 2.536664198351276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  25%|█████████████████████████████████████████                                                                                                                           | 5000/20000 [57:39<2:51:10,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 2.5355122947997986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  30%|████████████████████████████████████████████████▌                                                                                                                 | 6000/20000 [1:09:11<2:39:43,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 2.5361875466675863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  35%|████████████████████████████████████████████████████████▋                                                                                                         | 7000/20000 [1:20:42<2:28:17,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 2.5362516426559925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  40%|████████████████████████████████████████████████████████████████▊                                                                                                 | 8000/20000 [1:32:13<2:16:50,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 2.536130606897204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                         | 9000/20000 [1:43:44<2:05:21,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 2.536287611132926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  50%|████████████████████████████████████████████████████████████████████████████████▌                                                                                | 10000/20000 [1:55:16<1:54:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 2.537219134441269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  55%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 11000/20000 [2:06:48<1:42:32,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 2.5376003970775636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  60%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 12000/20000 [2:18:20<1:31:35,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 2.5375584860700697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/16:  64%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                          | 12744/20000 [2:26:59<1:23:41,  1.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(brain.parameters(), lr=0.00001*80/256, eps=1e-9)#, #betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    brain.train()\n",
    "    train_loss = 0\n",
    "    i = -1\n",
    "    for inputs in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        i += 1\n",
    "        inputs = inputs.to(device)\n",
    "        img_context = torch.randn((batch_size, 256, 768), device=inputs.device) # easier for pretraining\n",
    "        src_attention_mask, src_key_padding_mask = brain.get_masks(inputs, use_masks=True)\n",
    "        text_encoding = brain.get_text_encoding(inputs, src_attention_mask, src_key_padding_mask)\n",
    "        res = brain.get_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full=True)\n",
    "        loss = get_loss(res, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            avg_train_loss = train_loss / (i + 1)\n",
    "            print(f\"Average Training Loss at batch {i + 1}: {avg_train_loss}\")\n",
    "            torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v6.pth')\n",
    "            torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v6.pth')\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss after epoch {epoch}: {avg_train_loss}\")\n",
    "    torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v6.pth')\n",
    "    torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea61b830-ed6a-4c1f-af57-7cc66c0b79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v4 final score: 2.73\n",
    "# Relaunching v5 on top of v4, training further, but keeping v4 around as overfitting insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaf33f-3a95-4675-a619-8528b2016c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
