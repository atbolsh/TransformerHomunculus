{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fcd4aa-7533-4182-9e01-962a2ffc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the brain, using text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95177782-9045-4f95-841c-f6080981f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "brain = DefaultAgentBrain().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928dd60-7533-4516-b1a1-55ad1eefcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71488f80-d08b-4f75-9830-8ad80dc11d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "paths = [\"text_pretraining_data/eng_sentences_pruned-train.txt\"]\n",
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "## Dataset\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, seq_length = 32, evaluate: bool = False, tokenizer=None, device = None):\n",
    "        if device is None:\n",
    "            device = 'cpu'\n",
    "        self.device = device\n",
    "        self.seq_length = seq_length\n",
    "        if tokenizer is None:\n",
    "            tokenizer = ByteLevelBPETokenizer(\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-vocab.json\",\n",
    "                \"./text_pretraining_tokenizer/eng_sentences_tokenizer_v2-merges.txt\",\n",
    "            )   \n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )   \n",
    "        tokenizer.enable_truncation(max_length=self.seq_length)\n",
    "        tokenizer.enable_padding()#length=seq_length)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./text_pretraining_data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./text_pretraining_data/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i]).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1773ac06-3515-41a6-8abf-9449265107d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "🔥 text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    }
   ],
   "source": [
    "sdt = SampleDataset(tokenizer=tokenizer)\n",
    "sdv = SampleDataset(tokenizer=tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206c4e99-fb84-433d-84c7-99f3625cb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 80 #32*4*2#8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(sdt, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05667868-c9d4-427c-a582-cf3263b95cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def get_loss(res, inputs):\n",
    "    return torch.sum(criterion(res[:, :, :-1], inputs[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7ab3a-d550-4e2d-a33c-c60267941926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 9.272705078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:   5%|███████▋                                                                                                                                                  | 1000/20000 [16:17<5:09:09,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 4.7953915281610175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  10%|███████████████▍                                                                                                                                          | 2000/20000 [32:41<4:53:04,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 4.463658342714133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  15%|███████████████████████                                                                                                                                   | 3000/20000 [49:04<4:36:14,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 4.296980200748132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [1:05:26<4:19:47,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 4.185086747402848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  25%|██████████████████████████████████████                                                                                                                  | 5000/20000 [1:21:51<4:04:38,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 4.101664095109903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  30%|█████████████████████████████████████████████▌                                                                                                          | 6000/20000 [1:38:18<3:48:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 4.0330391909515555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  35%|█████████████████████████████████████████████████████▏                                                                                                  | 7000/20000 [1:54:47<3:32:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 3.979179816592712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  40%|████████████████████████████████████████████████████████████▊                                                                                           | 8000/20000 [2:11:14<3:16:04,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 3.9335583499812614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  45%|████████████████████████████████████████████████████████████████████▍                                                                                   | 9000/20000 [2:27:39<2:58:17,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 3.8925628928843636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  50%|███████████████████████████████████████████████████████████████████████████▌                                                                           | 10000/20000 [2:44:00<2:42:30,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 3.85664019638533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  55%|███████████████████████████████████████████████████████████████████████████████████                                                                    | 11000/20000 [3:00:23<2:26:03,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 3.8258677660015623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  60%|██████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 12000/20000 [3:16:47<2:10:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 3.7970306248716907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 13000/20000 [3:33:09<1:53:38,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 3.770603746627351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 14000/20000 [3:49:31<1:37:27,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 3.746779536904629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 15000/20000 [4:05:53<1:21:09,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 3.725424042352636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 16000/20000 [4:22:14<1:05:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 3.7051318852232886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 17000/20000 [4:38:36<48:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 3.6865961545215313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 18000/20000 [4:55:00<32:28,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 3.668931382794187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 19000/20000 [5:11:22<16:16,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 3.652422341188188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/16: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [5:27:43<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 0: 3.6364555095791817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 3.248002767562866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:   5%|███████▋                                                                                                                                                  | 1000/20000 [16:22<5:08:54,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 3.289244535086038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  10%|███████████████▍                                                                                                                                          | 2000/20000 [32:45<4:51:41,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 3.2809787302956113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  15%|███████████████████████                                                                                                                                   | 3000/20000 [49:08<4:36:02,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 3.2768868855816726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [1:05:29<4:19:38,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 3.2731534249482825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  25%|██████████████████████████████████████                                                                                                                  | 5000/20000 [1:21:51<4:03:31,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 3.2648995723087437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  30%|█████████████████████████████████████████████▌                                                                                                          | 6000/20000 [1:38:13<3:47:08,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 3.2596631492699926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  35%|█████████████████████████████████████████████████████▏                                                                                                  | 7000/20000 [1:54:35<3:31:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 3.2533026084850865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  40%|████████████████████████████████████████████████████████████▊                                                                                           | 8000/20000 [2:10:58<3:15:09,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 3.24745299574465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  45%|████████████████████████████████████████████████████████████████████▍                                                                                   | 9000/20000 [2:27:19<2:58:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 3.2410369217892434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  50%|███████████████████████████████████████████████████████████████████████████▌                                                                           | 10000/20000 [2:43:41<2:42:16,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 3.236295720742066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  55%|███████████████████████████████████████████████████████████████████████████████████                                                                    | 11000/20000 [3:00:02<2:25:57,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 3.2315737024327538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  60%|██████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 12000/20000 [3:16:25<2:09:58,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 3.226836702643052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 13000/20000 [3:32:47<1:53:29,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 3.2218281782257514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 14000/20000 [3:49:08<1:37:27,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 3.2164572482193874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 15000/20000 [4:05:29<1:21:11,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 3.2108862442967414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 16000/20000 [4:21:51<1:04:53,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 3.206527235373714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 17000/20000 [4:38:14<48:40,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 3.2019728333744872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 18000/20000 [4:54:36<32:25,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 3.1976699054681568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 19000/20000 [5:10:57<16:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 3.1928321555579164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/16: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [5:27:18<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 1: 3.188260354137421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.926788568496704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:   5%|███████▋                                                                                                                                                  | 1000/20000 [16:22<5:08:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 3.058787497130784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  10%|███████████████▍                                                                                                                                          | 2000/20000 [32:45<4:52:26,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 3.0589513995539006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  15%|███████████████████████                                                                                                                                   | 3000/20000 [49:06<4:35:56,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 3.0543040839166333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  20%|██████████████████████████████▍                                                                                                                         | 4000/20000 [1:05:27<4:19:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 4001: 3.054232844827295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  25%|██████████████████████████████████████                                                                                                                  | 5000/20000 [1:21:49<4:03:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 5001: 3.0535841679911546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  30%|█████████████████████████████████████████████▌                                                                                                          | 6000/20000 [1:38:11<3:47:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 6001: 3.0513037437002892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  35%|█████████████████████████████████████████████████████▏                                                                                                  | 7000/20000 [1:54:33<3:30:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 7001: 3.049199909810389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  40%|████████████████████████████████████████████████████████████▊                                                                                           | 8000/20000 [2:10:54<3:14:40,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 8001: 3.046996507357395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  45%|████████████████████████████████████████████████████████████████████▍                                                                                   | 9000/20000 [2:27:15<2:58:02,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 9001: 3.044235893387673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  50%|███████████████████████████████████████████████████████████████████████████▌                                                                           | 10000/20000 [2:43:37<2:42:10,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 10001: 3.0418109289944955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  55%|███████████████████████████████████████████████████████████████████████████████████                                                                    | 11000/20000 [2:59:59<2:26:14,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 11001: 3.0399402837386598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  60%|██████████████████████████████████████████████████████████████████████████████████████████▌                                                            | 12000/20000 [3:16:20<2:09:44,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 12001: 3.038440912428046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 13000/20000 [3:32:40<1:53:40,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 13001: 3.036228129830913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 14000/20000 [3:49:01<1:37:16,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 14001: 3.0340645200022203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 15000/20000 [4:05:23<1:21:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 15001: 3.031592088924647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 16000/20000 [4:21:45<1:04:48,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 16001: 3.0294835937745077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 17000/20000 [4:38:06<48:41,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 17001: 3.027429769000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 18000/20000 [4:54:26<32:26,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 18001: 3.0249233229823473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 19000/20000 [5:10:48<16:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 19001: 3.023165461603111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/16: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [5:27:10<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss after epoch 2: 3.0211149790406227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:   0%|                                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1: 2.8632798194885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:   5%|███████▋                                                                                                                                                  | 1000/20000 [16:20<5:08:37,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 1001: 2.93395313897452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  10%|███████████████▍                                                                                                                                          | 2000/20000 [32:41<4:51:46,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 2001: 2.9381897224300446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  15%|███████████████████████                                                                                                                                   | 3000/20000 [49:03<4:35:54,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss at batch 3001: 2.9379653870284814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/16:  17%|██████████████████████████▋                                                                                                                               | 3468/20000 [56:48<4:34:20,  1.00it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(brain.parameters(), lr=0.0001*80/256, eps=1e-9)#, #betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    brain.train()\n",
    "    train_loss = 0\n",
    "    i = -1\n",
    "    for inputs in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        i += 1\n",
    "        inputs = inputs.to(device)\n",
    "        img_context = torch.randn((batch_size, 256, 768), device=inputs.device) # easier for pretraining\n",
    "        src_attention_mask, src_key_padding_mask = brain.get_masks(inputs, use_masks=True)\n",
    "        text_encoding = brain.get_text_encoding(inputs, src_attention_mask, src_key_padding_mask)\n",
    "        res = brain.get_text_decoding(text_encoding, src_attention_mask, src_key_padding_mask, img_context, return_full=True)\n",
    "        loss = get_loss(res, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            avg_train_loss = train_loss / (i + 1)\n",
    "            print(f\"Average Training Loss at batch {i + 1}: {avg_train_loss}\")\n",
    "            torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v2.pth')\n",
    "            torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v2.pth')\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss after epoch {epoch}: {avg_train_loss}\")\n",
    "    torch.save(brain.text_enc.state_dict(), 'brain_checkpoints/text_encoder_weights_v2.pth')\n",
    "    torch.save(brain.text_dec.state_dict(), 'brain_checkpoints/text_decoder_weights_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b830-ed6a-4c1f-af57-7cc66c0b79d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
