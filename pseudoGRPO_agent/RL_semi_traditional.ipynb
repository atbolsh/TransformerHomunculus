{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bdbd20-2df8-46c3-8500-c5f74ffeaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will only use a 'fake' brain for this one, only results are actions 0 - 4\n",
    "# This means I will start with an untrained text_dec and text_enc (though I will steal the img_enc from the old brain)\n",
    "# If this proves too hard, I will start with an easier task\n",
    "\n",
    "# This notebook will attempt actual PPO as a test study.\n",
    "# If I want this for the actual system, I may use a 'burn-in' phase which includes making the relevant actions have non-negligible probability,\n",
    "# and THEN launching the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04dcd88c-587c-4125-9ef4-c40cb39941b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db869825-5953-4bce-9dfa-e48f004a71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings) # kind of a waste; will only call this object to generate random versions of itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a3dafb-614a-48bf-8c99-e188d28a581d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f00e0424e30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badc0e2d-1831-4d10-8b8f-e785a8701270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051aec13-5b41-47d2-8309-38aa36c66169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "device = torch.device('cuda:1') # doing this on the P40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c144f17f-4ae9-488d-9778-bf15ec9d65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(torch_img):\n",
    "    clean = torch_img.detach().cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb391465-6429-46a9-a79b-f252aba5e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, on to RL-specifics things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ff9d55-3cc1-4838-951a-b2fc26876c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 3: 3, 4: 4, 108: 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9150c21e-fed3-4841-9e2d-44ee984c050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis work for the full brain, but I am rewriting this for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a01b64-bd36-4d75-a3c9-c5c29e39a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 3, 4: 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_action_map = {1: 1, 2: 2, 3: 3, 4: 4}\n",
    "symbol_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca831ae8-3366-414d-8116-2821dd04be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I need to set up the brain itself\n",
    "old_brain = DefaultAgentBrain()\n",
    "old_brain.load_state_dict(torch.load('brain_checkpoints/brain_weights_tutorial1_v3_batch95000.pth', weights_only=True, map_location='cpu'))\n",
    "\n",
    "brain = DefaultAgentBrain(5)\n",
    "brain.img_enc = old_brain.img_enc\n",
    "brain.img_dec = old_brain.img_dec\n",
    "brain = brain.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a400a2b1-1dfb-40e1-aba2-7ed54568b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm starting to regret feeding the traces in at all\n",
    "# I should really just have a single 'prompt' like \"what is the best action here?\"\n",
    "# However, I don't feel like rewriting RL_helper just for this experiment, so this is what we are dealing with.\n",
    "# Hopefully PPO will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1989aec-7459-4a2a-ac5d-862100324d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = GameOutputBuffer(brain, brain.evaluate_text, gamma=0.99, tau=0.97, default_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7077869c-4af8-485d-af11-d421049c61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70c35e56-8081-4ea5-998e-177530ecd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.fill(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b2269d-a6fb-45a8-870c-65504a7b3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30fb665-263d-4f05-8542-d0e5718150df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = buff.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f030f73-f8c3-4574-a432-46a87821b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530b7e7-ef3c-4285-a3ec-ef0ee902d4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25cd4c22-6024-4a73-9423-c93fef18cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_logic_solver import * # convenient way to get fake, logical traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72b70cfc-6b89-4846-af74-dfb4bbabadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_return(bb):\n",
    "    \"\"\"The average return (at the end of the seeds alone) from a buffer-buffer\"\"\"\n",
    "    s = torch.zeros(bb[0].returns[:, 0].size(), device = bb[0].returns[:, 0].device)\n",
    "    for b in bb:\n",
    "        #s += bb[0].returns[:, 0]\n",
    "        s += b.returns[:, 0]\n",
    "    return torch.sum(s).item()/(len(bb) * bb[0].returns.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60190e2b-64dc-43ae-a9af-adc424f5be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb(num_buffers=64, batch_size=1, start_with_guide=True):\n",
    "    bb = []\n",
    "    brain.eval()\n",
    "    for i in range(num_buffers):\n",
    "        print(i)\n",
    "        # In this case, we are only training the 'dopamine' layer on the val training loop\n",
    "        buff = GameOutputBuffer(brain, brain.evaluate_text, gamma=0.99, tau=0.97, default_batch_size=batch_size)\n",
    "        if start_with_guide and (i == 0):\n",
    "            fake_data_fill(buff, G, batch_size, device=device) # This one guide is the only difference between this and the main one.\n",
    "        else:\n",
    "            buff.fill(G, num_games=batch_size)\n",
    "        buff.cpu()\n",
    "        bb.append(buff)\n",
    "        #print(buff.traces)\n",
    "    score = average_return(bb)\n",
    "    score_tensor = torch.tensor([score], device=device)\n",
    "    for i in range(num_buffers):\n",
    "        print(i)\n",
    "        buff = bb[i]\n",
    "        buff.to(device)\n",
    "        buff.use_values(score_tensor) # computes values tensor and gae's\n",
    "        buff.cpu()\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe40924-ceb7-42ae-9692-5233a562b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = GameOutputBuffer(brain, brain.evaluate_text, gamma=0.99, tau=0.97, default_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "252ae66c-03fd-4706-8a01-49b2ba4e9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data_fill(buff, G, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9879b63d-cac4-4598-bc5f-9e3e025b7fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RL_helper.GameOutputBuffer at 0x7f0018b7ff50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8badc53-f670-422a-979c-8077a6d78abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff.traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74ed062b-ac7e-4be9-8e07-a44b4962c1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 1.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff.returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "236ed0cb-59d1-4d1a-a32a-58e2012832cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0536c9f1-6b15-4009-81e3-576e6bd5be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "bb = get_bb(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64870770-3cb7-4da1-82f0-377579ad93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb[0].traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b066648-6d8a-42f7-8ca7-d0dd470ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8416fcc7-20a5-44e2-ab9f-73847c06f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvidia_smi_spoof(device=device):\n",
    "    return torch.cuda.memory_allocated() / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d007dd1f-fb0f-4e73-a4c1-7a4cb4f61e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(list(brain.text_enc.parameters()) + list(brain.text_dec.parameters()), lr=0.00001, eps=1e-9)\n",
    "policy_epochs = 4\n",
    "epochs = policy_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f45ac1a-261b-4980-b4a7-030345cbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(policy_optimizer, epochs, buffer_buffer, policy_clip_range=0.1, entropy_loss_weight=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"==========Epoch {epoch}=====================\")\n",
    "        #print(nvidia_smi_spoof())\n",
    "        brain.train()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        train_loss = 0\n",
    "        random.shuffle(buffer_buffer)\n",
    "        i = 0\n",
    "        for buffer in buffer_buffer:\n",
    "            i += 1\n",
    "            buffer.to(device)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_optimizer.zero_grad()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            logpas, entropies = buffer.get_probabilities_and_entropies(evaluation=False)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            ratios = (logpas - buffer.logpas).exp()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            pi_obj = buffer.gaes * ratios\n",
    "            #print(nvidia_smi_spoof())\n",
    "            pi_obj_clipped = buffer.gaes * ratios.clamp(1.0 - policy_clip_range,\n",
    "                                                       1.0 + policy_clip_range)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_loss = -torch.min(pi_obj, pi_obj_clipped).mean()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            entropy_loss = -entropies.mean() * entropy_loss_weight\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss = policy_loss + entropy_loss\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss.backward()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_optimizer.step()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            train_loss += loss.item()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            buffer.cpu()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"episode {i}, policy loss {loss.item()}\\n\")\n",
    "        del loss, logpas, entropies, ratios, pi_obj, pi_obj_clipped, policy_loss, entropy_loss\n",
    "        policy_optimizer.zero_grad()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        print(f\"Policy train loss in epoch {epoch}:{train_loss / (len(buffer_buffer))}\")\n",
    "#train_policy(policy_optimizer, policy_epochs, buffer_buffer, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f46f9c5a-9bc3-4de3-bd4b-fe18cd667995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60eb9fa9-46f1-4782-93ff-eda1a59aa32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_policy(policy_optimizer, 1, bb[:4])#, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "497e45a3-0370-4279-a3f5-d36eaf431e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_policy(policy_optimizer, 2, bb[4:8])#, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "812b0e2b-c32b-4605-810c-16acea4b68bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy_optimizer.zero_grad()\n",
    "#torch.cuda.empty_cache()\n",
    "nvidia_smi_spoof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95dfe71b-cc9e-43dc-b11d-4491b7c39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for b in bb:\n",
    "    #print(b.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293b064-6de3-4be7-a1db-074ae3c393de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "700176bf-191e-4502-b15c-73a125c33eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no policy optimization on first round, only subsequent\n",
    "def run_round(round_num, policy_optimizer, num_buffers=64, batch_size=6, policy_epochs=4, policy_clip_range=0.5, entropy_loss_weight=1e-3):\n",
    "    # First, get some samples\n",
    "    brain.eval()\n",
    "#    get_value.eval()\n",
    "    buffer_buffer = get_bb(num_buffers, batch_size) # run the inference side\n",
    "    print(f\"Return before training was {average_return(buffer_buffer)}\")\n",
    "    #if round_num > 0:\n",
    "    print(\"\\n~~~~~~~POLICY loop~~~~~~~\\n\")\n",
    "    train_policy(policy_optimizer, policy_epochs, buffer_buffer, policy_clip_range, entropy_loss_weight)\n",
    "    #print(\"\\n~~~~~~~~VALUE loop~~~~~~~~~~~\\n\")\n",
    "    #train_val_func(val_optimizer, val_epochs, buffer_buffer)\n",
    "    del buffer_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ddc6d12-3ee2-460d-ab49-19b853173d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************ROUND 0 ***************************\n",
      "\n",
      "Return before training was 0.6967120170593262\n",
      "\n",
      "~~~~~~~POLICY loop~~~~~~~\n",
      "\n",
      "==========Epoch 0=====================\n",
      "episode 1, policy loss 0.0032264201436191797\n",
      "\n",
      "episode 2, policy loss 0.002106467494741082\n",
      "\n",
      "episode 3, policy loss 0.00236302986741066\n",
      "\n",
      "episode 4, policy loss 0.002399618271738291\n",
      "\n",
      "episode 5, policy loss -0.00024642690550535917\n",
      "\n",
      "episode 6, policy loss 0.002241970971226692\n",
      "\n",
      "episode 7, policy loss 0.001999273896217346\n",
      "\n",
      "episode 8, policy loss -0.04256659746170044\n",
      "\n",
      "Policy train loss in epoch 0:-0.0035595304652815685\n",
      "==========Epoch 1=====================\n",
      "episode 1, policy loss 0.00217349361628294\n",
      "\n",
      "episode 2, policy loss 0.0026274563279002905\n",
      "\n",
      "episode 3, policy loss -0.003329175990074873\n",
      "\n",
      "episode 4, policy loss 0.003539941506460309\n",
      "\n",
      "episode 5, policy loss -0.07602822035551071\n",
      "\n",
      "episode 6, policy loss 0.005011497065424919\n",
      "\n",
      "episode 7, policy loss 0.004019663669168949\n",
      "\n",
      "episode 8, policy loss 0.004618199076503515\n",
      "\n",
      "Policy train loss in epoch 1:-0.007170893135480583\n",
      "==========Epoch 2=====================\n",
      "episode 1, policy loss 0.00519831757992506\n",
      "\n",
      "episode 2, policy loss 0.005327532067894936\n",
      "\n",
      "episode 3, policy loss 0.0059084100648760796\n",
      "\n",
      "episode 4, policy loss 0.004488777834922075\n",
      "\n",
      "episode 5, policy loss 0.00466445367783308\n",
      "\n",
      "episode 6, policy loss 0.0047303009778261185\n",
      "\n",
      "episode 7, policy loss -0.10136222094297409\n",
      "\n",
      "episode 8, policy loss -0.0018718013307079673\n",
      "\n",
      "Policy train loss in epoch 2:-0.009114528758800589\n",
      "==========Epoch 3=====================\n",
      "episode 1, policy loss 0.004720989614725113\n",
      "\n",
      "episode 2, policy loss 0.004637219477444887\n",
      "\n",
      "episode 3, policy loss 0.004303325433284044\n",
      "\n",
      "episode 4, policy loss 0.0046592941507697105\n",
      "\n",
      "episode 5, policy loss -0.11637768149375916\n",
      "\n",
      "episode 6, policy loss 0.005849434062838554\n",
      "\n",
      "episode 7, policy loss 0.004661886487156153\n",
      "\n",
      "episode 8, policy loss 0.0011584045132622123\n",
      "\n",
      "Policy train loss in epoch 3:-0.01079839096928481\n",
      "***********************TIME WAS 3.2555286725362143 min*****************************\n",
      "\n",
      "**********************ROUND 1 ***************************\n",
      "\n",
      "Return before training was 0.6776482462882996\n",
      "\n",
      "~~~~~~~POLICY loop~~~~~~~\n",
      "\n",
      "==========Epoch 0=====================\n",
      "episode 1, policy loss 0.022691011428833008\n",
      "\n",
      "episode 2, policy loss 0.01701812818646431\n",
      "\n",
      "episode 3, policy loss 0.017725421115756035\n",
      "\n",
      "episode 4, policy loss 0.000774423242546618\n",
      "\n",
      "episode 5, policy loss -0.00756182661280036\n",
      "\n",
      "episode 6, policy loss 0.003027295460924506\n",
      "\n",
      "episode 7, policy loss 0.01711549051105976\n",
      "\n",
      "episode 8, policy loss -0.11175055801868439\n",
      "\n",
      "Policy train loss in epoch 0:-0.005120076835737564\n",
      "==========Epoch 1=====================\n",
      "episode 1, policy loss 0.016547096893191338\n",
      "\n",
      "episode 2, policy loss 0.014606470242142677\n",
      "\n",
      "episode 3, policy loss 0.02026645839214325\n",
      "\n",
      "episode 4, policy loss -0.12543915212154388\n",
      "\n",
      "episode 5, policy loss -0.005907309707254171\n",
      "\n",
      "episode 6, policy loss 0.0037909424863755703\n",
      "\n",
      "episode 7, policy loss 0.017747338861227036\n",
      "\n",
      "episode 8, policy loss 0.0009612872963771224\n",
      "\n",
      "Policy train loss in epoch 1:-0.007178358457167633\n",
      "==========Epoch 2=====================\n",
      "episode 1, policy loss 0.01815122179687023\n",
      "\n",
      "episode 2, policy loss 0.00017734267748892307\n",
      "\n",
      "episode 3, policy loss 0.019730906933546066\n",
      "\n",
      "episode 4, policy loss 0.017141515389084816\n",
      "\n",
      "episode 5, policy loss 0.02197321131825447\n",
      "\n",
      "episode 6, policy loss 0.0028537530452013016\n",
      "\n",
      "episode 7, policy loss -0.005484971217811108\n",
      "\n",
      "episode 8, policy loss -0.12852396070957184\n",
      "\n",
      "Policy train loss in epoch 2:-0.006747622595867142\n",
      "==========Epoch 3=====================\n",
      "episode 1, policy loss 0.002375864889472723\n",
      "\n",
      "episode 2, policy loss -0.14044642448425293\n",
      "\n",
      "episode 3, policy loss 0.015749657526612282\n",
      "\n",
      "episode 4, policy loss -0.004985187202692032\n",
      "\n",
      "episode 5, policy loss 0.023863987997174263\n",
      "\n",
      "episode 6, policy loss 0.01944654993712902\n",
      "\n",
      "episode 7, policy loss 0.021970169618725777\n",
      "\n",
      "episode 8, policy loss 0.00042824388947337866\n",
      "\n",
      "Policy train loss in epoch 3:-0.00769964222854469\n",
      "***********************TIME WAS 8.15078562895457 min*****************************\n",
      "\n",
      "**********************ROUND 2 ***************************\n",
      "\n",
      "Return before training was 0.7225262522697449\n",
      "\n",
      "~~~~~~~POLICY loop~~~~~~~\n",
      "\n",
      "==========Epoch 0=====================\n",
      "episode 1, policy loss 0.028368448838591576\n",
      "\n",
      "episode 2, policy loss -0.1402934491634369\n",
      "\n",
      "episode 3, policy loss 0.010627840645611286\n",
      "\n",
      "episode 4, policy loss 0.031743235886096954\n",
      "\n",
      "episode 5, policy loss 0.022974621504545212\n",
      "\n",
      "episode 6, policy loss 0.017084987834095955\n",
      "\n",
      "episode 7, policy loss 0.02959718368947506\n",
      "\n",
      "episode 8, policy loss 0.02328760363161564\n",
      "\n",
      "Policy train loss in epoch 0:0.002923809108324349\n",
      "==========Epoch 1=====================\n",
      "episode 1, policy loss 0.010689116083085537\n",
      "\n",
      "episode 2, policy loss 0.02826794423162937\n",
      "\n",
      "episode 3, policy loss 0.020712602883577347\n",
      "\n",
      "episode 4, policy loss 0.025436164811253548\n",
      "\n",
      "episode 5, policy loss -0.12910807132720947\n",
      "\n",
      "episode 6, policy loss 0.02213732898235321\n",
      "\n",
      "episode 7, policy loss 0.013954085297882557\n",
      "\n",
      "episode 8, policy loss 0.018261229619383812\n",
      "\n",
      "Policy train loss in epoch 1:0.0012938000727444887\n",
      "==========Epoch 2=====================\n",
      "episode 1, policy loss 0.00931982509791851\n",
      "\n",
      "episode 2, policy loss 0.013445444405078888\n",
      "\n",
      "episode 3, policy loss 0.024453099817037582\n",
      "\n",
      "episode 4, policy loss 0.024022456258535385\n",
      "\n",
      "episode 5, policy loss -0.13511672616004944\n",
      "\n",
      "episode 6, policy loss 0.016201447695493698\n",
      "\n",
      "episode 7, policy loss 0.01592966541647911\n",
      "\n",
      "episode 8, policy loss 0.02152938023209572\n",
      "\n",
      "Policy train loss in epoch 2:-0.0012769259046763182\n",
      "==========Epoch 3=====================\n",
      "episode 1, policy loss 0.020687662065029144\n",
      "\n",
      "episode 2, policy loss 0.02133304998278618\n",
      "\n",
      "episode 3, policy loss -0.13967843353748322\n",
      "\n",
      "episode 4, policy loss 0.02184945158660412\n",
      "\n",
      "episode 5, policy loss 0.013004298321902752\n",
      "\n",
      "episode 6, policy loss 0.011465140618383884\n",
      "\n",
      "episode 7, policy loss 0.008417055942118168\n",
      "\n",
      "episode 8, policy loss 0.015844807028770447\n",
      "\n",
      "Policy train loss in epoch 3:-0.0033846209989860654\n",
      "***********************TIME WAS 7.520892572402954 min*****************************\n",
      "\n",
      "**********************ROUND 3 ***************************\n",
      "\n",
      "Return before training was 0.6532108187675476\n",
      "\n",
      "~~~~~~~POLICY loop~~~~~~~\n",
      "\n",
      "==========Epoch 0=====================\n",
      "episode 1, policy loss 0.004322410561144352\n",
      "\n",
      "episode 2, policy loss -0.13800835609436035\n",
      "\n",
      "episode 3, policy loss 0.0016914202133193612\n",
      "\n",
      "episode 4, policy loss 0.005718758795410395\n",
      "\n",
      "episode 5, policy loss 0.011510569602251053\n",
      "\n",
      "episode 6, policy loss 0.006276020780205727\n",
      "\n",
      "episode 7, policy loss -0.021224796772003174\n",
      "\n",
      "episode 8, policy loss 0.0037093651480972767\n",
      "\n",
      "Policy train loss in epoch 0:-0.01575057597074192\n",
      "==========Epoch 1=====================\n",
      "episode 1, policy loss 0.006307010073214769\n",
      "\n",
      "episode 2, policy loss 0.002092610113322735\n",
      "\n",
      "episode 3, policy loss 0.004267135169357061\n",
      "\n",
      "episode 4, policy loss 0.005394824780523777\n",
      "\n",
      "episode 5, policy loss 0.008908665738999844\n",
      "\n",
      "episode 6, policy loss 0.0031888489611446857\n",
      "\n",
      "episode 7, policy loss -0.13653087615966797\n",
      "\n",
      "episode 8, policy loss -0.01835714839398861\n",
      "\n",
      "Policy train loss in epoch 1:-0.015591116214636713\n",
      "==========Epoch 2=====================\n",
      "episode 1, policy loss 0.003567560575902462\n",
      "\n",
      "episode 2, policy loss 0.005991155281662941\n",
      "\n",
      "episode 3, policy loss 0.01199218351393938\n",
      "\n",
      "episode 4, policy loss -0.178218811750412\n",
      "\n",
      "episode 5, policy loss 0.006678041070699692\n",
      "\n",
      "episode 6, policy loss 0.0022289426997303963\n",
      "\n",
      "episode 7, policy loss -0.021977728232741356\n",
      "\n",
      "episode 8, policy loss 0.005654875189065933\n",
      "\n",
      "Policy train loss in epoch 2:-0.020510472706519067\n",
      "==========Epoch 3=====================\n",
      "episode 1, policy loss 0.0058970325626432896\n",
      "\n",
      "episode 2, policy loss 0.005591093096882105\n",
      "\n",
      "episode 3, policy loss 0.002708995249122381\n",
      "\n",
      "episode 4, policy loss 0.006919419392943382\n",
      "\n",
      "episode 5, policy loss -0.17125408351421356\n",
      "\n",
      "episode 6, policy loss 0.013995887711644173\n",
      "\n",
      "episode 7, policy loss -0.022080929949879646\n",
      "\n",
      "episode 8, policy loss 0.004955891985446215\n",
      "\n",
      "Policy train loss in epoch 3:-0.019158336683176458\n",
      "***********************TIME WAS 4.78926362991333 min*****************************\n",
      "\n",
      "**********************ROUND 4 ***************************\n",
      "\n",
      "Return before training was 0.7120513916015625\n",
      "\n",
      "~~~~~~~POLICY loop~~~~~~~\n",
      "\n",
      "==========Epoch 0=====================\n",
      "episode 1, policy loss 0.027781307697296143\n",
      "\n",
      "episode 2, policy loss 0.01613306626677513\n",
      "\n",
      "episode 3, policy loss 0.03408385068178177\n",
      "\n",
      "episode 4, policy loss 0.0021544236224144697\n",
      "\n",
      "episode 5, policy loss 0.036819472908973694\n",
      "\n",
      "episode 6, policy loss 0.00408745976164937\n",
      "\n",
      "episode 7, policy loss -0.17133347690105438\n",
      "\n",
      "episode 8, policy loss 0.042475830763578415\n",
      "\n",
      "Policy train loss in epoch 0:-0.0009747581498231739\n",
      "==========Epoch 1=====================\n",
      "episode 1, policy loss 0.041528403759002686\n",
      "\n",
      "episode 2, policy loss -0.18049393594264984\n",
      "\n",
      "episode 3, policy loss 0.011495974846184254\n",
      "\n",
      "episode 4, policy loss -0.0006339120445773005\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**********************ROUND \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ***************************\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrun_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_buffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_clip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_loss_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(brain\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrain_checkpoints/brain_EXPERIMENTAL_5output_weights_semi-guided_RL_GRPO_v1_round\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m, in \u001b[0;36mrun_round\u001b[0;34m(round_num, policy_optimizer, num_buffers, batch_size, policy_epochs, policy_clip_range, entropy_loss_weight)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#if round_num > 0:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m~~~~~~~POLICY loop~~~~~~~\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_clip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_loss_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(\"\\n~~~~~~~~VALUE loop~~~~~~~~~~~\\n\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#train_val_func(val_optimizer, val_epochs, buffer_buffer)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m buffer_buffer\n",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m, in \u001b[0;36mtrain_policy\u001b[0;34m(policy_optimizer, epochs, buffer_buffer, policy_clip_range, entropy_loss_weight)\u001b[0m\n\u001b[1;32m     14\u001b[0m policy_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(nvidia_smi_spoof())\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m logpas, entropies \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities_and_entropies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#print(nvidia_smi_spoof())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ratios \u001b[38;5;241m=\u001b[39m (logpas \u001b[38;5;241m-\u001b[39m buffer\u001b[38;5;241m.\u001b[39mlogpas)\u001b[38;5;241m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/RL_helper.py:342\u001b[0m, in \u001b[0;36mGameOutputBuffer.get_probabilities_and_entropies\u001b[0;34m(self, policy_model, update_self, temp, evaluation)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_probabilities_and_entropies(policy_model, update_self, temp)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_probabilities_and_entropies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/RL_helper.py:251\u001b[0m, in \u001b[0;36mGameOutputBuffer._get_probabilities_and_entropies\u001b[0;34m(self, policy_model, update_self, temp, batch_coords, action_coords, img_gradient)\u001b[0m\n\u001b[1;32m    249\u001b[0m             past_terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_terminated[bS:bE, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m ind] \u001b[38;5;66;03m# we record action leading into '2', but no others\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#            print(f\"past_terminated size: {past_terminated.size()}\")\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m             newlp, newent \u001b[38;5;241m=\u001b[39m \u001b[43mprobs_and_entropies_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbS\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed_offset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_terminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m             logpas[:, i] \u001b[38;5;241m=\u001b[39m newlp\n\u001b[1;32m    253\u001b[0m             entropies[:, i] \u001b[38;5;241m=\u001b[39m newent\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/RL_helper.py:76\u001b[0m, in \u001b[0;36mprobs_and_entropies_all\u001b[0;34m(settings_batch, traces, past_terminated, brain, temp, img_gradient)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprobs_and_entropies_all\u001b[39m(settings_batch, traces, past_terminated, brain, temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, img_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     75\u001b[0m     device \u001b[38;5;241m=\u001b[39m brain\u001b[38;5;241m.\u001b[39mget_device()\n\u001b[0;32m---> 76\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_images_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img_gradient:\n\u001b[1;32m     78\u001b[0m         context \u001b[38;5;241m=\u001b[39m brain\u001b[38;5;241m.\u001b[39mimg_enc(imgs)\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/RL_helper.py:38\u001b[0m, in \u001b[0;36mget_images_settings\u001b[0;34m(settings_batch, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 38\u001b[0m     G2 \u001b[38;5;241m=\u001b[39m \u001b[43mdiscreteGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     img[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(G2\u001b[38;5;241m.\u001b[39mgetData())\n\u001b[1;32m     40\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(img, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/game/discreteEngine.py:60\u001b[0m, in \u001b[0;36mdiscreteGame.__init__\u001b[0;34m(self, settings, envMode)\u001b[0m\n\u001b[1;32m     57\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscrete engine\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindowSurface\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWHITE)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniversal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvMode:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhumanGame()\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/game/discreteEngine.py:281\u001b[0m, in \u001b[0;36mdiscreteGame.universal_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21muniversal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    280\u001b[0m     collected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgold_update()\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvMode:\n\u001b[1;32m    283\u001b[0m         sleep(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/game/discreteEngine.py:244\u001b[0m, in \u001b[0;36mdiscreteGame.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindowSurface\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWHITE)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_walls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_gold()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvMode:\n",
      "File \u001b[0;32m~/Player/pseudoGRPO_agent/game/discreteEngine.py:239\u001b[0m, in \u001b[0;36mdiscreteGame.draw_walls\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m clientSurface \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mrotate(clientSurface, \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m params[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m180\u001b[39m\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;66;03m# Format is consistent with js\u001b[39;00m\n\u001b[1;32m    238\u001b[0m newX, newY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_corner_adjustment(tp[\u001b[38;5;241m0\u001b[39m], tp[\u001b[38;5;241m1\u001b[39m], tp[\u001b[38;5;241m2\u001b[39m], tp[\u001b[38;5;241m3\u001b[39m], tp[\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindowSurface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclientSurface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "policy_epochs=4\n",
    "val_epochs=16\n",
    "num_buffers=8 # keep it simpler\n",
    "batch_size=32#16\n",
    "num_rounds = 150*10*10 # give it more of a chance to learn policy, which can only change a little over each round.\n",
    "policy_clip_range=0.5\n",
    "entropy_loss_weight=5e-3\n",
    "for i in range(num_rounds):\n",
    "    start = time.time()\n",
    "    print(f\"**********************ROUND {i} ***************************\\n\")\n",
    "    run_round(i, policy_optimizer, num_buffers, batch_size, policy_epochs, policy_clip_range, entropy_loss_weight)\n",
    "    torch.save(brain.state_dict(), f'brain_checkpoints/brain_EXPERIMENTAL_5output_weights_semi-guided_RL_GRPO_v1_round{i + 1}.pth')\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"***********************TIME WAS {elapsed / 60} min*****************************\\n\")\n",
    "    # I think the entropy was too low last time, let's see if this fixes the issue.\n",
    "    if i > 40:\n",
    "        entropy_loss_weight = max(entropy_loss_weight / 2, 1e-4)#1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f507f-c1c1-442d-94e2-d31681c7c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having the '2' value available at all for the trace is a problem. It's cutting these short\n",
    "# Also, it needs more steps to find the gold.\n",
    "# THe next reasonable task is to fix these two issues, then rerun this training sesssion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
