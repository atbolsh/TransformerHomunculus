{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f5bd94-74fb-4588-b47b-273035eb2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823f8c1-608a-4865-aeca-1458b0610761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4335c011-7512-4fde-bf66-16bdb5591fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f015e48acc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4245931c-72c7-470f-9e6e-2f6de6e1c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(torch_img):\n",
    "    clean = torch_img.detach().cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c25d90-1bdd-46b4-ae86-a6e8170070d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain = EnhancedAgentBrain()\n",
    "brain.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "\n",
    "brain.load_state_dict(torch.load('brain_checkpoints/enhanced_brain_first_training_batch10000.pth', weights_only=True, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6a4389-e113-4dc7-bf96-7538570644e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(brain.parameters(), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c522ad35-e749-4b70-a282-f4489bd4069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should possibly also include mem_enc? Should just be gen_optimizer? \n",
    "text_optimizer = optim.Adam(list(brain.text_enc.parameters()) + list(brain.text_dec.parameters()), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dedeb05-aeef-4cc0-8c32-41e4c74f92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to randomize the order in which the tasks are trained\n",
    "class ReusableBuffer:\n",
    "    def __init__(self, L):\n",
    "        self.L = L\n",
    "        self.inds = list(range(len(L)))\n",
    "\n",
    "    def draw(self, ind):\n",
    "        return self.L[ind]\n",
    "\n",
    "    def random_draw(self):\n",
    "        ind_ind = random.randint(0, len(self.inds)-1)\n",
    "        ind = self.inds[ind_ind]\n",
    "        if ind_ind == (len(self.inds) - 1):\n",
    "            self.inds = self.inds[:-1]\n",
    "        else:\n",
    "            self.inds = self.inds[:ind_ind] + self.inds[ind_ind + 1:]\n",
    "        if len(self.inds) == 0:\n",
    "            self.inds = list(range(len(self.L)))\n",
    "        return self.L[ind], ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8113bf0-e27a-4964-b993-1511284e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first task (and really anywhere I want to not reset between tasks)\n",
    "# make sure the batch size matches\n",
    "rb = ReusableBuffer([(arrow_task_batch, gen_optimizer, 16), \\\n",
    "                     (qa_task_batch, text_optimizer, 16)]) # add further functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0bf3cd-1207-4726-8983-3708c97ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fcade85-fe40-4a68-b137-4c94c737614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_mins = [1000.0, 1000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6fc04a-8056-41bd-85c0-7e60970a13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88167a4b-dd8d-43e6-b4ae-8f77c51adaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d1940-ba13-4da2-bb5f-b1af6d45d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atbolsh/anaconda3/envs/player/lib/python3.12/site-packages/torch/_tensor.py:955: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, task 1, task batch_num 0\n",
      "\n",
      "batch 1, task 0, task batch_num 0\n",
      "\n",
      "batch 2, task 0, task batch_num 1\n",
      "\n",
      "batch 3, task 1, task batch_num 1\n",
      "\n",
      "batch 4, task 1, task batch_num 2\n",
      "\n",
      "batch 5, task 0, task batch_num 2\n",
      "\n",
      "batch 6, task 1, task batch_num 3\n",
      "\n",
      "batch 7, task 0, task batch_num 3\n",
      "\n",
      "batch 8, task 1, task batch_num 4\n",
      "\n",
      "batch 9, task 0, task batch_num 4\n",
      "\n",
      "Total loss: 22.553747177124023:\n",
      "5.90061616897583 control,\n",
      "4.118896007537842 lrg,\n",
      "4.552083492279053 udg,\n",
      "3.9597909450531006 lra,\n",
      "4.022360324859619 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 30.17000051498413\n",
      "\n",
      "Total loss: 0.49829792976379395; that's 0.24864144623279572 task and 0.24760109186172485 recon and 10.277008056640625 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.8703511562943459\n",
      "\n",
      "Total loss: 12.683548927307129:\n",
      "5.499452590942383 control,\n",
      "1.8017323017120361 lrg,\n",
      "2.1801743507385254 udg,\n",
      "1.4238511323928833 lra,\n",
      "1.7783384323120117 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 17.281756172180177\n",
      "\n",
      "Total loss: 0.4866833984851837; that's 0.2335004210472107 task and 0.25154611468315125 recon and 8.184311866760254 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.4909328338503838\n",
      "\n",
      "Total loss: 0.5050315260887146; that's 0.25817567110061646 task and 0.2452324479818344 recon and 8.116829872131348 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.45928038090467455\n",
      "\n",
      "Total loss: 10.138731956481934:\n",
      "6.377170085906982 control,\n",
      "0.8965486288070679 lrg,\n",
      "1.2501091957092285 udg,\n",
      "0.7482510805130005 lra,\n",
      "0.8666523694992065 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 11.34691481590271\n",
      "\n",
      "Total loss: 8.259605407714844:\n",
      "5.567933559417725 control,\n",
      "0.6878618597984314 lrg,\n",
      "0.9119501113891602 udg,\n",
      "0.5588011741638184 lra,\n",
      "0.5330588817596436 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 8.996881170272827\n",
      "\n",
      "Total loss: 0.4508500397205353; that's 0.22579239308834076 task and 0.22372306883335114 recon and 6.672938823699951 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.4630358603596687\n",
      "\n",
      "Total loss: 0.4482705891132355; that's 0.2247714251279831 task and 0.222275972366333 recon and 6.115939617156982 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.4573390254378319\n",
      "\n",
      "Total loss: 7.703762054443359:\n",
      "5.3517889976501465 control,\n",
      "0.6420815587043762 lrg,\n",
      "0.7585561871528625 udg,\n",
      "0.45886966586112976 lra,\n",
      "0.4924653172492981 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 8.196894960403442\n",
      "\n",
      "Total loss: 7.238184928894043:\n",
      "5.184377670288086 control,\n",
      "0.5429797768592834 lrg,\n",
      "0.6263812780380249 udg,\n",
      "0.4494229555130005 lra,\n",
      "0.43502330780029297 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.797910327911377\n",
      "\n",
      "Total loss: 0.14056381583213806; that's 0.07240144908428192 task and 0.06698562204837799 recon and 5.883692741394043 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.2966720761358738\n",
      "\n",
      "Total loss: 0.12789446115493774; that's 0.05556480959057808 task and 0.07094857841730118 recon and 6.90538215637207 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.12878637336194515\n",
      "\n",
      "Total loss: 8.260026931762695:\n",
      "6.334689617156982 control,\n",
      "0.5156065225601196 lrg,\n",
      "0.5645286440849304 udg,\n",
      "0.40730810165405273 lra,\n",
      "0.43789416551589966 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.573080921173096\n",
      "\n",
      "Total loss: 0.10727670043706894; that's 0.05472128465771675 task and 0.051231782883405685 recon and 6.618162155151367 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.11430917367339134\n",
      "\n",
      "Total loss: 7.947452068328857:\n",
      "6.059776782989502 control,\n",
      "0.5091887712478638 lrg,\n",
      "0.5480396747589111 udg,\n",
      "0.3993813991546631 lra,\n",
      "0.43106505274772644 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.61575553894043\n",
      "\n",
      "Total loss: 7.986446380615234:\n",
      "6.184212684631348 control,\n",
      "0.4784988760948181 lrg,\n",
      "0.5146611928939819 udg,\n",
      "0.40563100576400757 lra,\n",
      "0.40344223380088806 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.62965265750885\n",
      "\n",
      "Total loss: 0.10455796867609024; that's 0.05311354622244835 task and 0.05011395364999771 recon and 6.652359962463379 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.10851532571017743\n",
      "\n",
      "Total loss: 0.10899094492197037; that's 0.05262337252497673 task and 0.05526306480169296 recon and 5.522573947906494 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.10570700779557228\n",
      "\n",
      "Total loss: 6.844134330749512:\n",
      "5.139870643615723 control,\n",
      "0.4816208779811859 lrg,\n",
      "0.49552997946739197 udg,\n",
      "0.3638569116592407 lra,\n",
      "0.36325615644454956 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.104808082580567\n",
      "\n",
      "Total loss: 7.111235618591309:\n",
      "5.305211544036865 control,\n",
      "0.5101438164710999 lrg,\n",
      "0.5302289724349976 udg,\n",
      "0.3932132422924042 lra,\n",
      "0.3724377155303955 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.559808106422424\n",
      "\n",
      "Total loss: 0.1067299172282219; that's 0.05641883239150047 task and 0.04916175454854965 recon and 5.7466607093811035 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.10451312892138959\n",
      "\n",
      "Total loss: 0.10200505703687668; that's 0.051902614533901215 task and 0.048872895538806915 recon and 6.14775276184082 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.10330689691007137\n",
      "\n",
      "Total loss: 7.459380626678467:\n",
      "5.696165561676025 control,\n",
      "0.4541223347187042 lrg,\n",
      "0.5616806149482727 udg,\n",
      "0.3707629144191742 lra,\n",
      "0.3766487240791321 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.582545948028565\n",
      "\n",
      "Total loss: 6.877572059631348:\n",
      "5.138911247253418 control,\n",
      "0.48627349734306335 lrg,\n",
      "0.5193048715591431 udg,\n",
      "0.3586856722831726 lra,\n",
      "0.37439748644828796 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.290009422302246\n",
      "\n",
      "Total loss: 0.10128821432590485; that's 0.052153829485177994 task and 0.0480225495994091 recon and 5.559161186218262 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.10226003259420395\n",
      "\n",
      "Total loss: 0.09875310212373734; that's 0.05054677650332451 task and 0.04719483107328415 recon and 5.057478904724121 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.1011459244787693\n",
      "\n",
      "Total loss: 6.205635070800781:\n",
      "4.43958854675293 control,\n",
      "0.4942784309387207 lrg,\n",
      "0.5211514830589294 udg,\n",
      "0.3666853904724121 lra,\n",
      "0.3839311897754669 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.046588563919068\n",
      "\n",
      "Total loss: 0.09916278719902039; that's 0.05082074552774429 task and 0.04726122319698334 recon and 5.404087543487549 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09985888689756393\n",
      "\n",
      "Total loss: 6.596215724945068:\n",
      "4.916755676269531 control,\n",
      "0.44850319623947144 lrg,\n",
      "0.48485660552978516 udg,\n",
      "0.3734447956085205 lra,\n",
      "0.3726555407047272 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.078212080001831\n",
      "\n",
      "Total loss: 7.111203193664551:\n",
      "5.353342533111572 control,\n",
      "0.4837398827075958 lrg,\n",
      "0.4815850555896759 udg,\n",
      "0.39650657773017883 lra,\n",
      "0.39602890610694885 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.939341831207275\n",
      "\n",
      "Total loss: 0.09704063087701797; that's 0.049305301159620285 task and 0.046583205461502075 recon and 5.760592460632324 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09846400707960129\n",
      "\n",
      "Total loss: 0.09572867304086685; that's 0.0490487739443779 task and 0.04557628184556961 recon and 5.518070697784424 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09703094750642777\n",
      "\n",
      "Total loss: 6.808216094970703:\n",
      "5.095158100128174 control,\n",
      "0.47156286239624023 lrg,\n",
      "0.5035003423690796 udg,\n",
      "0.37453269958496094 lra,\n",
      "0.3634618818759918 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.672108812332153\n",
      "\n",
      "Total loss: 0.09444553405046463; that's 0.04820885881781578 task and 0.045161500573158264 recon and 5.375839710235596 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09570456385612487\n",
      "\n",
      "Total loss: 6.712753772735596:\n",
      "5.0000200271606445 control,\n",
      "0.45942574739456177 lrg,\n",
      "0.5292031168937683 udg,\n",
      "0.3585106134414673 lra,\n",
      "0.3655945956707001 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.931922430992127\n",
      "\n",
      "Total loss: 0.09524263441562653; that's 0.04751106724143028 task and 0.04666387289762497 recon and 5.338478088378906 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09449861317873001\n",
      "\n",
      "Total loss: 6.4488091468811035:\n",
      "4.876107692718506 control,\n",
      "0.4259452223777771 lrg,\n",
      "0.4648319184780121 udg,\n",
      "0.3389720618724823 lra,\n",
      "0.3429524302482605 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.90749258518219\n",
      "\n",
      "Total loss: 7.260396480560303:\n",
      "5.514941692352295 control,\n",
      "0.49512356519699097 lrg,\n",
      "0.5419334769248962 udg,\n",
      "0.37261274456977844 lra,\n",
      "0.33578479290008545 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.760802631378174\n",
      "\n",
      "Total loss: 0.09223616868257523; that's 0.04808464273810387 task and 0.04299401864409447 recon and 5.787520408630371 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09295399330556392\n",
      "\n",
      "Total loss: 0.09009139239788055; that's 0.046218980103731155 task and 0.042866017669439316 recon and 5.031982898712158 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.09086988471448422\n",
      "\n",
      "Total loss: 6.3641886711120605:\n",
      "4.701626777648926 control,\n",
      "0.46620866656303406 lrg,\n",
      "0.48636680841445923 udg,\n",
      "0.36441338062286377 lra,\n",
      "0.3455735445022583 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.804553151130676\n",
      "\n",
      "Total loss: 6.425493240356445:\n",
      "4.7165913581848145 control,\n",
      "0.4255281090736389 lrg,\n",
      "0.5279452800750732 udg,\n",
      "0.3933936655521393 lra,\n",
      "0.3620348572731018 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.965853018760681\n",
      "\n",
      "Total loss: 0.08803786337375641; that's 0.04477064684033394 task and 0.04225262999534607 recon and 5.072945594787598 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.08884520284831524\n",
      "\n",
      "Total loss: 0.08544982969760895; that's 0.04393240064382553 task and 0.04021171107888222 recon and 6.528567790985107 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.08667100191116334\n",
      "\n",
      "Total loss: 7.78481388092041:\n",
      "6.16501522064209 control,\n",
      "0.42689117789268494 lrg,\n",
      "0.46227335929870605 udg,\n",
      "0.36446139216423035 lra,\n",
      "0.36617234349250793 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.329177350997925\n",
      "\n",
      "Total loss: 0.08031895756721497; that's 0.04129956662654877 task and 0.038163814693689346 recon and 4.27790641784668 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0829025387018919\n",
      "\n",
      "Total loss: 5.587920188903809:\n",
      "3.939450979232788 control,\n",
      "0.41275957226753235 lrg,\n",
      "0.4548611640930176 udg,\n",
      "0.34644603729248047 lra,\n",
      "0.43440255522727966 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.852564134597778\n",
      "\n",
      "Total loss: 5.827221393585205:\n",
      "4.147067546844482 control,\n",
      "0.4770071804523468 lrg,\n",
      "0.48907509446144104 udg,\n",
      "0.3635139763355255 lra,\n",
      "0.3505573272705078 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.270781779289246\n",
      "\n",
      "Total loss: 0.07604721188545227; that's 0.039095886051654816 task and 0.036068402230739594 recon and 4.414602756500244 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0789516232907772\n",
      "\n",
      "Total loss: 6.651906967163086:\n",
      "4.917477130889893 control,\n",
      "0.452153742313385 lrg,\n",
      "0.5201489329338074 udg,\n",
      "0.3849092125892639 lra,\n",
      "0.37721800804138184 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.575291128158569\n",
      "\n",
      "Total loss: 0.07362200319766998; that's 0.03898032382130623 task and 0.03359973430633545 recon and 5.209747314453125 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.07436780847609042\n",
      "\n",
      "Total loss: 6.802957534790039:\n",
      "5.13054084777832 control,\n",
      "0.42131635546684265 lrg,\n",
      "0.4776765704154968 udg,\n",
      "0.3781578540802002 lra,\n",
      "0.39526674151420593 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.535423903465271\n",
      "\n",
      "Total loss: 0.06563199311494827; that's 0.0336175374686718 task and 0.03092959150671959 recon and 5.424316883087158 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.06912756808102132\n",
      "\n",
      "Total loss: 7.698009490966797:\n",
      "6.057917594909668 control,\n",
      "0.45232832431793213 lrg,\n",
      "0.46828940510749817 udg,\n",
      "0.37365415692329407 lra,\n",
      "0.3458201587200165 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.60124370098114\n",
      "\n",
      "Total loss: 0.06110979616641998; that's 0.031580060720443726 task and 0.02824024297297001 recon and 6.4474687576293945 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0639059654995799\n",
      "\n",
      "Total loss: 6.474217891693115:\n",
      "4.75274658203125 control,\n",
      "0.4500091075897217 lrg,\n",
      "0.5629670023918152 udg,\n",
      "0.34700509905815125 lra,\n",
      "0.3614901900291443 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.027767028808594\n",
      "\n",
      "Total loss: 0.0563834123313427; that's 0.0299422238022089 task and 0.02542765624821186 recon and 5.067654132843018 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.058511741533875464\n",
      "\n",
      "Total loss: 0.050044044852256775; that's 0.025676026940345764 task and 0.023316914215683937 recon and 5.255526542663574 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.05362459346652031\n",
      "\n",
      "Total loss: 6.560187816619873:\n",
      "4.910270690917969 control,\n",
      "0.4461429715156555 lrg,\n",
      "0.49664708971977234 udg,\n",
      "0.3564354181289673 lra,\n",
      "0.3506920039653778 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.8866122484207155\n",
      "\n",
      "Total loss: 6.127711296081543:\n",
      "4.532339572906494 control,\n",
      "0.430046945810318 lrg,\n",
      "0.46937260031700134 udg,\n",
      "0.3469538986682892 lra,\n",
      "0.34899789094924927 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.808589148521423\n",
      "\n",
      "Total loss: 0.04648758843541145; that's 0.024393519386649132 task and 0.021128777414560318 recon and 4.8264689445495605 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.04884733136743307\n",
      "\n",
      "Total loss: 6.06424617767334:\n",
      "4.385249137878418 control,\n",
      "0.44038721919059753 lrg,\n",
      "0.48437055945396423 udg,\n",
      "0.36261141300201416 lra,\n",
      "0.3916279077529907 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.3652088069915775\n",
      "\n",
      "Total loss: 0.04512016475200653; that's 0.024965284392237663 task and 0.019190629944205284 recon and 4.821249961853027 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.04522345624864101\n",
      "\n",
      "Total loss: 0.038943205028772354; that's 0.020244572311639786 task and 0.01772463694214821 recon and 4.869973182678223 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.04133363325148821\n",
      "\n",
      "Total loss: 6.196595668792725:\n",
      "4.476412296295166 control,\n",
      "0.49987226724624634 lrg,\n",
      "0.4922613203525543 udg,\n",
      "0.370475709438324 lra,\n",
      "0.35757383704185486 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.891004014015198\n",
      "\n",
      "Total loss: 0.0384819470345974; that's 0.02088572084903717 task and 0.016507495194673538 recon and 5.4436540603637695 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.038889867700636384\n",
      "\n",
      "Total loss: 6.736996650695801:\n",
      "5.127436637878418 control,\n",
      "0.4220651686191559 lrg,\n",
      "0.4450581967830658 udg,\n",
      "0.3792000710964203 lra,\n",
      "0.36323633790016174 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.859900331497192\n",
      "\n",
      "Total loss: 0.03442652150988579; that's 0.018245959654450417 task and 0.015170351602137089 recon and 5.051053524017334 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.03650578293949366\n",
      "\n",
      "Total loss: 6.249738693237305:\n",
      "4.63918924331665 control,\n",
      "0.41744011640548706 lrg,\n",
      "0.4895709753036499 udg,\n",
      "0.3290122151374817 lra,\n",
      "0.37452617287635803 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.011431818008423\n",
      "\n",
      "Total loss: 0.03269222751259804; that's 0.017280444502830505 task and 0.014150709845125675 recon and 6.305372714996338 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.03387875191867352\n",
      "\n",
      "Total loss: 7.644423007965088:\n",
      "5.966886520385742 control,\n",
      "0.46559029817581177 lrg,\n",
      "0.4988546371459961 udg,\n",
      "0.37568792700767517 lra,\n",
      "0.33740341663360596 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.908941354751587\n",
      "\n",
      "Total loss: 0.03468366339802742; that's 0.01889283023774624 task and 0.014820750802755356 recon and 4.850423812866211 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.03276608387008309\n",
      "\n",
      "Total loss: 6.219552993774414:\n",
      "4.496963024139404 control,\n",
      "0.42474061250686646 lrg,\n",
      "0.5443011522293091 udg,\n",
      "0.3690635859966278 lra,\n",
      "0.38448482751846313 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.837392091751099\n",
      "\n",
      "Total loss: 0.02922949194908142; that's 0.015372561290860176 task and 0.01282433606684208 recon and 5.162973880767822 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.03215382538735866\n",
      "\n",
      "Total loss: 6.452742099761963:\n",
      "4.759613990783691 control,\n",
      "0.47414928674697876 lrg,\n",
      "0.48591744899749756 udg,\n",
      "0.3674919605255127 lra,\n",
      "0.3655693829059601 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.199408173561096\n",
      "\n",
      "Total loss: 5.715834140777588:\n",
      "4.105673789978027 control,\n",
      "0.438618928194046 lrg,\n",
      "0.48431116342544556 udg,\n",
      "0.3589034378528595 lra,\n",
      "0.3283264935016632 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.75316620349884\n",
      "\n",
      "Total loss: 0.026804883033037186; that's 0.014452232047915459 task and 0.01148674264550209 recon and 4.329542636871338 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.029208789877593518\n",
      "\n",
      "Total loss: 6.050023078918457:\n",
      "4.416409492492676 control,\n",
      "0.44507497549057007 lrg,\n",
      "0.4855511784553528 udg,\n",
      "0.331539124250412 lra,\n",
      "0.37144795060157776 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.4749526691436765\n",
      "\n",
      "Total loss: 0.025980260223150253; that's 0.013550428673624992 task and 0.011483896523714066 recon and 4.729676246643066 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.02751423029229045\n",
      "\n",
      "Total loss: 5.875304222106934:\n",
      "4.234046936035156 control,\n",
      "0.4165800213813782 lrg,\n",
      "0.5003262758255005 udg,\n",
      "0.354327529668808 lra,\n",
      "0.37002313137054443 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.238547258377075\n",
      "\n",
      "Total loss: 0.026347395032644272; that's 0.014379046857357025 task and 0.011051657609641552 recon and 4.5834527015686035 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.026319114193320274\n",
      "\n",
      "Total loss: 5.3842573165893555:\n",
      "3.7971014976501465 control,\n",
      "0.4036791920661926 lrg,\n",
      "0.4828704595565796 udg,\n",
      "0.3545459806919098 lra,\n",
      "0.34605976939201355 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.079243383407593\n",
      "\n",
      "Total loss: 0.02404206246137619; that's 0.013044068589806557 task and 0.010195670649409294 recon and 4.011612892150879 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.02539775464683771\n",
      "\n",
      "Total loss: 0.028678109869360924; that's 0.015921106562018394 task and 0.011848238296806812 recon and 4.543819427490234 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.024838567413389682\n",
      "\n",
      "Total loss: 5.8854289054870605:\n",
      "4.219790458679199 control,\n",
      "0.4282551109790802 lrg,\n",
      "0.5099121928215027 udg,\n",
      "0.3429698348045349 lra,\n",
      "0.3845016658306122 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.139909543991089\n",
      "\n",
      "Total loss: 0.02250455878674984; that's 0.012309035286307335 task and 0.009253712370991707 recon and 4.7090582847595215 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.025103680323809385\n",
      "\n",
      "Total loss: 6.014991760253906:\n",
      "4.357061386108398 control,\n",
      "0.4776883125305176 lrg,\n",
      "0.4580276906490326 udg,\n",
      "0.37062081694602966 lra,\n",
      "0.35159361362457275 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.9434147596359255\n",
      "\n",
      "Total loss: 5.644326686859131:\n",
      "4.041412830352783 control,\n",
      "0.44947531819343567 lrg,\n",
      "0.45657265186309814 udg,\n",
      "0.37162160873413086 lra,\n",
      "0.325244277715683 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.686218056678772\n",
      "\n",
      "Total loss: 0.021417036652565002; that's 0.011712796986103058 task and 0.008842658251523972 recon and 4.307906627655029 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.02240136291831732\n",
      "\n",
      "Total loss: 5.907614707946777:\n",
      "4.267621040344238 control,\n",
      "0.45644402503967285 lrg,\n",
      "0.48104628920555115 udg,\n",
      "0.35306188464164734 lra,\n",
      "0.3494420647621155 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.75020504951477\n",
      "\n",
      "Total loss: 0.02241208776831627; that's 0.012322399765253067 task and 0.009175309911370277 recon and 4.57189416885376 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.02179698260501027\n",
      "\n",
      "Total loss: 6.3439435958862305:\n",
      "4.7137041091918945 control,\n",
      "0.4407041370868683 lrg,\n",
      "0.46078088879585266 udg,\n",
      "0.34749636054039 lra,\n",
      "0.38125768303871155 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.740206089019775\n",
      "\n",
      "Total loss: 0.021145140752196312; that's 0.011957722716033459 task and 0.008178898133337498 recon and 5.042603492736816 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.021199517343193294\n",
      "\n",
      "Total loss: 0.020043285563588142; that's 0.011381616815924644 task and 0.00789905060082674 recon and 3.813081979751587 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.02050580842420459\n",
      "\n",
      "Total loss: 5.073249340057373:\n",
      "3.4453728199005127 control,\n",
      "0.4434524178504944 lrg,\n",
      "0.46435320377349854 udg,\n",
      "0.37084636092185974 lra,\n",
      "0.34922441840171814 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.494744310379028\n",
      "\n",
      "Total loss: 0.02044825628399849; that's 0.010809596627950668 task and 0.008741173893213272 recon and 4.4874267578125 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.020073933470994235\n",
      "\n",
      "Total loss: 5.843379497528076:\n",
      "4.101599216461182 control,\n",
      "0.48089462518692017 lrg,\n",
      "0.473962664604187 udg,\n",
      "0.382403165102005 lra,\n",
      "0.40451955795288086 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.869794907569886\n",
      "\n",
      "Total loss: 0.018476732075214386; that's 0.010094266384840012 task and 0.0072595952078700066 recon and 5.614346504211426 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.019506689570844175\n",
      "\n",
      "Total loss: 6.891243934631348:\n",
      "5.337105751037598 control,\n",
      "0.4246673882007599 lrg,\n",
      "0.4524780511856079 udg,\n",
      "0.33204153180122375 lra,\n",
      "0.34495100378990173 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.462767081260681\n",
      "\n",
      "Total loss: 6.516232967376709:\n",
      "4.954948902130127 control,\n",
      "0.38794317841529846 lrg,\n",
      "0.4867371618747711 udg,\n",
      "0.3414369225502014 lra,\n",
      "0.3451668620109558 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.661393818855285\n",
      "\n",
      "Total loss: 0.019093530252575874; that's 0.010641387663781643 task and 0.007400814443826675 recon and 5.2566375732421875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.019272414557635783\n",
      "\n",
      "Total loss: 6.626950740814209:\n",
      "5.029616355895996 control,\n",
      "0.42862847447395325 lrg,\n",
      "0.46817389130592346 udg,\n",
      "0.3402823507785797 lra,\n",
      "0.36024972796440125 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.697073454856873\n",
      "\n",
      "Total loss: 0.01804395206272602; that's 0.010011067613959312 task and 0.006968424189835787 recon and 5.322305202484131 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01892356775701046\n",
      "\n",
      "Total loss: 6.521953582763672:\n",
      "4.805526256561279 control,\n",
      "0.5101808905601501 lrg,\n",
      "0.46700242161750793 udg,\n",
      "0.3674332797527313 lra,\n",
      "0.3718108534812927 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.693500370979309\n",
      "\n",
      "Total loss: 0.019341550767421722; that's 0.010132691822946072 task and 0.008187846280634403 recon and 5.105066299438477 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.018497206158936023\n",
      "\n",
      "Total loss: 8.063295364379883:\n",
      "6.472601890563965 control,\n",
      "0.40083885192871094 lrg,\n",
      "0.5146088004112244 udg,\n",
      "0.30711814761161804 lra,\n",
      "0.36812812089920044 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.844834914207459\n",
      "\n",
      "Total loss: 0.017548006027936935; that's 0.00969349592924118 task and 0.006501856260001659 recon and 6.763261318206787 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.018476684968918564\n",
      "\n",
      "Total loss: 7.109743595123291:\n",
      "5.456962585449219 control,\n",
      "0.46110308170318604 lrg,\n",
      "0.4920857548713684 udg,\n",
      "0.34556734561920166 lra,\n",
      "0.3540249764919281 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.610577058792114\n",
      "\n",
      "Total loss: 0.01607268676161766; that's 0.008401235565543175 task and 0.00651650270447135 recon and 5.774744033813477 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.018014576956629752\n",
      "\n",
      "Total loss: 6.966802597045898:\n",
      "5.459688186645508 control,\n",
      "0.3837305009365082 lrg,\n",
      "0.45208680629730225 udg,\n",
      "0.32650917768478394 lra,\n",
      "0.34478822350502014 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.6547426414489745\n",
      "\n",
      "Total loss: 0.01837727427482605; that's 0.010807277634739876 task and 0.006434332113713026 recon and 5.678319931030273 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01776315610855818\n",
      "\n",
      "Total loss: 7.094893455505371:\n",
      "5.444926738739014 control,\n",
      "0.4307169020175934 lrg,\n",
      "0.5010458827018738 udg,\n",
      "0.3528625965118408 lra,\n",
      "0.36534106731414795 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.72474769115448\n",
      "\n",
      "Total loss: 0.01714750938117504; that's 0.009490077383816242 task and 0.006517264060676098 recon and 5.700836181640625 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.017689555529505015\n",
      "\n",
      "Total loss: 6.923957347869873:\n",
      "5.393228054046631 control,\n",
      "0.40867117047309875 lrg,\n",
      "0.4194800853729248 udg,\n",
      "0.3358233869075775 lra,\n",
      "0.366754412651062 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.695981101989746\n",
      "\n",
      "Total loss: 0.01749693974852562; that's 0.010384844616055489 task and 0.005973178427666426 recon and 5.694584369659424 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.017283489694818854\n",
      "\n",
      "Total loss: 0.016802562400698662; that's 0.009014018811285496 task and 0.006525898817926645 recon and 6.313225269317627 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01701830042526126\n",
      "\n",
      "Total loss: 7.566057205200195:\n",
      "5.9356184005737305 control,\n",
      "0.4191785454750061 lrg,\n",
      "0.47376275062561035 udg,\n",
      "0.3519921600818634 lra,\n",
      "0.38550540804862976 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.695373964309693\n",
      "\n",
      "Total loss: 0.018144479021430016; that's 0.010003464296460152 task and 0.006953075528144836 recon and 5.939693450927734 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.017357900757342577\n",
      "\n",
      "Total loss: 7.257855415344238:\n",
      "5.640505313873291 control,\n",
      "0.4093409776687622 lrg,\n",
      "0.4871571362018585 udg,\n",
      "0.36441662907600403 lra,\n",
      "0.35643553733825684 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.8648107051849365\n",
      "\n",
      "Total loss: 0.016222894191741943; that's 0.009576075710356236 task and 0.005940062925219536 recon and 3.5337746143341064 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.017235193196684123\n",
      "\n",
      "Total loss: 4.724996089935303:\n",
      "3.163139581680298 control,\n",
      "0.4056461453437805 lrg,\n",
      "0.45505452156066895 udg,\n",
      "0.36642226576805115 lra,\n",
      "0.334733784198761 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.951584711074829\n",
      "\n",
      "Total loss: 5.541937828063965:\n",
      "3.9082367420196533 control,\n",
      "0.4060112237930298 lrg,\n",
      "0.4916623532772064 udg,\n",
      "0.34942781925201416 lra,\n",
      "0.3865995705127716 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.94240562915802\n",
      "\n",
      "Total loss: 0.01703280210494995; that's 0.009027224034070969 task and 0.007170862052589655 recon and 4.173583984375 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01644579635001719\n",
      "\n",
      "Total loss: 0.01549314521253109; that's 0.008449207060039043 task and 0.006018193904310465 recon and 5.128720760345459 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016686296602711082\n",
      "\n",
      "Total loss: 6.3822832107543945:\n",
      "4.814033031463623 control,\n",
      "0.4065578579902649 lrg,\n",
      "0.4454330801963806 udg,\n",
      "0.3530019223690033 lra,\n",
      "0.3632569909095764 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.149649028778076\n",
      "\n",
      "Total loss: 6.149625778198242:\n",
      "4.439568042755127 control,\n",
      "0.47486868500709534 lrg,\n",
      "0.48313918709754944 udg,\n",
      "0.3753677010536194 lra,\n",
      "0.3766823410987854 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.97333333492279\n",
      "\n",
      "Total loss: 0.01578742079436779; that's 0.009003566578030586 task and 0.005833821836858988 recon and 4.7501630783081055 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016477045789361\n",
      "\n",
      "Total loss: 5.437438011169434:\n",
      "3.8347482681274414 control,\n",
      "0.42358946800231934 lrg,\n",
      "0.44742244482040405 udg,\n",
      "0.3773369789123535 lra,\n",
      "0.3543405830860138 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.746241784095764\n",
      "\n",
      "Total loss: 0.019048627465963364; that's 0.01117656659334898 task and 0.007044239901006222 recon and 4.139101982116699 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016722502363845707\n",
      "\n",
      "Total loss: 5.377495765686035:\n",
      "3.754014730453491 control,\n",
      "0.4459270238876343 lrg,\n",
      "0.46178507804870605 udg,\n",
      "0.35943710803985596 lra,\n",
      "0.3563319742679596 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.720972952842712\n",
      "\n",
      "Total loss: 0.01603533886373043; that's 0.009415436536073685 task and 0.005820701830089092 recon and 3.996005058288574 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016006146781146525\n",
      "\n",
      "Total loss: 5.333067893981934:\n",
      "3.72965931892395 control,\n",
      "0.4609118402004242 lrg,\n",
      "0.447204053401947 udg,\n",
      "0.34093236923217773 lra,\n",
      "0.3543601334095001 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.741014790534973\n",
      "\n",
      "Total loss: 0.017096389085054398; that's 0.009679230861365795 task and 0.006608754396438599 recon and 4.042017459869385 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01688047987408936\n",
      "\n",
      "Total loss: 0.016245568171143532; that's 0.009135434404015541 task and 0.0062836045399308205 recon and 4.132650375366211 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01942861646413803\n",
      "\n",
      "Total loss: 5.485819339752197:\n",
      "3.8602428436279297 control,\n",
      "0.4442444443702698 lrg,\n",
      "0.45649829506874084 udg,\n",
      "0.37159115076065063 lra,\n",
      "0.3532427251338959 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.793305401802063\n",
      "\n",
      "Total loss: 0.014132129028439522; that's 0.008089644834399223 task and 0.005258046556264162 recon and 3.922192096710205 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016043111253529786\n",
      "\n",
      "Total loss: 5.2748847007751465:\n",
      "3.5570309162139893 control,\n",
      "0.4484751224517822 lrg,\n",
      "0.5134768486022949 udg,\n",
      "0.3583438992500305 lra,\n",
      "0.3975578546524048 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.500621700286866\n",
      "\n",
      "Total loss: 5.073206424713135:\n",
      "3.5097856521606445 control,\n",
      "0.38324156403541565 lrg,\n",
      "0.47823286056518555 udg,\n",
      "0.33298739790916443 lra,\n",
      "0.368958979845047 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.702600989341736\n",
      "\n",
      "Total loss: 0.015308808535337448; that's 0.008803279139101505 task and 0.005751296877861023 recon and 3.771162748336792 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015840130327269435\n",
      "\n",
      "Total loss: 0.015349438413977623; that's 0.008836043067276478 task and 0.005699646193534136 recon and 4.068745136260986 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015751943131908773\n",
      "\n",
      "Total loss: 5.384417533874512:\n",
      "3.7440860271453857 control,\n",
      "0.4287552535533905 lrg,\n",
      "0.4891173243522644 udg,\n",
      "0.36435410380363464 lra,\n",
      "0.35810497403144836 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.750196261405945\n",
      "\n",
      "Total loss: 5.750524044036865:\n",
      "4.164050102233887 control,\n",
      "0.4573722779750824 lrg,\n",
      "0.45585599541664124 udg,\n",
      "0.35031354427337646 lra,\n",
      "0.32293203473091125 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.718627371788025\n",
      "\n",
      "Total loss: 0.015885164961218834; that's 0.009167445823550224 task and 0.0058311764150857925 recon and 4.43271017074585 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016280009197071194\n",
      "\n",
      "Total loss: 0.01664045639336109; that's 0.009134795516729355 task and 0.006354283541440964 recon and 5.756889343261719 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015552843706682324\n",
      "\n",
      "Total loss: 6.95676326751709:\n",
      "5.419435024261475 control,\n",
      "0.4474301040172577 lrg,\n",
      "0.4220811724662781 udg,\n",
      "0.3417479991912842 lra,\n",
      "0.3260689079761505 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.8685321569442745\n",
      "\n",
      "Total loss: 0.01377763319760561; that's 0.007808406371623278 task and 0.00508979381993413 recon and 4.3971662521362305 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015575238987803459\n",
      "\n",
      "Total loss: 5.714729309082031:\n",
      "4.0435028076171875 control,\n",
      "0.4694315493106842 lrg,\n",
      "0.5161448121070862 udg,\n",
      "0.33560216426849365 lra,\n",
      "0.3500482738018036 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.978818373680115\n",
      "\n",
      "Total loss: 5.567049026489258:\n",
      "3.9329609870910645 control,\n",
      "0.4216517508029938 lrg,\n",
      "0.5115478038787842 udg,\n",
      "0.3648780286312103 lra,\n",
      "0.3360104560852051 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.842905173301697\n",
      "\n",
      "Total loss: 0.014750744216144085; that's 0.0084451949223876 task and 0.005459300708025694 recon and 4.231247425079346 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015268129957839847\n",
      "\n",
      "Total loss: 5.603994846343994:\n",
      "3.918574571609497 control,\n",
      "0.45859652757644653 lrg,\n",
      "0.5013235807418823 udg,\n",
      "0.3722919821739197 lra,\n",
      "0.35320812463760376 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.003272070884704\n",
      "\n",
      "Total loss: 0.015942972153425217; that's 0.009336485527455807 task and 0.005764523055404425 recon and 4.2098188400268555 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014919837899506093\n",
      "\n",
      "Total loss: 0.014594057574868202; that's 0.008209421299397945 task and 0.0055070375092327595 recon and 4.3879923820495605 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01485156487673521\n",
      "\n",
      "Total loss: 5.714846134185791:\n",
      "4.104495525360107 control,\n",
      "0.4236873686313629 lrg,\n",
      "0.4860773980617523 udg,\n",
      "0.3635537922382355 lra,\n",
      "0.3370317220687866 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.648143534660339\n",
      "\n",
      "Total loss: 5.266226768493652:\n",
      "3.761777639389038 control,\n",
      "0.3698768615722656 lrg,\n",
      "0.48189041018486023 udg,\n",
      "0.32086870074272156 lra,\n",
      "0.3318132162094116 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.605576992034912\n",
      "\n",
      "Total loss: 0.01451081968843937; that's 0.008289641700685024 task and 0.005401800852268934 recon and 4.096888542175293 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014903993578627706\n",
      "\n",
      "Total loss: 5.901041507720947:\n",
      "4.31516695022583 control,\n",
      "0.42052289843559265 lrg,\n",
      "0.44196709990501404 udg,\n",
      "0.3407759368419647 lra,\n",
      "0.3826090395450592 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.5226717710495\n",
      "\n",
      "Total loss: 0.01520602684468031; that's 0.008860128931701183 task and 0.005430291406810284 recon and 4.578034400939941 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014920189641416073\n",
      "\n",
      "Total loss: 0.014657323248684406; that's 0.008287984877824783 task and 0.0055410610511898994 recon and 4.141385555267334 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014808934107422829\n",
      "\n",
      "Total loss: 5.464764595031738:\n",
      "3.7917263507843018 control,\n",
      "0.40076354146003723 lrg,\n",
      "0.5074580907821655 udg,\n",
      "0.39372435212135315 lra,\n",
      "0.3710918128490448 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.574501218795777\n",
      "\n",
      "Total loss: 0.014308586716651917; that's 0.008271150290966034 task and 0.005319598130881786 recon and 3.589191198348999 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015011889934539795\n",
      "\n",
      "Total loss: 4.755914688110352:\n",
      "3.21728253364563 control,\n",
      "0.3996778130531311 lrg,\n",
      "0.43311721086502075 udg,\n",
      "0.3322501480579376 lra,\n",
      "0.37358716130256653 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.351203541755677\n",
      "\n",
      "Total loss: 7.040425777435303:\n",
      "5.440448760986328 control,\n",
      "0.4241087734699249 lrg,\n",
      "0.46301236748695374 udg,\n",
      "0.37504252791404724 lra,\n",
      "0.3378135859966278 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.261616291999817\n",
      "\n",
      "Total loss: 0.01489957608282566; that's 0.008878916501998901 task and 0.00488462345674634 recon and 5.680180549621582 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014854899533092976\n",
      "\n",
      "Total loss: 6.498401165008545:\n",
      "4.9511284828186035 control,\n",
      "0.39456185698509216 lrg,\n",
      "0.4465729296207428 udg,\n",
      "0.3786562383174896 lra,\n",
      "0.32748162746429443 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.3210190534591675\n",
      "\n",
      "Total loss: 0.01615433394908905; that's 0.009389777667820454 task and 0.0057056196965277195 recon and 5.294680118560791 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01498767975717783\n",
      "\n",
      "Total loss: 0.014842623844742775; that's 0.008683326654136181 task and 0.00528961606323719 recon and 4.34840726852417 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.015147615987807513\n",
      "\n",
      "Total loss: 5.719814300537109:\n",
      "4.09722900390625 control,\n",
      "0.4164953827857971 lrg,\n",
      "0.480286568403244 udg,\n",
      "0.38390207290649414 lra,\n",
      "0.3419010639190674 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.211148829460144\n",
      "\n",
      "Total loss: 0.01435444038361311; that's 0.008220597170293331 task and 0.005118146073073149 recon and 5.078486919403076 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014648855524137616\n",
      "\n",
      "Total loss: 6.334697723388672:\n",
      "4.748878002166748 control,\n",
      "0.463460236787796 lrg,\n",
      "0.44204971194267273 udg,\n",
      "0.3468025326728821 lra,\n",
      "0.33350715041160583 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.511056180000305\n",
      "\n",
      "Total loss: 0.014488942921161652; that's 0.00806527491658926 task and 0.005186876747757196 recon and 6.1839518547058105 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014638965604826809\n",
      "\n",
      "Total loss: 7.398153305053711:\n",
      "5.888937950134277 control,\n",
      "0.37197667360305786 lrg,\n",
      "0.4563296437263489 udg,\n",
      "0.3148133456707001 lra,\n",
      "0.36609548330307007 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.424095096588135\n",
      "\n",
      "Total loss: 0.013307404704391956; that's 0.007559244055300951 task and 0.004627252463251352 recon and 5.604540824890137 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01450663199648261\n",
      "\n",
      "Total loss: 6.935419082641602:\n",
      "5.370177745819092 control,\n",
      "0.4130857288837433 lrg,\n",
      "0.48615357279777527 udg,\n",
      "0.3175608515739441 lra,\n",
      "0.3484410345554352 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 7.419994168281555\n",
      "\n",
      "Total loss: 6.498438358306885:\n",
      "4.9254069328308105 control,\n",
      "0.3696920573711395 lrg,\n",
      "0.5197557210922241 udg,\n",
      "0.3377670645713806 lra,\n",
      "0.345816433429718 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.56605477809906\n",
      "\n",
      "Total loss: 0.013983006589114666; that's 0.00780339352786541 task and 0.005134883336722851 recon and 5.223648548126221 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014248665105551481\n",
      "\n",
      "Total loss: 0.01380283385515213; that's 0.00785716064274311 task and 0.004695040173828602 recon and 6.253166198730469 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014155306397005915\n",
      "\n",
      "Total loss: 7.602659225463867:\n",
      "5.9766740798950195 control,\n",
      "0.40526458621025085 lrg,\n",
      "0.5182824730873108 udg,\n",
      "0.3595123589038849 lra,\n",
      "0.34292587637901306 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.453684616088867\n",
      "\n",
      "Total loss: 6.331967830657959:\n",
      "4.792665958404541 control,\n",
      "0.4267370104789734 lrg,\n",
      "0.43368375301361084 udg,\n",
      "0.32718518376350403 lra,\n",
      "0.3516959846019745 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.7012632465362545\n",
      "\n",
      "Total loss: 0.014160671271383762; that's 0.008214821107685566 task and 0.004934970289468765 recon and 5.054400444030762 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014262261977419257\n",
      "\n",
      "Total loss: 6.393856048583984:\n",
      "4.763552665710449 control,\n",
      "0.4157601594924927 lrg,\n",
      "0.48568373918533325 udg,\n",
      "0.35258010029792786 lra,\n",
      "0.3762793242931366 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.50829598903656\n",
      "\n",
      "Total loss: 0.013990126550197601; that's 0.008105277083814144 task and 0.00487943971529603 recon and 5.0270466804504395 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014097233256325125\n",
      "\n",
      "Total loss: 6.361944675445557:\n",
      "4.82309627532959 control,\n",
      "0.42488592863082886 lrg,\n",
      "0.4417451322078705 udg,\n",
      "0.33102017641067505 lra,\n",
      "0.3411972224712372 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.201302251815796\n",
      "\n",
      "Total loss: 0.014602412469685078; that's 0.008958891034126282 task and 0.004622726701200008 recon and 5.10397481918335 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014129983251914382\n",
      "\n",
      "Total loss: 5.913753509521484:\n",
      "4.272854328155518 control,\n",
      "0.4690580666065216 lrg,\n",
      "0.466074138879776 udg,\n",
      "0.34563061594963074 lra,\n",
      "0.36013638973236084 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.18876660823822\n",
      "\n",
      "Total loss: 0.013391486369073391; that's 0.007553961593657732 task and 0.004925787914544344 recon and 4.558686256408691 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013585639037191868\n",
      "\n",
      "Total loss: 7.165862083435059:\n",
      "5.590793132781982 control,\n",
      "0.44110170006752014 lrg,\n",
      "0.4425757825374603 udg,\n",
      "0.35826897621154785 lra,\n",
      "0.3331224024295807 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.402835383415222\n",
      "\n",
      "Total loss: 0.013552067801356316; that's 0.007803730200976133 task and 0.004577431362122297 recon and 5.8545331954956055 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013864954989403487\n",
      "\n",
      "Total loss: 0.012905276380479336; that's 0.007390456739813089 task and 0.004359142389148474 recon and 5.77838659286499 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013650501538068056\n",
      "\n",
      "Total loss: 7.104819297790527:\n",
      "5.523390769958496 control,\n",
      "0.4431554079055786 lrg,\n",
      "0.41161325573921204 udg,\n",
      "0.3636931777000427 lra,\n",
      "0.36296653747558594 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.468948998451233\n",
      "\n",
      "Total loss: 6.526828289031982:\n",
      "5.008630275726318 control,\n",
      "0.4241618514060974 lrg,\n",
      "0.4252384901046753 udg,\n",
      "0.3401722013950348 lra,\n",
      "0.32862532138824463 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.626608729362488\n",
      "\n",
      "Total loss: 0.013902826234698296; that's 0.007937601767480373 task and 0.0049248188734054565 recon and 5.202030658721924 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013690121518447996\n",
      "\n",
      "Total loss: 6.933942794799805:\n",
      "5.3707966804504395 control,\n",
      "0.43267178535461426 lrg,\n",
      "0.4400237500667572 udg,\n",
      "0.34080344438552856 lra,\n",
      "0.3496466279029846 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.8523206758499144\n",
      "\n",
      "Total loss: 0.014325671829283237; that's 0.008277079090476036 task and 0.0049134669825434685 recon and 5.675630569458008 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013449424114078283\n",
      "\n",
      "Total loss: 6.221635818481445:\n",
      "4.635156154632568 control,\n",
      "0.43219563364982605 lrg,\n",
      "0.4463064968585968 udg,\n",
      "0.3533770442008972 lra,\n",
      "0.3546004295349121 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.66235921382904\n",
      "\n",
      "Total loss: 0.013551652431488037; that's 0.007687057834118605 task and 0.004884576890617609 recon and 4.900086879730225 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01328417825512588\n",
      "\n",
      "Total loss: 6.325997352600098:\n",
      "4.824681758880615 control,\n",
      "0.4094467759132385 lrg,\n",
      "0.4269247353076935 udg,\n",
      "0.33641892671585083 lra,\n",
      "0.3285251557826996 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.62241376876831\n",
      "\n",
      "Total loss: 0.012796256691217422; that's 0.007382998708635569 task and 0.0044062500819563866 recon and 5.03503942489624 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013195462748408318\n",
      "\n",
      "Total loss: 6.419342517852783:\n",
      "4.9807329177856445 control,\n",
      "0.38135015964508057 lrg,\n",
      "0.3939272165298462 udg,\n",
      "0.3615889847278595 lra,\n",
      "0.3017435073852539 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.566999077796936\n",
      "\n",
      "Total loss: 0.013079359196126461; that's 0.007730172481387854 task and 0.004292858764529228 recon and 5.2816386222839355 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012954572960734368\n",
      "\n",
      "Total loss: 0.012883725576102734; that's 0.0072869895957410336 task and 0.004553614649921656 recon and 5.215607643127441 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.014113978957757354\n",
      "\n",
      "Total loss: 6.573238849639893:\n",
      "4.883342266082764 control,\n",
      "0.47185084223747253 lrg,\n",
      "0.49634072184562683 udg,\n",
      "0.36427581310272217 lra,\n",
      "0.3574291467666626 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.560672011375427\n",
      "\n",
      "Total loss: 0.012751606293022633; that's 0.007281354162842035 task and 0.0044179349206388 recon and 5.261587619781494 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013518062615767122\n",
      "\n",
      "Total loss: 6.5503387451171875:\n",
      "4.930320739746094 control,\n",
      "0.43499165773391724 lrg,\n",
      "0.45470869541168213 udg,\n",
      "0.34484556317329407 lra,\n",
      "0.38547202944755554 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.438619847297669\n",
      "\n",
      "Total loss: 0.013419483788311481; that's 0.008029275573790073 task and 0.004475198686122894 recon and 4.575047969818115 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01353621789254248\n",
      "\n",
      "Total loss: 5.851555824279785:\n",
      "4.2384257316589355 control,\n",
      "0.43399256467819214 lrg,\n",
      "0.4738529324531555 udg,\n",
      "0.35893523693084717 lra,\n",
      "0.34634923934936523 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.740459408760071\n",
      "\n",
      "Total loss: 0.013305521570146084; that's 0.007847757078707218 task and 0.0044186534360051155 recon and 5.195557117462158 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013241097265854478\n",
      "\n",
      "Total loss: 6.462684631347656:\n",
      "4.8942155838012695 control,\n",
      "0.45863810181617737 lrg,\n",
      "0.4295128881931305 udg,\n",
      "0.32211610674858093 lra,\n",
      "0.35820186138153076 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.921031608581543\n",
      "\n",
      "Total loss: 0.012908825650811195; that's 0.0076782894320786 task and 0.004305344540625811 recon and 4.6259589195251465 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012758042998611926\n",
      "\n",
      "Total loss: 5.972022533416748:\n",
      "4.2949066162109375 control,\n",
      "0.47665444016456604 lrg,\n",
      "0.48263058066368103 udg,\n",
      "0.37682032585144043 lra,\n",
      "0.34101077914237976 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.409224362373352\n",
      "\n",
      "Total loss: 6.139206886291504:\n",
      "4.622703552246094 control,\n",
      "0.41845202445983887 lrg,\n",
      "0.41158464550971985 udg,\n",
      "0.3285325765609741 lra,\n",
      "0.3579336404800415 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.69814024925232\n",
      "\n",
      "Total loss: 0.015027856454253197; that's 0.009160016663372517 task and 0.004885269794613123 recon and 4.912848949432373 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.016009115613996983\n",
      "\n",
      "Total loss: 6.339616775512695:\n",
      "4.804079055786133 control,\n",
      "0.34764373302459717 lrg,\n",
      "0.4519418776035309 udg,\n",
      "0.3437969982624054 lra,\n",
      "0.3921546936035156 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.551338658332825\n",
      "\n",
      "Total loss: 0.012981765903532505; that's 0.007524735294282436 task and 0.004439512733370066 recon and 5.087587833404541 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013763922024518252\n",
      "\n",
      "Total loss: 0.013131371699273586; that's 0.0076412600465118885 task and 0.004373036790639162 recon and 5.585374355316162 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013583197211846709\n",
      "\n",
      "Total loss: 6.796685695648193:\n",
      "5.259654998779297 control,\n",
      "0.4268980324268341 lrg,\n",
      "0.4394315779209137 udg,\n",
      "0.3363688290119171 lra,\n",
      "0.3343318700790405 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.925402665138245\n",
      "\n",
      "Total loss: 6.647552013397217:\n",
      "5.032283306121826 control,\n",
      "0.43646740913391113 lrg,\n",
      "0.44913560152053833 udg,\n",
      "0.364566445350647 lra,\n",
      "0.3650988042354584 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.44716073513031\n",
      "\n",
      "Total loss: 0.01370205357670784; that's 0.008025585673749447 task and 0.004598422907292843 recon and 5.390224456787109 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.013234492568299174\n",
      "\n",
      "Total loss: 6.343544006347656:\n",
      "4.718576431274414 control,\n",
      "0.44263148307800293 lrg,\n",
      "0.5022988319396973 udg,\n",
      "0.33098864555358887 lra,\n",
      "0.34904852509498596 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.451618304252625\n",
      "\n",
      "Total loss: 0.012520846910774708; that's 0.00730625307187438 task and 0.004214006010442972 recon and 5.002938270568848 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01295981926843524\n",
      "\n",
      "Total loss: 6.543291091918945:\n",
      "5.039183616638184 control,\n",
      "0.40132206678390503 lrg,\n",
      "0.4258110225200653 udg,\n",
      "0.33136382699012756 lra,\n",
      "0.34561124444007874 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.711593451499939\n",
      "\n",
      "Total loss: 0.012024871073663235; that's 0.006946370471268892 task and 0.003995084203779697 recon and 5.417081832885742 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012563646184280514\n",
      "\n",
      "Total loss: 0.012656138278543949; that's 0.007575951516628265 task and 0.004079363774508238 recon and 5.0041117668151855 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012658369215205311\n",
      "\n",
      "Total loss: 6.2833428382873535:\n",
      "4.717695236206055 control,\n",
      "0.4478771984577179 lrg,\n",
      "0.4277827739715576 udg,\n",
      "0.36728909611701965 lra,\n",
      "0.322698712348938 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.207391233444214\n",
      "\n",
      "Total loss: 0.012247675098478794; that's 0.006800635252147913 task and 0.004293765872716904 recon and 5.766373157501221 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012647906513884664\n",
      "\n",
      "Total loss: 7.015888214111328:\n",
      "5.480175018310547 control,\n",
      "0.4171120524406433 lrg,\n",
      "0.43611380457878113 udg,\n",
      "0.3354664742946625 lra,\n",
      "0.3470212519168854 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.6508269691467286\n",
      "\n",
      "Total loss: 5.824154376983643:\n",
      "4.191499710083008 control,\n",
      "0.4470808804035187 lrg,\n",
      "0.44873562455177307 udg,\n",
      "0.36705467104911804 lra,\n",
      "0.3697836101055145 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.120537691116333\n",
      "\n",
      "Total loss: 0.012477226555347443; that's 0.007488878443837166 task and 0.004109545610845089 recon and 4.394014358520508 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012357238046824932\n",
      "\n",
      "Total loss: 0.012012862600386143; that's 0.006995434407144785 task and 0.004093632102012634 recon and 4.618981838226318 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012476117303594946\n",
      "\n",
      "Total loss: 5.860980033874512:\n",
      "4.2681565284729 control,\n",
      "0.4375549256801605 lrg,\n",
      "0.4605727195739746 udg,\n",
      "0.3446299433708191 lra,\n",
      "0.3500659763813019 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.238139343261719\n",
      "\n",
      "Total loss: 5.8923492431640625:\n",
      "4.2716064453125 control,\n",
      "0.4532369077205658 lrg,\n",
      "0.4592586159706116 udg,\n",
      "0.34418997168540955 lra,\n",
      "0.3640572130680084 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.308250102996826\n",
      "\n",
      "Total loss: 0.01255911123007536; that's 0.007510176859796047 task and 0.004147999919950962 recon and 4.5046706199646 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012338967304676771\n",
      "\n",
      "Total loss: 0.012519126757979393; that's 0.007344092708081007 task and 0.004231730941683054 recon and 4.716513633728027 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01216492487117648\n",
      "\n",
      "Total loss: 5.989080905914307:\n",
      "4.3042988777160645 control,\n",
      "0.4226871728897095 lrg,\n",
      "0.47588711977005005 udg,\n",
      "0.41081565618515015 lra,\n",
      "0.37539178133010864 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.378265342712402\n",
      "\n",
      "Total loss: 0.011975753121078014; that's 0.006604078691452742 task and 0.004440249875187874 recon and 4.657121658325195 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012415925059467554\n",
      "\n",
      "Total loss: 5.974786281585693:\n",
      "4.304487228393555 control,\n",
      "0.43693631887435913 lrg,\n",
      "0.4997207224369049 udg,\n",
      "0.3842693269252777 lra,\n",
      "0.3493726849555969 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.2591168856620785\n",
      "\n",
      "Total loss: 0.012474137358367443; that's 0.0074846986681222916 task and 0.0042708455584943295 recon and 3.5929627418518066 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012880623042583465\n",
      "\n",
      "Total loss: 4.922478199005127:\n",
      "3.2401857376098633 control,\n",
      "0.44986864924430847 lrg,\n",
      "0.5114645957946777 udg,\n",
      "0.35057058930397034 lra,\n",
      "0.37038859724998474 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.346305727958679\n",
      "\n",
      "Total loss: 5.583626747131348:\n",
      "4.040735244750977 control,\n",
      "0.4128040373325348 lrg,\n",
      "0.4483353793621063 udg,\n",
      "0.35623422265052795 lra,\n",
      "0.3255183696746826 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.144050915241241\n",
      "\n",
      "Total loss: 0.012045840732753277; that's 0.007143249269574881 task and 0.004037459380924702 recon and 4.325662136077881 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012397536365315319\n",
      "\n",
      "Total loss: 6.210745334625244:\n",
      "4.592039585113525 control,\n",
      "0.44988322257995605 lrg,\n",
      "0.47245270013809204 udg,\n",
      "0.3484634757041931 lra,\n",
      "0.3479064404964447 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.130059061050415\n",
      "\n",
      "Total loss: 0.01260366290807724; that's 0.007685310207307339 task and 0.0039509739726781845 recon and 4.836893558502197 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012323383335024118\n",
      "\n",
      "Total loss: 0.011636673472821712; that's 0.0066598523408174515 task and 0.004176928196102381 recon and 3.999464988708496 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012072042310610414\n",
      "\n",
      "Total loss: 5.289102554321289:\n",
      "3.6086912155151367 control,\n",
      "0.43741804361343384 lrg,\n",
      "0.4842450022697449 udg,\n",
      "0.36577358841896057 lra,\n",
      "0.3929749131202698 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.966823449134827\n",
      "\n",
      "Total loss: 0.0122014619410038; that's 0.007613282650709152 task and 0.003854663809761405 recon and 3.667576789855957 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011820637071505189\n",
      "\n",
      "Total loss: 4.979823112487793:\n",
      "3.312493085861206 control,\n",
      "0.46779847145080566 lrg,\n",
      "0.4841064214706421 udg,\n",
      "0.355019748210907 lra,\n",
      "0.36040520668029785 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.386824717521668\n",
      "\n",
      "Total loss: 0.011292905546724796; that's 0.007051545660942793 task and 0.00346222217194736 recon and 3.8956878185272217 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011670204922556876\n",
      "\n",
      "Total loss: 5.080334186553955:\n",
      "3.514301061630249 control,\n",
      "0.43694502115249634 lrg,\n",
      "0.4261326491832733 udg,\n",
      "0.3407902717590332 lra,\n",
      "0.36216530203819275 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.22503321647644\n",
      "\n",
      "Total loss: 6.885082721710205:\n",
      "5.271607875823975 control,\n",
      "0.44509515166282654 lrg,\n",
      "0.4678681790828705 udg,\n",
      "0.35021913051605225 lra,\n",
      "0.35029205679893494 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.087703700065613\n",
      "\n",
      "Total loss: 0.012372080236673355; that's 0.007311256602406502 task and 0.003945665434002876 recon and 5.5757927894592285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011922767646610736\n",
      "\n",
      "Total loss: 7.055898189544678:\n",
      "5.524948596954346 control,\n",
      "0.4203200340270996 lrg,\n",
      "0.42560720443725586 udg,\n",
      "0.34564757347106934 lra,\n",
      "0.3393751382827759 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.0907037687301635\n",
      "\n",
      "Total loss: 0.01271364651620388; that's 0.0078023807145655155 task and 0.0037466760259121656 recon and 5.822951793670654 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011890986012294888\n",
      "\n",
      "Total loss: 0.013146180659532547; that's 0.00799481850117445 task and 0.004071809817105532 recon and 5.397761344909668 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011800413262099027\n",
      "\n",
      "Total loss: 6.622325420379639:\n",
      "5.0537614822387695 control,\n",
      "0.46030816435813904 lrg,\n",
      "0.4294186234474182 udg,\n",
      "0.34359729290008545 lra,\n",
      "0.33524009585380554 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.004891614913941\n",
      "\n",
      "Total loss: 0.011920357123017311; that's 0.0071253469213843346 task and 0.0038168730679899454 recon and 4.890687942504883 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011860981369391083\n",
      "\n",
      "Total loss: 6.1688385009765625:\n",
      "4.605798721313477 control,\n",
      "0.42252153158187866 lrg,\n",
      "0.4538891911506653 udg,\n",
      "0.3272172212600708 lra,\n",
      "0.35941189527511597 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.136845908164978\n",
      "\n",
      "Total loss: 0.011614704504609108; that's 0.006918834988027811 task and 0.0038240912836045027 recon and 4.358893871307373 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011833573058247567\n",
      "\n",
      "Total loss: 5.64485502243042:\n",
      "4.08521032333374 control,\n",
      "0.42580169439315796 lrg,\n",
      "0.4122360050678253 udg,\n",
      "0.3684515655040741 lra,\n",
      "0.35315531492233276 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.13487847328186\n",
      "\n",
      "Total loss: 6.661007881164551:\n",
      "5.099366664886475 control,\n",
      "0.43113747239112854 lrg,\n",
      "0.43159595131874084 udg,\n",
      "0.36118391156196594 lra,\n",
      "0.3377237021923065 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.249724383354187\n",
      "\n",
      "Total loss: 0.012327381409704685; that's 0.007491121534258127 task and 0.003745402442291379 recon and 5.45428466796875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011850097710266709\n",
      "\n",
      "Total loss: 0.011965381912887096; that's 0.00690873060375452 task and 0.004044950939714909 recon and 5.058502674102783 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012053730878978968\n",
      "\n",
      "Total loss: 6.297446250915527:\n",
      "4.735414505004883 control,\n",
      "0.41333797574043274 lrg,\n",
      "0.44794145226478577 udg,\n",
      "0.34410718083381653 lra,\n",
      "0.3566451668739319 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.263654656410218\n",
      "\n",
      "Total loss: 0.011338087730109692; that's 0.00656748004257679 task and 0.0038592629134655 recon and 4.556723117828369 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011866593277081847\n",
      "\n",
      "Total loss: 5.8176960945129395:\n",
      "4.228427886962891 control,\n",
      "0.4086054861545563 lrg,\n",
      "0.4672695994377136 udg,\n",
      "0.3344205319881439 lra,\n",
      "0.37897247076034546 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.298318328857422\n",
      "\n",
      "Total loss: 5.778955936431885:\n",
      "4.203386306762695 control,\n",
      "0.4362068772315979 lrg,\n",
      "0.45684731006622314 udg,\n",
      "0.3400708734989166 lra,\n",
      "0.3424449563026428 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.092290830612183\n",
      "\n",
      "Total loss: 0.010939091444015503; that's 0.006355715449899435 task and 0.0036794953048229218 recon and 4.519400596618652 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011879715565592051\n",
      "\n",
      "Total loss: 0.01115386188030243; that's 0.006554563529789448 task and 0.003495911369100213 recon and 5.516934394836426 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011866574343293905\n",
      "\n",
      "Total loss: 6.877915382385254:\n",
      "5.209568500518799 control,\n",
      "0.519385576248169 lrg,\n",
      "0.46664494276046753 udg,\n",
      "0.338412344455719 lra,\n",
      "0.3439040780067444 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.301815009117126\n",
      "\n",
      "Total loss: 0.011447264812886715; that's 0.006836569868028164 task and 0.0036757984198629856 recon and 4.674480438232422 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01173671755939722\n",
      "\n",
      "Total loss: 5.923902988433838:\n",
      "4.325962066650391 control,\n",
      "0.4265005588531494 lrg,\n",
      "0.46687108278274536 udg,\n",
      "0.35384634137153625 lra,\n",
      "0.35072261095046997 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.224362630844116\n",
      "\n",
      "Total loss: 5.523443698883057:\n",
      "3.9347546100616455 control,\n",
      "0.45783114433288574 lrg,\n",
      "0.4329809546470642 udg,\n",
      "0.3293573558330536 lra,\n",
      "0.3685199022293091 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.198968725204468\n",
      "\n",
      "Total loss: 0.011724445968866348; that's 0.0070952014066278934 task and 0.003778962418437004 recon and 4.251410961151123 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011598201040178537\n",
      "\n",
      "Total loss: 0.012107215821743011; that's 0.007313946262001991 task and 0.0036686966195702553 recon and 5.622865676879883 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011545001743361354\n",
      "\n",
      "Total loss: 6.852447986602783:\n",
      "5.270706653594971 control,\n",
      "0.4255577623844147 lrg,\n",
      "0.4319818913936615 udg,\n",
      "0.3719702959060669 lra,\n",
      "0.3522316515445709 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.039063186645508\n",
      "\n",
      "Total loss: 0.011096658185124397; that's 0.006461006123572588 task and 0.003482900094240904 recon and 5.763759613037109 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011673852205276489\n",
      "\n",
      "Total loss: 6.9688825607299805:\n",
      "5.3746113777160645 control,\n",
      "0.42083853483200073 lrg,\n",
      "0.4633156657218933 udg,\n",
      "0.34005236625671387 lra,\n",
      "0.3700648248195648 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.967131013870239\n",
      "\n",
      "Total loss: 0.01245780661702156; that's 0.007568753324449062 task and 0.003970885183662176 recon and 4.590844631195068 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011555073810741305\n",
      "\n",
      "Total loss: 5.804379940032959:\n",
      "4.226980209350586 control,\n",
      "0.41118568181991577 lrg,\n",
      "0.46453309059143066 udg,\n",
      "0.34721580147743225 lra,\n",
      "0.3544648289680481 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.055139970779419\n",
      "\n",
      "Total loss: 0.012000503949820995; that's 0.007135567720979452 task and 0.0039000201504677534 recon and 4.824581146240234 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012167863612994552\n",
      "\n",
      "Total loss: 6.156163215637207:\n",
      "4.536527633666992 control,\n",
      "0.3873680531978607 lrg,\n",
      "0.4888589680194855 udg,\n",
      "0.35710933804512024 lra,\n",
      "0.386298805475235 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.000703101158142\n",
      "\n",
      "Total loss: 7.59147310256958:\n",
      "5.986669063568115 control,\n",
      "0.4189000427722931 lrg,\n",
      "0.46444717049598694 udg,\n",
      "0.3572509288787842 lra,\n",
      "0.3642060458660126 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.2511136436462404\n",
      "\n",
      "Total loss: 0.012107078917324543; that's 0.0072517856024205685 task and 0.0036108510103076696 recon and 6.222212791442871 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011712254956364632\n",
      "\n",
      "Total loss: 5.725264072418213:\n",
      "4.077964782714844 control,\n",
      "0.4334592819213867 lrg,\n",
      "0.477203369140625 udg,\n",
      "0.3692305386066437 lra,\n",
      "0.36740586161613464 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.0258648443222045\n",
      "\n",
      "Total loss: 0.011862408369779587; that's 0.0069593102671206 task and 0.004025173373520374 recon and 4.389623641967773 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011531449444592\n",
      "\n",
      "Total loss: 0.011964910663664341; that's 0.007158045656979084 task and 0.003782049287110567 recon and 5.124075412750244 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011467881547287107\n",
      "\n",
      "Total loss: 6.439888954162598:\n",
      "4.805878162384033 control,\n",
      "0.4209209382534027 lrg,\n",
      "0.4844588339328766 udg,\n",
      "0.36210304498672485 lra,\n",
      "0.36652812361717224 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.056709046363831\n",
      "\n",
      "Total loss: 6.389107704162598:\n",
      "4.841496467590332 control,\n",
      "0.3993941843509674 lrg,\n",
      "0.4584183096885681 udg,\n",
      "0.35204607248306274 lra,\n",
      "0.33775269985198975 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.115437021255493\n",
      "\n",
      "Total loss: 0.011597150936722755; that's 0.006821407470852137 task and 0.0037608628626912832 recon and 5.07440185546875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01160636192187667\n",
      "\n",
      "Total loss: 0.011562290601432323; that's 0.0067379167303442955 task and 0.0037867799401283264 recon and 5.187969207763672 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01172306681983173\n",
      "\n",
      "Total loss: 6.461695194244385:\n",
      "4.881635665893555 control,\n",
      "0.4071286916732788 lrg,\n",
      "0.4756375551223755 udg,\n",
      "0.3405165672302246 lra,\n",
      "0.35677674412727356 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.002654933929444\n",
      "\n",
      "Total loss: 0.011119178496301174; that's 0.006509454920887947 task and 0.0036335729528218508 recon and 4.880756378173828 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011433406444266438\n",
      "\n",
      "Total loss: 6.195573329925537:\n",
      "4.59080696105957 control,\n",
      "0.4365220367908478 lrg,\n",
      "0.4727383553981781 udg,\n",
      "0.3483841121196747 lra,\n",
      "0.3471224009990692 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.729217863082885\n",
      "\n",
      "Total loss: 0.011819389648735523; that's 0.007231110241264105 task and 0.0038360501639544964 recon and 3.7611474990844727 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011617688555270433\n",
      "\n",
      "Total loss: 5.036444187164307:\n",
      "3.4322962760925293 control,\n",
      "0.4345153570175171 lrg,\n",
      "0.44184184074401855 udg,\n",
      "0.36155930161476135 lra,\n",
      "0.36623117327690125 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.792048044204712\n",
      "\n",
      "Total loss: 0.01060669869184494; that's 0.006087473127990961 task and 0.0037012058310210705 recon and 4.090097427368164 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012135377814993263\n",
      "\n",
      "Total loss: 5.3412675857543945:\n",
      "3.771409034729004 control,\n",
      "0.4483788013458252 lrg,\n",
      "0.4404216408729553 udg,\n",
      "0.3388240337371826 lra,\n",
      "0.3422348201274872 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 6.165298795700073\n",
      "\n",
      "Total loss: 0.011559993028640747; that's 0.007145806681364775 task and 0.0037453777622431517 recon and 3.344045639038086 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011857932852581143\n",
      "\n",
      "Total loss: 4.543585777282715:\n",
      "2.9638657569885254 control,\n",
      "0.42453330755233765 lrg,\n",
      "0.44285207986831665 udg,\n",
      "0.3583492338657379 lra,\n",
      "0.3539851903915405 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.86882438659668\n",
      "\n",
      "Total loss: 6.993265151977539:\n",
      "5.434406757354736 control,\n",
      "0.3983004093170166 lrg,\n",
      "0.4243704080581665 udg,\n",
      "0.3632090985774994 lra,\n",
      "0.3729782998561859 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.820170025825501\n",
      "\n",
      "Total loss: 0.012413471937179565; that's 0.007207775488495827 task and 0.004058346152305603 recon and 5.736751079559326 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011691431859508156\n",
      "\n",
      "Total loss: 0.011228407733142376; that's 0.0066225724294781685 task and 0.003588919760659337 recon and 5.084579944610596 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011272253943607211\n",
      "\n",
      "Total loss: 6.325417518615723:\n",
      "4.787967681884766 control,\n",
      "0.4297747015953064 lrg,\n",
      "0.4538799226284027 udg,\n",
      "0.32033005356788635 lra,\n",
      "0.3334652781486511 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.90871561050415\n",
      "\n",
      "Total loss: 0.01189865916967392; that's 0.007281765341758728 task and 0.003708683652803302 recon and 4.541050434112549 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011426185183227062\n",
      "\n",
      "Total loss: 5.706377029418945:\n",
      "4.142394065856934 control,\n",
      "0.44319307804107666 lrg,\n",
      "0.45336049795150757 udg,\n",
      "0.3316569924354553 lra,\n",
      "0.33577266335487366 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.978817820549011\n",
      "\n",
      "Total loss: 0.011331766843795776; that's 0.006758411880582571 task and 0.0036169339437037706 recon and 4.7821044921875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011264075599610806\n",
      "\n",
      "Total loss: 6.105038166046143:\n",
      "4.483945846557617 control,\n",
      "0.4365469217300415 lrg,\n",
      "0.4760207235813141 udg,\n",
      "0.34639573097229004 lra,\n",
      "0.36212870478630066 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.909330234527588\n",
      "\n",
      "Total loss: 0.011221478693187237; that's 0.006666043773293495 task and 0.0038138912059366703 recon and 3.707720994949341 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.01132615962997079\n",
      "\n",
      "Total loss: 5.016775608062744:\n",
      "3.3563714027404785 control,\n",
      "0.4645764231681824 lrg,\n",
      "0.5002310276031494 udg,\n",
      "0.3688682019710541 lra,\n",
      "0.3267281651496887 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.544561843872071\n",
      "\n",
      "Total loss: 6.252563953399658:\n",
      "4.6645612716674805 control,\n",
      "0.46964526176452637 lrg,\n",
      "0.4323449730873108 udg,\n",
      "0.343765527009964 lra,\n",
      "0.34224674105644226 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.6764382314682\n",
      "\n",
      "Total loss: 0.011099851690232754; that's 0.006515657063573599 task and 0.0035962590482085943 recon and 4.939678192138672 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011211372530087828\n",
      "\n",
      "Total loss: 4.837582111358643:\n",
      "3.240399122238159 control,\n",
      "0.414055734872818 lrg,\n",
      "0.4643981158733368 udg,\n",
      "0.3819665312767029 lra,\n",
      "0.3367624282836914 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.3708682012557984\n",
      "\n",
      "Total loss: 0.010625101625919342; that's 0.006445903331041336 task and 0.003475927049294114 recon and 3.516359329223633 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011237018126994371\n",
      "\n",
      "Total loss: 4.534252166748047:\n",
      "2.824690103530884 control,\n",
      "0.46146464347839355 lrg,\n",
      "0.48894909024238586 udg,\n",
      "0.3571542501449585 lra,\n",
      "0.4019941985607147 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.2515644836425786\n",
      "\n",
      "Total loss: 0.010777559131383896; that's 0.006606393959373236 task and 0.0035500051453709602 recon and 3.1058013439178467 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011007953761145472\n",
      "\n",
      "Total loss: 0.010409947484731674; that's 0.006214338820427656 task and 0.0035080057568848133 recon and 3.4380130767822266 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.010964596532285214\n",
      "\n",
      "Total loss: 4.696972370147705:\n",
      "3.1075685024261475 control,\n",
      "0.43645110726356506 lrg,\n",
      "0.47811663150787354 udg,\n",
      "0.3384440541267395 lra,\n",
      "0.33639177680015564 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 4.967428455352783\n",
      "\n",
      "Total loss: 4.6811981201171875:\n",
      "3.02398419380188 control,\n",
      "0.47412407398223877 lrg,\n",
      "0.47961005568504333 udg,\n",
      "0.3639187812805176 lra,\n",
      "0.3395608365535736 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.11455715417862\n",
      "\n",
      "Total loss: 0.011110562831163406; that's 0.0066605182364583015 task and 0.0037923927884548903 recon and 3.2882559299468994 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011200537215918302\n",
      "\n",
      "Total loss: 5.978734016418457:\n",
      "4.467728137969971 control,\n",
      "0.39173373579978943 lrg,\n",
      "0.4422197639942169 udg,\n",
      "0.33792757987976074 lra,\n",
      "0.33912479877471924 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.724606719017029\n",
      "\n",
      "Total loss: 0.011566292494535446; that's 0.007088451646268368 task and 0.003516008146107197 recon and 4.809164524078369 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011104738945141435\n",
      "\n",
      "Total loss: 6.0371413230896:\n",
      "4.47518253326416 control,\n",
      "0.4507541358470917 lrg,\n",
      "0.4148976504802704 udg,\n",
      "0.34100425243377686 lra,\n",
      "0.3553026616573334 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.3946648430824276\n",
      "\n",
      "Total loss: 0.011203509755432606; that's 0.006758461240679026 task and 0.0034799063578248024 recon and 4.82570743560791 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011064718971028923\n",
      "\n",
      "Total loss: 0.010354605503380299; that's 0.006457998417317867 task and 0.003295411355793476 recon and 3.005977153778076 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.010857152193784714\n",
      "\n",
      "Total loss: 4.199853897094727:\n",
      "2.631948232650757 control,\n",
      "0.4097614884376526 lrg,\n",
      "0.43899571895599365 udg,\n",
      "0.38006940484046936 lra,\n",
      "0.3390793800354004 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 4.630851535797119\n",
      "\n",
      "Total loss: 0.01071082428097725; that's 0.006282767280936241 task and 0.003562553320080042 recon and 4.327520370483398 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011024197405204177\n",
      "\n",
      "Total loss: 5.564147472381592:\n",
      "4.020681381225586 control,\n",
      "0.4373911917209625 lrg,\n",
      "0.4320414662361145 udg,\n",
      "0.32669416069984436 lra,\n",
      "0.3473391532897949 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.073736424446106\n",
      "\n",
      "Total loss: 6.3790788650512695:\n",
      "4.791419982910156 control,\n",
      "0.42007389664649963 lrg,\n",
      "0.46920526027679443 udg,\n",
      "0.33535513281822205 lra,\n",
      "0.3630242347717285 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 5.788973922729492\n",
      "\n",
      "Total loss: 0.011652933433651924; that's 0.007183219771832228 task and 0.0034579792991280556 recon and 5.0586748123168945 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.011225174739956855\n",
      "\n",
      "Total loss: 4.004518508911133:\n",
      "2.2590529918670654 control,\n",
      "0.5082156658172607 lrg,\n",
      "0.5161222219467163 udg,\n",
      "0.3576611280441284 lra,\n",
      "0.3634662330150604 uda\n",
      "\n",
      "\n",
      "Average total loss for task 1, last 100 batches: 4.914421043395996\n",
      "\n",
      "Total loss: 0.010768497362732887; that's 0.006909296382218599 task and 0.003357056062668562 recon and 2.5107264518737793 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.010937102697789669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_batches = 10000*100#6250*32\n",
    "\n",
    "for b in range(total_batches):\n",
    "    triplet, ind = rb.random_draw()\n",
    "    func, opt, batch_size = triplet\n",
    "    \n",
    "    batch_num = batches[ind]\n",
    "    batches[ind] += 1\n",
    "\n",
    "    #reset_model = True #default option; only transfer memory within the task files\n",
    "    reset_model = (b % 3 == 2)\n",
    "\n",
    "    printing = ((batch_num % 100) == 99)\n",
    "    full_results = func(batch_size, brain, optimizer=opt, batch_num=batch_num, compute_grad=True, random_order=True, model_eval=False, reset_model=reset_model, printing=printing, training=True)\n",
    "    L = full_results[0] # no need to look into the detailed loss report\n",
    "    total_losses[ind] += L\n",
    "\n",
    "    if printing: # if this is a significant batch\n",
    "        avg_loss = total_losses[ind] / 100\n",
    "        total_losses[ind] = 0\n",
    "        print(f\"Average total loss for task {ind}, last 100 batches: {avg_loss}\\n\")\n",
    "        \n",
    "        if avg_loss < curr_mins[ind]:\n",
    "            curr_mins[ind] = avg_loss\n",
    "            torch.save(brain.state_dict(), f\"brain_checkpoints/enhanced_brain_first_training_v2_batch{b + 1}.pth\")\n",
    "            \n",
    "    if b < 10:\n",
    "        print(f\"batch {b}, task {ind}, task batch_num {batch_num}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a9094-8dec-4a9c-92c2-4f7342e7345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: not even close on the results. Need several days (basically seems to be relearning task1; taskQA basically\n",
    "# completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0519dcb-f815-45e6-a46f-621620263fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning on top of the last results, with many more batches (so several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb7c96-0ce6-4e96-99a6-0a2cddfd5a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
