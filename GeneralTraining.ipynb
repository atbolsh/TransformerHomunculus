{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f5bd94-74fb-4588-b47b-273035eb2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from control_framework import *\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from mem_canvas_use_framework import *\n",
    "\n",
    "from temp_recorder import *\n",
    "\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "#device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823f8c1-608a-4865-aeca-1458b0610761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4335c011-7512-4fde-bf66-16bdb5591fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd1fd1b3050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4245931c-72c7-470f-9e6e-2f6de6e1c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(torch_img):\n",
    "    clean = torch_img.detach().cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c25d90-1bdd-46b4-ae86-a6e8170070d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = EnhancedAgentBrain()\n",
    "brain.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_batch10000.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v1_batch16600.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v1_batch1199.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v1_batch14250.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "fname = 'brain_checkpoints/super_brain_transferred_weights.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#brain.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#brain.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "brain.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "brain.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6a4389-e113-4dc7-bf96-7538570644e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(brain.parameters(), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c522ad35-e749-4b70-a282-f4489bd4069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should possibly also include mem_enc? Should just be gen_optimizer? \n",
    "# ONLY use this in sessions where this is the only optimizer.\n",
    "# General optimizer gets messed up if this is used\n",
    "text_optimizer = optim.Adam(list(brain.text_enc.parameters()) + list(brain.text_dec.parameters()), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dedeb05-aeef-4cc0-8c32-41e4c74f92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to randomize the order in which the tasks are trained\n",
    "class ReusableBuffer:\n",
    "    def __init__(self, L, repetitions):\n",
    "        self.L = []\n",
    "        self.true_inds = []\n",
    "        for i in range(len(L)):\n",
    "            for j in range(repetitions[i]):\n",
    "                self.L.append(L[i])\n",
    "                self.true_inds.append(i)\n",
    "        self.inds = list(range(len(self.L))) # could be longer or shorter than input L\n",
    "\n",
    "    def draw(self, ind):\n",
    "        return self.L[ind]\n",
    "\n",
    "    def random_draw(self):\n",
    "        ind_ind = random.randint(0, len(self.inds)-1)\n",
    "        ind = self.inds[ind_ind]\n",
    "        if ind_ind == (len(self.inds) - 1):\n",
    "            self.inds = self.inds[:-1]\n",
    "        else:\n",
    "            self.inds = self.inds[:ind_ind] + self.inds[ind_ind + 1:]\n",
    "        if len(self.inds) == 0:\n",
    "            self.inds = list(range(len(self.L)))\n",
    "        return self.L[ind], ind, self.true_inds[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8113bf0-e27a-4964-b993-1511284e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first task (and really anywhere I want to not reset between tasks)\n",
    "# make sure the batch size matches\n",
    "# add further functions in the firs list, and add their repetition number to the second list\n",
    "#batch_size = 16\n",
    "batch_size = 8\n",
    "rb = ReusableBuffer([(arrow_task_batch, gen_optimizer, batch_size), \\\n",
    "                     (qa_task_batch, gen_optimizer, batch_size), \\\n",
    "                     (control_batch, gen_optimizer, batch_size), \n",
    "                     (mem_canvas_batch, gen_optimizer, batch_size)], \\\n",
    "                     #[0, 0, 0, 1])\n",
    "                    [1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0bf3cd-1207-4726-8983-3708c97ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fcade85-fe40-4a68-b137-4c94c737614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_mins = [1000.0, 1000.0, 1000.0, 1.0e6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6fc04a-8056-41bd-85c0-7e60970a13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses = [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88167a4b-dd8d-43e6-b4ae-8f77c51adaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e62f64af-1ff3-4f80-804b-17b8488af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# I'll find the place that causes the 'non in-place resize later; for now, I don't want to clutter the results'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c1d1940-ba13-4da2-bb5f-b1af6d45d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, task 0, task batch_num 0\n",
      "\n",
      "batch 1, task 2, task batch_num 0\n",
      "\n",
      "batch 2, task 2, task batch_num 1\n",
      "\n",
      "batch 3, task 0, task batch_num 1\n",
      "\n",
      "batch 4, task 2, task batch_num 2\n",
      "\n",
      "batch 5, task 0, task batch_num 2\n",
      "\n",
      "batch 6, task 2, task batch_num 3\n",
      "\n",
      "batch 7, task 0, task batch_num 3\n",
      "\n",
      "batch 8, task 0, task batch_num 4\n",
      "\n",
      "batch 9, task 2, task batch_num 4\n",
      "\n",
      "Total recon loss: 0.0064416443929076195; that's 4.3228020668029785 text and 0.0021188424434512854 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.01102496888488531\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007393233012408018; that's 0.0047905626706779 task and 0.0016958763590082526 recon and 4.533969402313232 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007623811955563724\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007551493123173714; that's 0.00538284657523036 task and 0.00149982247967273 recon and 3.3441216945648193 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007457783278077841\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00867375172674656; that's 3.07424259185791 text and 0.005599508993327618 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009186960775405168\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007579986471682787; that's 0.0048558833077549934 task and 0.0018185942899435759 recon and 4.527543544769287 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007289294749498367\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007898503914475441; that's 3.9874026775360107 text and 0.003911101724952459 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008769306102767587\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006879073567688465; that's 0.004627542570233345 task and 0.001530389185063541 recon and 3.6057095527648926 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007147682071663439\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011409753933548927; that's 3.361222743988037 text and 0.008048531599342823 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008990417150780559\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009639411233365536; that's 5.183927536010742 text and 0.004455483518540859 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009408144750632346\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007365190424025059; that's 0.004822633694857359 task and 0.001437356579117477 recon and 5.526000499725342 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0074593567568808795\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007782855071127415; that's 0.0049665989354252815 task and 0.0018188833491876721 recon and 4.986863613128662 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007454112055711448\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008492383174598217; that's 4.790387153625488 text and 0.0037019953597337008 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.010062316870316864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.012620514258742332; that's 5.700807094573975 text and 0.006919706705957651 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009956415742635727\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008328625932335854; that's 0.005272959358990192 task and 0.0018657996552065015 recon and 5.949333667755127 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0073161731380969285\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009105748496949673; that's 4.480167388916016 text and 0.0046255807392299175 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.011311483872123062\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00700719840824604; that's 0.004623676184564829 task and 0.00140670919790864 recon and 4.884064197540283 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0077228233637288216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.012356102466583252; that's 4.632723808288574 text and 0.007723378483206034 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00969022852834314\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007358259055763483; that's 0.004841367714107037 task and 0.0015374320792034268 recon and 4.897297382354736 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007322661876678467\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008410271257162094; that's 4.4742865562438965 text and 0.003935983870178461 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009478500029072166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008685433305799961; that's 0.006049817893654108 task and 0.0016850936226546764 recon and 4.752608299255371 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007286546328105032\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.014968070201575756; that's 0.005189885385334492 task and 0.008850092068314552 recon and 4.640462398529053 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.009778017494827509\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009808959439396858; that's 4.388265132904053 text and 0.0054206945933401585 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.014485999569296837\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007302295416593552; that's 0.004586964845657349 task and 0.0018928144127130508 recon and 4.112581253051758 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.012042607627809048\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.01095181331038475; that's 3.572143077850342 text and 0.007379669696092606 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.015011386061087251\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00975177250802517; that's 5.397907257080078 text and 0.004353864584118128 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.010124809751287102\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00904959999024868; that's 0.005887233652174473 task and 0.0020329959224909544 recon and 5.6468505859375 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007796161430887878\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011839577928185463; that's 5.195028781890869 text and 0.006644548382610083 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.010487678004428744\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006768060848116875; that's 0.004299535881727934 task and 0.0013778352877125144 recon and 5.453446865081787 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007695679422467947\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.01580587960779667; that's 4.387678146362305 text and 0.01141820102930069 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.010189923890866339\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0072433846071362495; that's 0.0048596216365695 task and 0.0014721171464771032 recon and 4.558228015899658 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007431894498877227\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.01042572408914566; that's 5.037062644958496 text and 0.005388661287724972 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009520652564242482\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.009308474138379097; that's 0.006328467279672623 task and 0.001765864435583353 recon and 6.070713996887207 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007376919211819768\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008154163137078285; that's 0.0056732515804469585 task and 0.0015332092298194766 recon and 4.7385125160217285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0072954897861927746\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009339417330920696; that's 4.429390907287598 text and 0.004910026211291552 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009556748857721686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.0106426402926445; that's 5.72260856628418 text and 0.004920030944049358 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009694894165731967\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00832558423280716; that's 0.0054369764402508736 task and 0.00151739246211946 recon and 6.856078147888184 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007263932432979345\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00895782932639122; that's 4.146554946899414 text and 0.004811273887753487 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009603448887355625\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007605884224176407; that's 0.004784322343766689 task and 0.0019209847087040544 recon and 4.50288724899292 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007709199525415897\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008729716762900352; that's 3.5717685222625732 text and 0.005157948471605778 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008660228918306529\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005142117850482464; that's 0.0029159709811210632 task and 0.0014616604894399643 recon and 3.82243275642395 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007103362930938602\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008256523869931698; that's 4.556990146636963 text and 0.0036995336413383484 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00971679570619017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007562683895230293; that's 0.005047160666435957 task and 0.0015544844791293144 recon and 4.805192947387695 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007311928831040859\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.010836873203516006; that's 4.80681848526001 text and 0.006030053831636906 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009299322203733027\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007306580897420645; that's 0.004456499125808477 task and 0.0018332324689254165 recon and 5.0842461585998535 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007298591900616884\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00701529998332262; that's 0.004440343007445335 task and 0.0014756221789866686 recon and 5.49667501449585 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007182977152988315\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009475786238908768; that's 5.258033275604248 text and 0.00421775272116065 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009494216474704444\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007602809462696314; that's 0.004917746875435114 task and 0.0017028263537213206 recon and 4.911180019378662 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007251855162903667\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008978525176644325; that's 4.633157730102539 text and 0.0043453676626086235 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009575807359069585\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008729550056159496; that's 5.103962421417236 text and 0.003625587560236454 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009225372145883738\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007907331921160221; that's 0.005269642453640699 task and 0.0015730680897831917 recon and 5.32310676574707 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007138043153099715\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008135748095810413; that's 4.35601806640625 text and 0.0037797298282384872 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00916810156777501\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007607990875840187; that's 0.005269548390060663 task and 0.0013983105309307575 recon and 4.7006611824035645 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007155621824786067\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.0134273711591959; that's 3.9367854595184326 text and 0.009490585885941982 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00923214447684586\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0076705957762897015; that's 0.005342700053006411 task and 0.0014921166002750397 recon and 4.178895950317383 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007071157759055495\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007427210919559002; that's 4.09567928314209 text and 0.003331531072035432 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008693422246724368\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008305460214614868; that's 0.005816765129566193 task and 0.0016324828611686826 recon and 4.281060695648193 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007251213327981532\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006322341039776802; that's 0.003868135856464505 task and 0.001440888037905097 recon and 5.066586017608643 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007171674910932779\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009766627103090286; that's 4.7681403160095215 text and 0.004998486489057541 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008737496682442725\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011769445613026619; that's 4.138032913208008 text and 0.0076314122416079044 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008708891407586634\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008020734414458275; that's 0.005723305977880955 task and 0.0014186953194439411 recon and 4.393664836883545 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007203908604569733\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007970691658556461; that's 0.005147359799593687 task and 0.0017612478695809841 recon and 5.310421466827393 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.008382471734657884\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.015285147354006767; that's 5.0064802169799805 text and 0.010278667323291302 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.013068445543758571\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011968695558607578; that's 4.671967029571533 text and 0.007296728435903788 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.011018999987281859\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0072748130187392235; that's 0.004416820127516985 task and 0.0018319458467885852 recon and 5.130234718322754 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.008655214691534638\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.013945412822067738; that's 4.018183708190918 text and 0.009927229024469852 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.011275804140605033\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007541169412434101; that's 0.004800937604159117 task and 0.001894886139780283 recon and 4.226728439331055 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.008166661784052849\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007284424267709255; that's 0.0048600765876472 task and 0.0013852120609954 recon and 5.195678234100342 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0071422529919072985\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00769721670076251; that's 4.529158115386963 text and 0.003168058581650257 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009258547453209759\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008525791577994823; that's 4.333243370056152 text and 0.004192547872662544 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0093548471480608\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007259412668645382; that's 0.004932831972837448 task and 0.0013789691729471087 recon and 4.738057613372803 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007072920645587146\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008501781150698662; that's 4.267828941345215 text and 0.004233952611684799 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00998038989957422\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0064065661281347275; that's 0.0037463444750756025 task and 0.0016896974993869662 recon and 4.852621078491211 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0075151884695515035\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009748643264174461; that's 3.920891523361206 text and 0.005827751941978931 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009092985885217786\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006772298831492662; that's 0.004396422300487757 task and 0.0014826400438323617 recon and 4.466183185577393 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007285691755823791\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006732393987476826; that's 0.0044075679033994675 task and 0.0014074885984882712 recon and 4.586686134338379 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007070837956853211\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006868001539260149; that's 4.2979631423950195 text and 0.0025700381956994534 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008778217844665051\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006164120975881815; that's 0.003938518930226564 task and 0.001332854270003736 recon and 4.463738441467285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00693695061840117\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008676208555698395; that's 4.219155788421631 text and 0.004457053262740374 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008720776215195656\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005716717801988125; that's 0.0032462417148053646 task and 0.0013313221279531717 recon and 5.6957688331604 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007297749659046531\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008343745954334736; that's 5.242430210113525 text and 0.003101315349340439 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008955243122763933\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008410738781094551; that's 3.9945056438446045 text and 0.00441623292863369 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0090015063341707\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006869135890156031; that's 0.0046856701374053955 task and 0.0013403261546045542 recon and 4.215699672698975 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007041905033402145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009637011215090752; that's 4.174184322357178 text and 0.005462827160954475 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008710059300065041\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006970900110900402; that's 0.004674686584621668 task and 0.0014150391798466444 recon and 4.405872821807861 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007095020520500839\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00882704183459282; that's 5.3019585609436035 text and 0.0035250831861048937 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009414137434214353\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007745333947241306; that's 0.00486555602401495 task and 0.0017125778831541538 recon and 5.835999011993408 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0070701053878292446\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.01673257350921631; that's 3.8615012168884277 text and 0.012871071696281433 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008884729123674334\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006579894572496414; that's 0.004407541360706091 task and 0.0013637121301144361 recon and 4.043207168579102 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007103241593576968\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.008567826822400093; that's 0.00582472886890173 task and 0.001488457783125341 recon and 6.273199081420898 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007019172781147063\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00887611135840416; that's 5.117781162261963 text and 0.00375833036378026 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009153181919828057\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008965619839727879; that's 5.568150997161865 text and 0.0033974682446569204 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009214406316168607\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0066455393098294735; that's 0.00404725968837738 task and 0.0014391187578439713 recon and 5.795804977416992 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007009840039536357\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007141539826989174; that's 2.7379353046417236 text and 0.004403604660183191 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008635792070999742\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0059977369382977486; that's 0.004119252320379019 task and 0.001301540993154049 recon and 2.884718179702759 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00695162623655051\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007038334850221872; that's 0.005086082033813 task and 0.0013336676638573408 recon and 3.092925548553467 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007016073069535196\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.005971505306661129; that's 2.9111123085021973 text and 0.0030603930354118347 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008578370427712798\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00904802419245243; that's 4.71722412109375 text and 0.004330799449235201 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008099276344291867\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007288581691682339; that's 0.004916544537991285 task and 0.001411863835528493 recon and 4.8008646965026855 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006919849319383502\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.0076454984955489635; that's 3.363434076309204 text and 0.0042820642702281475 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008055326975882054\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007021443452686071; that's 0.0050237420946359634 task and 0.0012847837060689926 recon and 3.5645878314971924 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006891864109784365\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007793481461703777; that's 0.0048988196067512035 task and 0.0018537597497925162 recon and 5.2045111656188965 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006918514943681658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008879861794412136; that's 4.876685619354248 text and 0.004003175999969244 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00860018261242658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006544942036271095; that's 4.299710273742676 text and 0.0022452312987297773 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008304360788315535\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007777847349643707; that's 0.00538610527291894 task and 0.0014877432258799672 recon and 4.519996166229248 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0069846597267314795\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006523823365569115; that's 0.004172131419181824 task and 0.001526820007711649 recon and 4.124361038208008 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.010408360050059856\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007597561459988356; that's 3.9031713008880615 text and 0.003694389946758747 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.014041759613901377\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007555213291198015; that's 0.005090240389108658 task and 0.001407200819812715 recon and 5.28886079788208 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007492015240713954\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009413058869540691; that's 4.999350070953369 text and 0.004413708578795195 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009009944712743164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007109572179615498; that's 0.004660177510231733 task and 0.0013148904545232654 recon and 5.672519683837891 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0070962602598592635\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008497964590787888; that's 4.729250907897949 text and 0.0037687132135033607 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009163705776445567\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009194919839501381; that's 5.482795238494873 text and 0.0037121244240552187 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008743799002841115\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0069878483191132545; that's 0.004530740436166525 task and 0.0013017021119594574 recon and 5.777029991149902 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006983859073370695\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00787074863910675; that's 3.4573214054107666 text and 0.0044134268537163734 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009251132253557443\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007878955453634262; that's 0.005721114110201597 task and 0.0014158894773572683 recon and 3.7097628116607666 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007073269481770695\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009504284709692001; that's 4.278042316436768 text and 0.005226242356002331 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009193124654702843\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007756092585623264; that's 0.005225284490734339 task and 0.0016312209190800786 recon and 4.4979376792907715 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00706010714173317\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006808036006987095; that's 0.00437760166823864 task and 0.001303581870160997 recon and 5.634262561798096 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006878359038382769\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008034121245145798; that's 5.219809055328369 text and 0.0028143119998276234 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008715068134479224\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008572901599109173; that's 4.069291114807129 text and 0.004503610078245401 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00909737791866064\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006506496109068394; that's 0.0042960806749761105 task and 0.0013639002572745085 recon and 4.232576847076416 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007095459289848805\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007612339220941067; that's 4.722293376922607 text and 0.002890045987442136 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008106917315162719\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007638735696673393; that's 0.00510249612852931 task and 0.0014633635291829705 recon and 5.364381790161133 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006933748992159963\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008703183382749557; that's 3.5629820823669434 text and 0.0051402016542851925 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00875227847136557\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007342345081269741; that's 0.005348111502826214 task and 0.001252553309313953 recon and 3.7084004878997803 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006856853300705552\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009037245064973831; that's 4.0281453132629395 text and 0.005009098909795284 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008572470778599382\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006022203713655472; that's 0.003827278269454837 task and 0.001351222861558199 recon and 4.218514919281006 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00689542330801487\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006328376941382885; that's 3.386357069015503 text and 0.002942019607871771 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0077684422256425025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006681882310658693; that's 0.004529647529125214 task and 0.0014271835098043084 recon and 3.6252565383911133 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006734698815271258\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008431064896285534; that's 3.59682035446167 text and 0.004834244027733803 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007146310820244253\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0071871401742100716; that's 0.005038443021476269 task and 0.0012342688860371709 recon and 4.572141170501709 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006581062879413366\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011073604226112366; that's 3.7017745971679688 text and 0.007371829356998205 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00800005808006972\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007087986450642347; that's 0.004971801768988371 task and 0.0013134974287822843 recon and 4.013436317443848 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006799720101989806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.011244529858231544; that's 5.846085548400879 text and 0.00539844436571002 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008196836155839264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007714908570051193; that's 0.00451864767819643 task and 0.0017961289267987013 recon and 7.000659942626953 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006859720726497471\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008454209193587303; that's 3.6880743503570557 text and 0.004766135010868311 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00865827155765146\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006841940339654684; that's 0.004733932204544544 task and 0.0012140048202127218 recon and 4.470015525817871 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006913471608422697\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.01110539585351944; that's 4.2837677001953125 text and 0.006821627262979746 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008718341523781419\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0071975770406425; that's 0.004966991022229195 task and 0.0013107304694131017 recon and 4.599278450012207 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0068932948820292946\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006263100542128086; that's 0.004126820247620344 task and 0.001260675024241209 recon and 4.378025531768799 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006916214232333005\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009683073498308659; that's 4.123668670654297 text and 0.005559404846280813 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.009072406301274896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006188926752656698; that's 0.003908579703420401 task and 0.0012189573608338833 recon and 5.306947708129883 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006745733399875462\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007962694391608238; that's 4.984663963317871 text and 0.0029780296608805656 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00867774486541748\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008536447770893574; that's 5.446319103240967 text and 0.003090128069743514 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008798583475872875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00625799223780632; that's 0.0038871716242283583 task and 0.0012534547131508589 recon and 5.586831092834473 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006852695974521339\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009774499572813511; that's 4.685851573944092 text and 0.005088647827506065 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00835294718388468\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007289735134691; that's 0.004470217041671276 task and 0.0017095860093832016 recon and 5.549659729003906 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006899353452026844\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005205947905778885; that's 0.00311382208019495 task and 0.0012418234255164862 recon and 4.2515106201171875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006868427982553839\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006734505295753479; that's 3.974724292755127 text and 0.002759780967608094 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008382849823683501\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008458648808300495; that's 4.41630220413208 text and 0.004042346496134996 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007937439740635455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007105069700628519; that's 0.00484508229419589 task and 0.0012071800883859396 recon and 5.264037609100342 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006671174350194633\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008480475284159184; that's 3.8602640628814697 text and 0.004620211198925972 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008183650271967054\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.009433193132281303; that's 0.0056489622220396996 task and 0.0029398268088698387 recon and 4.222018718719482 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007574285543523729\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.009861405938863754; that's 4.118937969207764 text and 0.005742467939853668 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.010253094877116382\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007329756859689951; that's 0.0050861709751188755 task and 0.0013185956049710512 recon and 4.624950885772705 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007370100999251008\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006924488116055727; that's 0.004963668528944254 task and 0.001221755868755281 recon and 3.6953179836273193 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006784422630444169\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.0074552372097969055; that's 3.273879289627075 text and 0.004181357566267252 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008892425019294024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.00848523061722517; that's 0.005229437258094549 task and 0.0022511749994009733 recon and 5.023092269897461 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006761097903363406\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008310087956488132; that's 4.779355525970459 text and 0.003530732588842511 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008079190086573363\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006907479837536812; that's 0.004447164945304394 task and 0.0016129175201058388 recon and 4.236988067626953 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007858760859817267\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008060564287006855; that's 3.894700765609741 text and 0.0041658636182546616 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.013640246386639774\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006844283081591129; that's 0.0048014638014137745 task and 0.0012270089937373996 recon and 4.079050064086914 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0076930928695946935\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007170040626078844; that's 3.755002498626709 text and 0.003415037877857685 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.012324122218415142\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008773746900260448; that's 3.655452251434326 text and 0.0051182946190238 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.01081984826363623\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.009378102608025074; that's 0.005679018329828978 task and 0.00292411376722157 recon and 3.874850273132324 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007301845168694854\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.010715924203395844; that's 5.9399590492248535 text and 0.004775964189320803 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.01031077140942216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007382900454103947; that's 0.004648012109100819 task and 0.0015266162808984518 recon and 6.04135799407959 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007149391132406891\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006851851940155029; that's 0.004793096333742142 task and 0.0013232029741629958 recon and 3.677762508392334 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006675246879458427\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006705278530716896; that's 3.2678427696228027 text and 0.0034374359529465437 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007571321590803564\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007992574945092201; that's 4.567784786224365 text and 0.0034247899893671274 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008274897253140807\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006353123113512993; that's 0.004018075298517942 task and 0.001369910198263824 recon and 4.825687408447266 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006773969177156686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.0063788872212171555; that's 3.516406297683716 text and 0.0028624804690480232 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007699550976976752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006812816020101309; that's 0.004881017841398716 task and 0.0011945939622819424 recon and 3.686021566390991 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00662220717407763\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006821923889219761; that's 3.5034172534942627 text and 0.0033185062929987907 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007587032299488783\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006384367123246193; that's 0.004412480164319277 task and 0.0012034347746521235 recon and 3.8422603607177734 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00659994559828192\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006918855011463165; that's 3.9986793994903564 text and 0.002920175436884165 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0075613538129255175\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0056589581072330475; that's 0.0035642676521092653 task and 0.001268970314413309 recon and 4.128600120544434 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006621540058404207\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007708653807640076; that's 3.532149314880371 text and 0.004176504444330931 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007293111602775753\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005890103988349438; that's 0.004016340244561434 task and 0.001126237795688212 recon and 3.7376303672790527 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00657443642616272\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008290378376841545; that's 3.92327618598938 text and 0.004367102403193712 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007238923227414489\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007240055128931999; that's 0.005174524616450071 task and 0.0012418439146131277 recon and 4.118433475494385 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006577905206941068\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006690600886940956; that's 0.00489073758944869 task and 0.0011389547726139426 recon and 3.304542064666748 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006560887899249792\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006916140206158161; that's 3.0788044929504395 text and 0.0038373356219381094 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007108539822511375\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005855377297848463; that's 0.003762726904824376 task and 0.0012263500830158591 recon and 4.3315019607543945 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00643179836217314\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006727566942572594; that's 3.969013214111328 text and 0.002758553484454751 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007272626864723861\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008127842098474503; that's 3.196394920349121 text and 0.004931446630507708 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007112721521407366\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006186663638800383; that's 0.004257027059793472 task and 0.0012485652696341276 recon and 3.4053564071655273 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006463317219167947\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008644888177514076; that's 4.8114094734191895 text and 0.003833478782325983 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007401108266785741\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.007225074805319309; that's 0.00476516829803586 task and 0.0014477880904451013 recon and 5.0605926513671875 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006601397367194295\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006176568567752838; that's 3.0898828506469727 text and 0.0030866859015077353 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.006759490612894297\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0061201113276183605; that's 0.004343752283602953 task and 0.0011145937023684382 recon and 3.308826446533203 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006443544779904187\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007049866020679474; that's 2.912471055984497 text and 0.004137394949793816 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0069074033154174685\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005958769004791975; that's 0.0038760672323405743 task and 0.0014480635290965438 recon and 3.173190116882324 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006528659421019256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.007147413678467274; that's 3.299199342727661 text and 0.003848214400932193 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007774475337937474\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006512610241770744; that's 0.004409062676131725 task and 0.0013838873710483313 recon and 3.598299980163574 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006637276136316359\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005992909427732229; that's 0.004185821861028671 task and 0.0011669764062389731 recon and 3.200554847717285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006619721460156143\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.005830379668623209; that's 2.9135937690734863 text and 0.002916785655543208 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.007529970407485962\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0067893136292696; that's 0.004611913114786148 task and 0.0012367194285616279 recon and 4.703405857086182 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00661350233014673\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008139783516526222; that's 3.986685037612915 text and 0.0041530984453856945 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00804317815694958\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.006178148090839386; that's 4.020113468170166 text and 0.002158034360036254 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00949007302056998\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0060952045023441315; that's 0.0039493790827691555 task and 0.0012702277163043618 recon and 4.377989292144775 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.007664267318323255\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00772513821721077; that's 4.546909332275391 text and 0.0031782283913344145 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00795646547805518\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.005545921623706818; that's 0.0034631830640137196 task and 0.0011217087740078568 recon and 4.805150032043457 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006647892454639077\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.006859318818897009; that's 0.004770294297486544 task and 0.001182354404591024 recon and 4.533350467681885 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006768511487171054\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00790300965309143; that's 4.28648567199707 text and 0.003616523463279009 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008293638527393342\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0075274500995874405; that's 0.005219000857323408 task and 0.0011196878040209413 recon and 5.943807601928711 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006728787627071142\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.008456035517156124; that's 4.875970363616943 text and 0.0035800649784505367 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008430229276418686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total loss: 0.0059979259967803955; that's 0.003820915473625064 task and 0.0011555114760994911 recon and 5.107492923736572 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.006554519450291991\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total recon loss: 0.00789710320532322; that's 4.9026618003845215 text and 0.0029944414272904396 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.008115802891552448\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m reset_model \u001b[38;5;241m=\u001b[39m (b \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m printing \u001b[38;5;241m=\u001b[39m ((batch_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m full_results \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprinting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprinting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m L \u001b[38;5;241m=\u001b[39m full_results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# no need to look into the detailed loss report\u001b[39;00m\n\u001b[1;32m     20\u001b[0m total_losses[ind] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L\n",
      "File \u001b[0;32m~/Player/control_framework.py:64\u001b[0m, in \u001b[0;36mcontrol_batch\u001b[0;34m(batch_size, model, optimizer, batch_num, compute_grad, random_order, model_eval, reset_model, printing, training)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcontrol_batch\u001b[39m(batch_size, model, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, compute_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reset_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, printing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_grad:\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_control_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprinting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m training:\n",
      "File \u001b[0;32m~/Player/control_framework.py:31\u001b[0m, in \u001b[0;36m_control_batch\u001b[0;34m(batch_size, model, optimizer, batch_num, random_order, model_eval, reset_model, printing, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m         inds \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(img_set\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), batch_size)\n\u001b[1;32m     29\u001b[0m         img_set \u001b[38;5;241m=\u001b[39m img_set[inds]\n\u001b[0;32m---> 31\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_set\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     33\u001b[0m ind \u001b[38;5;241m=\u001b[39m (batch_num \u001b[38;5;241m*\u001b[39m batch_size) \u001b[38;5;241m%\u001b[39m num_controls\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;241m+\u001b[39m batch_size \u001b[38;5;241m>\u001b[39m num_controls:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_batches = 10000*100#6250*32\n",
    "\n",
    "for b in range(total_batches):\n",
    "    secs_to_cool = monitor_stage(device)\n",
    "    if secs_to_cool > 0:\n",
    "        print(f\"Had to cool device for {secs_to_cool} seconds\\n\")\n",
    "        \n",
    "    triplet, _, ind = rb.random_draw()\n",
    "    func, opt, batch_size = triplet\n",
    "    \n",
    "    batch_num = batches[ind]\n",
    "    batches[ind] += 1\n",
    "\n",
    "    #reset_model = True #default option; only transfer memory within the task files\n",
    "    reset_model = (b % 3 == 2)\n",
    "\n",
    "    printing = ((batch_num % 100) == 99)\n",
    "    full_results = func(batch_size, brain, optimizer=opt, batch_num=batch_num, compute_grad=True, random_order=True, model_eval=False, reset_model=reset_model, printing=printing, training=True)\n",
    "    L = full_results[0] # no need to look into the detailed loss report\n",
    "    total_losses[ind] += L\n",
    "\n",
    "    if printing: # if this is a significant batch\n",
    "        avg_loss = total_losses[ind] / 100\n",
    "        total_losses[ind] = 0\n",
    "        print(f\"Average total loss for task {ind}, last 100 batches: {avg_loss}\\n\\n\\n\\n\")\n",
    "        \n",
    "        if avg_loss < curr_mins[ind]:\n",
    "            curr_mins[ind] = avg_loss\n",
    "            torch.save(brain.state_dict(), f\"brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch{b + 1}.pth\")\n",
    "            \n",
    "    if b < 10:\n",
    "        print(f\"batch {b}, task {ind}, task batch_num {batch_num}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb7c96-0ce6-4e96-99a6-0a2cddfd5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates: add sampling weights; add batch_num to print statement; split these two optimizers and optimize them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82cb545-7d75-492b-84a2-7cd0f532f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05416fd3-ccdf-4069-bc43-57e7bd7a6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
