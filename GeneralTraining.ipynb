{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f5bd94-74fb-4588-b47b-273035eb2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from control_framework import *\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from temp_recorder import *\n",
    "\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "#device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6823f8c1-608a-4865-aeca-1458b0610761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4335c011-7512-4fde-bf66-16bdb5591fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fae2e8cf0e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4245931c-72c7-470f-9e6e-2f6de6e1c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(torch_img):\n",
    "    clean = torch_img.detach().cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c25d90-1bdd-46b4-ae86-a6e8170070d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain = EnhancedAgentBrain()\n",
    "brain.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_batch10000.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v1_batch16600.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v1_batch1199.pth'\n",
    "fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "brain.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "\n",
    "brain.load_state_dict(torch.load(fname, weights_only=True, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6a4389-e113-4dc7-bf96-7538570644e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(brain.parameters(), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c522ad35-e749-4b70-a282-f4489bd4069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should possibly also include mem_enc? Should just be gen_optimizer? \n",
    "# ONLY use this in sessions where this is the only optimizer.\n",
    "# General optimizer gets messed up if this is used\n",
    "text_optimizer = optim.Adam(list(brain.text_enc.parameters()) + list(brain.text_dec.parameters()), lr=0.00001, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dedeb05-aeef-4cc0-8c32-41e4c74f92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to randomize the order in which the tasks are trained\n",
    "class ReusableBuffer:\n",
    "    def __init__(self, L, repetitions):\n",
    "        self.L = []\n",
    "        self.true_inds = []\n",
    "        for i in range(len(L)):\n",
    "            for j in range(repetitions[i]):\n",
    "                self.L.append(L[i])\n",
    "                self.true_inds.append(i)\n",
    "        self.inds = list(range(len(self.L))) # could be longer or shorter than input L\n",
    "\n",
    "    def draw(self, ind):\n",
    "        return self.L[ind]\n",
    "\n",
    "    def random_draw(self):\n",
    "        ind_ind = random.randint(0, len(self.inds)-1)\n",
    "        ind = self.inds[ind_ind]\n",
    "        if ind_ind == (len(self.inds) - 1):\n",
    "            self.inds = self.inds[:-1]\n",
    "        else:\n",
    "            self.inds = self.inds[:ind_ind] + self.inds[ind_ind + 1:]\n",
    "        if len(self.inds) == 0:\n",
    "            self.inds = list(range(len(self.L)))\n",
    "        return self.L[ind], ind, self.true_inds[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8113bf0-e27a-4964-b993-1511284e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first task (and really anywhere I want to not reset between tasks)\n",
    "# make sure the batch size matches\n",
    "# add further functions in the firs list, and add their repetition number to the second list\n",
    "rb = ReusableBuffer([(arrow_task_batch, gen_optimizer, 16), \\\n",
    "                     (qa_task_batch, gen_optimizer, 16), \\\n",
    "                     (control_batch, gen_optimizer, 16)], \\\n",
    "                    [3, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0bf3cd-1207-4726-8983-3708c97ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fcade85-fe40-4a68-b137-4c94c737614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_mins = [1000.0, 1000.0, 1000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6fc04a-8056-41bd-85c0-7e60970a13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88167a4b-dd8d-43e6-b4ae-8f77c51adaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62f64af-1ff3-4f80-804b-17b8488af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# I'll find the place that causes the 'non in-place resize later; for now, I don't want to clutter the results'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d1940-ba13-4da2-bb5f-b1af6d45d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, task 0, task batch_num 0\n",
      "\n",
      "batch 1, task 0, task batch_num 1\n",
      "\n",
      "batch 2, task 0, task batch_num 2\n",
      "\n",
      "batch 3, task 2, task batch_num 0\n",
      "\n",
      "batch 4, task 0, task batch_num 3\n",
      "\n",
      "batch 5, task 0, task batch_num 4\n",
      "\n",
      "batch 6, task 2, task batch_num 1\n",
      "\n",
      "batch 7, task 0, task batch_num 5\n",
      "\n",
      "batch 8, task 2, task batch_num 2\n",
      "\n",
      "batch 9, task 0, task batch_num 6\n",
      "\n",
      "Total loss: 0.004619971849024296; that's 0.0035834205336868763 task and 0.0004004405636806041 recon and 3.180553436279297 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004915425032377243\n",
      "\n",
      "Total loss: 0.00477561354637146; that's 0.0036068707704544067 task and 0.00041783705819398165 recon and 3.754530429840088 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004791974548716098\n",
      "\n",
      "Total recon loss: 0.0035838782787323; that's 2.7180943489074707 text and 0.0008657838916406035 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004668907541781664\n",
      "\n",
      "Total loss: 0.005890343338251114; that's 0.004409030079841614 task and 0.000421319215092808 recon and 5.299971103668213 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004993205426726491\n",
      "\n",
      "Total loss: 0.004234375432133675; that's 0.0028969866689294577 task and 0.0004129076551180333 recon and 4.622406005859375 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004966374945361167\n",
      "\n",
      "Total loss: 0.005108792334794998; that's 0.0036885421723127365 task and 0.0003938496229238808 recon and 5.132003307342529 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004965654895640909\n",
      "\n",
      "Total loss: 0.005445440765470266; that's 0.004217054694890976 task and 0.0004066824330948293 recon and 4.108517646789551 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.005045505731832236\n",
      "\n",
      "Total recon loss: 0.004039154388010502; that's 3.3162841796875 text and 0.0007228699396364391 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0041231536981649695\n",
      "\n",
      "Total loss: 0.005066983867436647; that's 0.0035744463093578815 task and 0.000447257625637576 recon and 5.226400375366211 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00498792267870158\n",
      "\n",
      "Total loss: 0.005391720682382584; that's 0.003908870741724968 task and 0.00040264628478325903 recon and 5.401018142700195 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004939528158865869\n",
      "\n",
      "Total loss: 0.005467860959470272; that's 0.003969921264797449 task and 0.0004023733490612358 recon and 5.477832317352295 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.005050343009643256\n",
      "\n",
      "Total recon loss: 0.0053703621961176395; that's 4.692780494689941 text and 0.0006775813526473939 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005092644889373332\n",
      "\n",
      "Total loss: 0.005229289177805185; that's 0.004019744228571653 task and 0.0004014002624899149 recon and 4.0407233238220215 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004866425213404\n",
      "\n",
      "Total loss: 0.004023365676403046; that's 0.0026793864089995623 task and 0.0004022570210509002 recon and 4.7086100578308105 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004998421538621187\n",
      "\n",
      "Total recon loss: 0.004863436333835125; that's 4.164999008178711 text and 0.0006984368665143847 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005109530370682478\n",
      "\n",
      "Total loss: 0.005172692239284515; that's 0.0038149708416312933 task and 0.00039757831837050617 recon and 4.800717830657959 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.005052267936989665\n",
      "\n",
      "Total loss: 0.00554246548563242; that's 0.004260567016899586 task and 0.00038686851621605456 recon and 4.4751482009887695 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004902729135937989\n",
      "\n",
      "Total loss: 0.004625589586794376; that's 0.003394581377506256 task and 0.0003931787796318531 recon and 4.189146518707275 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004950155424885452\n",
      "\n",
      "Total recon loss: 0.00520533649250865; that's 4.4643683433532715 text and 0.0007409682148136199 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005077713574282825\n",
      "\n",
      "Total loss: 0.004741678014397621; that's 0.00347967934794724 task and 0.0003890542720910162 recon and 4.3647236824035645 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004885148017201572\n",
      "\n",
      "Total loss: 0.00483910646289587; that's 0.0035310841631144285 task and 0.00039474215009249747 recon and 4.566400051116943 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004832912820857019\n",
      "\n",
      "Total loss: 0.003738709492608905; that's 0.0024209367111325264 task and 0.00038851224235258996 recon and 4.646303176879883 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004862623361404985\n",
      "\n",
      "Total recon loss: 0.004266674164682627; that's 3.35494065284729 text and 0.0009117334266193211 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0050222486583516\n",
      "\n",
      "Total loss: 0.004389500245451927; that's 0.0031431373208761215 task and 0.0003798190737143159 recon and 4.332719802856445 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004933623117394746\n",
      "\n",
      "Total loss: 0.003756868187338114; that's 0.0024398949462920427 task and 0.0003913013788405806 recon and 4.628358840942383 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0049689080193638805\n",
      "\n",
      "Total loss: 0.005447314120829105; that's 0.004058129154145718 task and 0.0003863211895804852 recon and 5.014320373535156 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004925780990161002\n",
      "\n",
      "Total loss: 0.004894542973488569; that's 0.003648032434284687 task and 0.000386205647373572 recon and 4.301525115966797 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0049936034996062514\n",
      "\n",
      "Total recon loss: 0.005620453506708145; that's 4.929129600524902 text and 0.0006913237739354372 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004878667311277241\n",
      "\n",
      "Total loss: 0.004720072261989117; that's 0.0035698190331459045 task and 0.00037845943006686866 recon and 3.85896897315979 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004891499523073435\n",
      "\n",
      "Total loss: 0.004689609631896019; that's 0.0031812177039682865 task and 0.0003813036310020834 recon and 5.635441780090332 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004937816439196468\n",
      "\n",
      "Total recon loss: 0.005572620779275894; that's 4.941155433654785 text and 0.0006314651691354811 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004984640735201538\n",
      "\n",
      "Total loss: 0.004808097146451473; that's 0.003723820438608527 task and 0.0003854481619782746 recon and 3.49414324760437 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004899445176124573\n",
      "\n",
      "Total loss: 0.004709349945187569; that's 0.003605693345889449 task and 0.0003804680600296706 recon and 3.615943431854248 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004797462308779359\n",
      "\n",
      "Total loss: 0.004979787394404411; that's 0.003702311310917139 task and 0.00038835417944937944 recon and 4.445610523223877 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004810459185391664\n",
      "\n",
      "Total recon loss: 0.005632065236568451; that's 4.969008922576904 text and 0.0006630562129430473 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0052452673995867375\n",
      "\n",
      "Total loss: 0.005773384589701891; that's 0.004452456254512072 task and 0.0003858538984786719 recon and 4.675372123718262 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004940330004319549\n",
      "\n",
      "Total loss: 0.004811840131878853; that's 0.003327104961499572 task and 0.0003776009543798864 recon and 5.535670280456543 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004953336813487113\n",
      "\n",
      "Total loss: 0.004939185921102762; that's 0.0037155020982027054 task and 0.00038828252581879497 recon and 4.177006721496582 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0048751899786293506\n",
      "\n",
      "Total recon loss: 0.0043299077078700066; that's 3.548691511154175 text and 0.000781215843744576 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004546656357124448\n",
      "\n",
      "Total loss: 0.0056019555777311325; that's 0.0043306672014296055 task and 0.00040315010119229555 recon and 4.340691566467285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004907565950416029\n",
      "\n",
      "Total loss: 0.004880282562226057; that's 0.0036459434777498245 task and 0.0003669045399874449 recon and 4.33717155456543 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004823488758411258\n",
      "\n",
      "Total loss: 0.004190781619399786; that's 0.0031226202845573425 task and 0.00038311700336635113 recon and 3.4252207279205322 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0048352224752306935\n",
      "\n",
      "Total recon loss: 0.005182175897061825; that's 4.345308780670166 text and 0.0008368671406060457 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005147888388019055\n",
      "\n",
      "Total loss: 0.004987041931599379; that's 0.003835542593151331 task and 0.0003785936569329351 recon and 3.864527463912964 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0046175933349877595\n",
      "\n",
      "Total loss: 0.004413096234202385; that's 0.0031246331054717302 task and 0.00038449096609838307 recon and 4.519862174987793 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0048111431230790916\n",
      "\n",
      "Total loss: 0.00427952129393816; that's 0.003060820046812296 task and 0.00036786386044695973 recon and 4.254188060760498 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004847546895034611\n",
      "\n",
      "Total recon loss: 0.004935900680720806; that's 4.292331218719482 text and 0.0006435691029764712 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005282148392871023\n",
      "\n",
      "Total loss: 0.005389031022787094; that's 0.0039034481160342693 task and 0.00038216906250454485 recon and 5.517067909240723 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004934858032502234\n",
      "\n",
      "Total loss: 0.004400423727929592; that's 0.003220659913495183 task and 0.0003663879178930074 recon and 4.066880226135254 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0049341775616630916\n",
      "\n",
      "Total loss: 0.004472116939723492; that's 0.00324849970638752 task and 0.0003650553117040545 recon and 4.292810440063477 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004722508452832699\n",
      "\n",
      "Total recon loss: 0.00485185207799077; that's 3.8390374183654785 text and 0.0010128144640475512 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00491937650134787\n",
      "\n",
      "Total loss: 0.004797760397195816; that's 0.003748029936105013 task and 0.00038191420026123524 recon and 3.3390815258026123 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004857969554141164\n",
      "\n",
      "Total loss: 0.004583549685776234; that's 0.003384736366569996 task and 0.0003720503009390086 recon and 4.133813858032227 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004763860492967069\n",
      "\n",
      "Total loss: 0.00427787471562624; that's 0.003109272103756666 task and 0.0003611414285842329 recon and 4.0373053550720215 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004737898912280798\n",
      "\n",
      "Total recon loss: 0.004077522549778223; that's 3.445305347442627 text and 0.0006322169792838395 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004647195436991752\n",
      "\n",
      "Total loss: 0.004167168866842985; that's 0.0030607411172240973 task and 0.000380255893105641 recon and 3.630859136581421 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004683356052264571\n",
      "\n",
      "Total loss: 0.005119691602885723; that's 0.00397845171391964 task and 0.00036859410465694964 recon and 3.863227605819702 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0046466379053890705\n",
      "\n",
      "Total loss: 0.004908371716737747; that's 0.0037278812378644943 task and 0.0003662350936792791 recon and 4.071277141571045 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004650313747115433\n",
      "\n",
      "Total recon loss: 0.0046949731186032295; that's 3.957836151123047 text and 0.0007371368119493127 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004774056151509285\n",
      "\n",
      "Total loss: 0.004061835817992687; that's 0.00290343863889575 task and 0.0003770668990910053 recon and 3.906651735305786 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004528538642916829\n",
      "\n",
      "Total loss: 0.004569117911159992; that's 0.0034314303193241358 task and 0.00037466632784344256 recon and 3.8151051998138428 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004563966242130846\n",
      "\n",
      "Total loss: 0.004933769814670086; that's 0.0036892059724777937 task and 0.0003675228508654982 recon and 4.385204315185547 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004526979697402567\n",
      "\n",
      "Total loss: 0.004656719509512186; that's 0.0036038144025951624 task and 0.00037524092476814985 recon and 3.388321876525879 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004497733120806515\n",
      "\n",
      "Total recon loss: 0.00472152978181839; that's 4.150050640106201 text and 0.0005714789149351418 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0047406566445715725\n",
      "\n",
      "Total loss: 0.004648515023291111; that's 0.003409041790291667 task and 0.0003828556218650192 recon and 4.283089637756348 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004559381841681897\n",
      "\n",
      "Total loss: 0.005296608433127403; that's 0.003921550698578358 task and 0.00036914783413521945 recon and 5.029548168182373 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004664481112267822\n",
      "\n",
      "Total recon loss: 0.0049014706164598465; that's 4.091724395751953 text and 0.0008097459212876856 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0046950913639739154\n",
      "\n",
      "Total loss: 0.005418802611529827; that's 0.0041236779652535915 task and 0.0003762712876778096 recon and 4.59426736831665 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0046353357052430514\n",
      "\n",
      "Total loss: 0.004986027721315622; that's 0.003704540431499481 task and 0.0003723208501469344 recon and 4.545831203460693 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004624032992869616\n",
      "\n",
      "Total loss: 0.0041957213543355465; that's 0.0029274593107402325 task and 0.00035855453461408615 recon and 4.5485382080078125 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00465281852055341\n",
      "\n",
      "Total recon loss: 0.004345929715782404; that's 3.597238063812256 text and 0.0007486912654712796 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00495597611181438\n",
      "\n",
      "Total loss: 0.004161547869443893; that's 0.002537811640650034 task and 0.00037705665454268456 recon and 6.233398914337158 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004608298391103744\n",
      "\n",
      "Total loss: 0.005147736519575119; that's 0.003775650868192315 task and 0.0003658679488580674 recon and 5.031087398529053 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0045885878754779695\n",
      "\n",
      "Total loss: 0.005436678417026997; that's 0.004029721021652222 task and 0.00036429960164241493 recon and 5.213289260864258 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004563892129808665\n",
      "\n",
      "Total recon loss: 0.004650415387004614; that's 3.9846599102020264 text and 0.0006657554185949266 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004915832553524524\n",
      "\n",
      "Total loss: 0.0050834608264267445; that's 0.003685066709294915 task and 0.00035901134833693504 recon and 5.196913242340088 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004511708957143128\n",
      "\n",
      "Total loss: 0.005322203040122986; that's 0.0039061533752828836 task and 0.00036355183692649007 recon and 5.26248836517334 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004525864887982607\n",
      "\n",
      "Total loss: 0.004985154140740633; that's 0.003479632781818509 task and 0.00038277945714071393 recon and 5.613710403442383 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004531355956569314\n",
      "\n",
      "Total recon loss: 0.0052358428947627544; that's 4.549509048461914 text and 0.0006863336311653256 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004834873895160854\n",
      "\n",
      "Total loss: 0.004518802743405104; that's 0.003119417931884527 task and 0.000354459451045841 recon and 5.224626541137695 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004621914434246719\n",
      "\n",
      "Total loss: 0.004308175295591354; that's 0.0033055320382118225 task and 0.00037935844738967717 recon and 3.1164238452911377 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004270370078738778\n",
      "\n",
      "Total loss: 0.004221141338348389; that's 0.0030603406485170126 task and 0.00037508015520870686 recon and 3.928603410720825 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004315332781989128\n",
      "\n",
      "Total recon loss: 0.004603976383805275; that's 3.7813687324523926 text and 0.0008226076606661081 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004986644138116389\n",
      "\n",
      "Total loss: 0.004597880411893129; that's 0.0032457695342600346 task and 0.00037527704262174666 recon and 4.884169578552246 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004432546435855329\n",
      "\n",
      "Total loss: 0.003984036855399609; that's 0.002793246414512396 task and 0.0003725464048329741 recon and 4.091219425201416 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004339502048678696\n",
      "\n",
      "Total loss: 0.004359640181064606; that's 0.0032321985345333815 task and 0.000373671151464805 recon and 3.768853187561035 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004207913083955645\n",
      "\n",
      "Total recon loss: 0.003909402526915073; that's 3.309950590133667 text and 0.000599451654125005 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004971066911239177\n",
      "\n",
      "Total loss: 0.0035802191123366356; that's 0.0025370153598487377 task and 0.000369646237231791 recon and 3.367788314819336 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004162489525042474\n",
      "\n",
      "Total loss: 0.0037687215954065323; that's 0.002642159117385745 task and 0.00038938329089432955 recon and 3.6858959197998047 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004177574731875211\n",
      "\n",
      "Total loss: 0.00403200788423419; that's 0.0029766298830509186 task and 0.00036128517240285873 recon and 3.4704651832580566 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0041753815137781205\n",
      "\n",
      "Total recon loss: 0.005894314032047987; that's 5.148484230041504 text and 0.0007458298350684345 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005227136444300413\n",
      "\n",
      "Total loss: 0.004364709369838238; that's 0.0032799537293612957 task and 0.00037582378718070686 recon and 3.544658899307251 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004028180399909615\n",
      "\n",
      "Total loss: 0.0039635710418224335; that's 0.0029328817036002874 task and 0.00035757850855588913 recon and 3.3655545711517334 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004122703936882317\n",
      "\n",
      "Total loss: 0.004137478768825531; that's 0.003009275533258915 task and 0.0003689549630507827 recon and 3.7962398529052734 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004132525259628892\n",
      "\n",
      "Total loss: 0.0043454766273498535; that's 0.0031087948009371758 task and 0.00036862929118797183 recon and 4.340263366699219 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004016066768672317\n",
      "\n",
      "Total recon loss: 0.003683340037241578; that's 3.0388851165771484 text and 0.0006444547907449305 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00491888452321291\n",
      "\n",
      "Total loss: 0.0037241734098643064; that's 0.0023444436956197023 task and 0.00037030995008535683 recon and 5.0470991134643555 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004013822239357978\n",
      "\n",
      "Total loss: 0.0038373786956071854; that's 0.002650923328474164 task and 0.00036671923589892685 recon and 4.0986809730529785 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004062944252509624\n",
      "\n",
      "Total recon loss: 0.003634215332567692; that's 2.983609676361084 text and 0.000650605361443013 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0043975507095456124\n",
      "\n",
      "Total loss: 0.00417654775083065; that's 0.0029794510919600725 task and 0.0003647995472420007 recon and 4.1614861488342285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0040385516546666625\n",
      "\n",
      "Total loss: 0.0038183187134563923; that's 0.0026731216348707676 task and 0.00036061849095858634 recon and 3.9228925704956055 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0040605078474618495\n",
      "\n",
      "Total loss: 0.004318447317928076; that's 0.0031468968372792006 task and 0.00037383922608569264 recon and 3.9885573387145996 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003903625188395381\n",
      "\n",
      "Total recon loss: 0.004793376196175814; that's 4.061314582824707 text and 0.0007320612785406411 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004835641826502979\n",
      "\n",
      "Total loss: 0.004303932189941406; that's 0.003188036847859621 task and 0.0003707949654199183 recon and 3.7255024909973145 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038827871065586807\n",
      "\n",
      "Total loss: 0.004345862194895744; that's 0.0031736516393721104 task and 0.00036311609437689185 recon and 4.04547119140625 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003897857896517962\n",
      "\n",
      "Total loss: 0.004425679799169302; that's 0.003291063942015171 task and 0.00037578842602670193 recon and 3.794138193130493 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003911567307077348\n",
      "\n",
      "Total recon loss: 0.0050005316734313965; that's 4.147438049316406 text and 0.0008530931081622839 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004789432450197637\n",
      "\n",
      "Total loss: 0.003638838417828083; that's 0.0025836951099336147 task and 0.00037058486486785114 recon and 3.4227921962738037 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004023581424262374\n",
      "\n",
      "Total loss: 0.004320050124078989; that's 0.0029022551607340574 task and 0.0003688572614919394 recon and 5.244688987731934 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.004034364898689091\n",
      "\n",
      "Total loss: 0.0040873014368116856; that's 0.0027523995377123356 task and 0.0003700937668327242 recon and 4.824039459228516 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003943261606618762\n",
      "\n",
      "Total recon loss: 0.005282313097268343; that's 4.6095099449157715 text and 0.0006728027947247028 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004777520417701453\n",
      "\n",
      "Total loss: 0.0031013917177915573; that's 0.0020042695105075836 task and 0.0003699912631418556 recon and 3.635655403137207 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003830105043016374\n",
      "\n",
      "Total loss: 0.0041900319047272205; that's 0.0029282316099852324 task and 0.00036811508471146226 recon and 4.468425750732422 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00392474360531196\n",
      "\n",
      "Total loss: 0.004153866786509752; that's 0.00265324953943491 task and 0.00036808158620260656 recon and 5.662678241729736 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00412724201567471\n",
      "\n",
      "Total recon loss: 0.0044417669996619225; that's 3.7212235927581787 text and 0.000720543204806745 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005140514897648245\n",
      "\n",
      "Total loss: 0.004256796557456255; that's 0.002903773682191968 task and 0.00037315397639758885 recon and 4.899345397949219 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0041833901172503825\n",
      "\n",
      "Total loss: 0.003946205135434866; that's 0.002659471705555916 task and 0.00037538178730756044 recon and 4.556759834289551 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00392665145220235\n",
      "\n",
      "Total loss: 0.0041878847405314445; that's 0.002678146818652749 task and 0.0003589155385270715 recon and 5.754110336303711 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003878785928245634\n",
      "\n",
      "Total recon loss: 0.004670191090553999; that's 3.820570707321167 text and 0.0008496202062815428 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005050760337617248\n",
      "\n",
      "Total loss: 0.004288128111511469; that's 0.003013772889971733 task and 0.0003648814745247364 recon and 4.54736852645874 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003952417310792953\n",
      "\n",
      "Total loss: 0.003838623408228159; that's 0.0025521174538880587 task and 0.00037012732354924083 recon and 4.581892490386963 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038377273571677506\n",
      "\n",
      "Total loss: 0.0038001632783561945; that's 0.0025220580864697695 task and 0.00036747445119544864 recon and 4.5531535148620605 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038448698143474756\n",
      "\n",
      "Total loss: 0.004063997883349657; that's 0.002859534230083227 task and 0.0003674580657389015 recon and 4.185029029846191 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037883909116499127\n",
      "\n",
      "Total recon loss: 0.004265887662768364; that's 3.544428586959839 text and 0.0007214589859358966 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004935374467168003\n",
      "\n",
      "Total loss: 0.004303285386413336; that's 0.002894066274166107 task and 0.0003729467571247369 recon and 5.181362152099609 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003860318122897297\n",
      "\n",
      "Total loss: 0.004428305197507143; that's 0.003021277254447341 task and 0.0003594447625800967 recon and 5.237915992736816 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003828388263937086\n",
      "\n",
      "Total recon loss: 0.0037232846952974796; that's 3.020134210586548 text and 0.0007031504064798355 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004534500758163631\n",
      "\n",
      "Total loss: 0.003983061295002699; that's 0.0027051151264458895 task and 0.0003697718493640423 recon and 4.5408711433410645 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00387559117982164\n",
      "\n",
      "Total loss: 0.004167675040662289; that's 0.002781884279102087 task and 0.0003571224515326321 recon and 5.1433424949646 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003921359539963305\n",
      "\n",
      "Total loss: 0.003985355142503977; that's 0.0027134327683597803 task and 0.00038000824861228466 recon and 4.45957088470459 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003859114279039204\n",
      "\n",
      "Total recon loss: 0.004130182787775993; that's 3.4546923637390137 text and 0.0006754905334673822 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00406586887082085\n",
      "\n",
      "Total loss: 0.003780532628297806; that's 0.0025401741731911898 task and 0.00035138134262524545 recon and 4.444886684417725 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038675069715827706\n",
      "\n",
      "Total loss: 0.003970752004534006; that's 0.0026354785077273846 task and 0.0003685987030621618 recon and 4.8333740234375 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037751693883910776\n",
      "\n",
      "Total loss: 0.003951151389628649; that's 0.0026378321927040815 task and 0.0003590242995414883 recon and 4.7714738845825195 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003799217096529901\n",
      "\n",
      "Total loss: 0.003909294959157705; that's 0.002627125708386302 task and 0.0003671969461720437 recon and 4.574861526489258 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003762479848228395\n",
      "\n",
      "Total recon loss: 0.004943283274769783; that's 4.032516956329346 text and 0.0009107658406719565 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005025904611684382\n",
      "\n",
      "Total loss: 0.0035564154386520386; that's 0.0023742143530398607 task and 0.00037326314486563206 recon and 4.044689178466797 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037772208149544893\n",
      "\n",
      "Total loss: 0.0037009255029261112; that's 0.0024063833989202976 task and 0.00035649427445605397 recon and 4.690239429473877 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038506099209189414\n",
      "\n",
      "Total recon loss: 0.004288516007363796; that's 3.73598575592041 text and 0.0005525302258320153 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.00501950646750629\n",
      "\n",
      "Total loss: 0.003397353459149599; that's 0.002218523295596242 task and 0.0003526850196067244 recon and 4.130725860595703 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037296592141501605\n",
      "\n",
      "Total loss: 0.0036222469061613083; that's 0.002337054582312703 task and 0.00036088679917156696 recon and 4.621528148651123 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003803257425315678\n",
      "\n",
      "Total loss: 0.00384454894810915; that's 0.002535496838390827 task and 0.0003576802264433354 recon and 4.756859302520752 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037652175151742994\n",
      "\n",
      "Total recon loss: 0.0055877394042909145; that's 5.027087211608887 text and 0.0005606518243439496 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005147706007119268\n",
      "\n",
      "Total loss: 0.003996598068624735; that's 0.002648310037329793 task and 0.0003650769649539143 recon and 4.916055202484131 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0038278424949385226\n",
      "\n",
      "Total loss: 0.0030018144752830267; that's 0.001710125943645835 task and 0.0003537556913215667 recon and 4.689664363861084 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0036842141789384187\n",
      "\n",
      "Total loss: 0.003919865470379591; that's 0.002664146712049842 task and 0.00036025879671797156 recon and 4.47730016708374 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0037331682606600225\n",
      "\n",
      "Total recon loss: 0.004247600678354502; that's 3.614412784576416 text and 0.0006331877666525543 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005153263849206269\n",
      "\n",
      "Total loss: 0.004477207083255053; that's 0.0031624361872673035 task and 0.0003459926228970289 recon and 4.843891620635986 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.00355309150647372\n",
      "\n",
      "Total loss: 0.003138243919238448; that's 0.0019223244162276387 task and 0.0003692080790642649 recon and 4.23355770111084 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0036462788726203143\n",
      "\n",
      "Total loss: 0.003945881966501474; that's 0.0025521053466945887 task and 0.0003569561813492328 recon and 5.1841020584106445 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0036597018549218772\n",
      "\n",
      "Total recon loss: 0.004527370911091566; that's 3.7932772636413574 text and 0.0007340933079831302 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004392758959438652\n",
      "\n",
      "Total loss: 0.0036937231197953224; that's 0.002598288469016552 task and 0.0003607587132137269 recon and 3.6733791828155518 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003612502289470285\n",
      "\n",
      "Total loss: 0.0035055612679570913; that's 0.0023097905796021223 task and 0.00035478666541166604 recon and 4.204920291900635 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003645224920473993\n",
      "\n",
      "Total loss: 0.004416929557919502; that's 0.003228941233828664 task and 0.0003619807248469442 recon and 4.130038261413574 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0035919215832836927\n",
      "\n",
      "Total loss: 0.0034876810386776924; that's 0.002310583833605051 task and 0.00036475242814049125 recon and 4.0617241859436035 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034008269151672723\n",
      "\n",
      "Total recon loss: 0.00400076899677515; that's 3.0305254459381104 text and 0.0009702435927465558 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004959873715415597\n",
      "\n",
      "Total loss: 0.0033657988533377647; that's 0.0021835947409272194 task and 0.00034837849671021104 recon and 4.169128894805908 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033535769348964095\n",
      "\n",
      "Total loss: 0.0035777033772319555; that's 0.0026317022275179625 task and 0.0003609101986512542 recon and 2.9254555702209473 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034255028748884798\n",
      "\n",
      "Total loss: 0.0033692389260977507; that's 0.0022539629135280848 task and 0.00037222891114652157 recon and 3.715235471725464 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003333803212735802\n",
      "\n",
      "Total recon loss: 0.004270078148692846; that's 3.5947492122650146 text and 0.0006753289490006864 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0047604923020116985\n",
      "\n",
      "Total loss: 0.003005536738783121; that's 0.0017886741552501917 task and 0.0003513092815410346 recon and 4.327765941619873 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003506978519726545\n",
      "\n",
      "Total loss: 0.003416013903915882; that's 0.0023420017678290606 task and 0.00033964094473049045 recon and 3.6718552112579346 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034870575042441486\n",
      "\n",
      "Total recon loss: 0.004503837786614895; that's 3.4645838737487793 text and 0.0010392540134489536 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004532068716362119\n",
      "\n",
      "Total loss: 0.003380358684808016; that's 0.0023785310331732035 task and 0.0003504455089569092 recon and 3.2569103240966797 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033828263427130877\n",
      "\n",
      "Total loss: 0.003412873949855566; that's 0.0024118637666106224 task and 0.00035169863258488476 recon and 3.2465577125549316 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003395494413562119\n",
      "\n",
      "Total loss: 0.004007363226264715; that's 0.0026197985280305147 task and 0.0003505665808916092 recon and 5.184990406036377 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003480015767272562\n",
      "\n",
      "Total loss: 0.0036672228015959263; that's 0.002221545670181513 task and 0.0003598713083192706 recon and 5.429028511047363 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0035245141759514807\n",
      "\n",
      "Total recon loss: 0.003933269064873457; that's 3.014281988143921 text and 0.0009189870324917138 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004445536788553\n",
      "\n",
      "Total loss: 0.0037180299405008554; that's 0.0023443906102329493 task and 0.00035975032369606197 recon and 5.0694451332092285 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003506936028134078\n",
      "\n",
      "Total loss: 0.003430458717048168; that's 0.0021654744632542133 task and 0.00035103820846416056 recon and 4.569730281829834 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0035026289476081727\n",
      "\n",
      "Total recon loss: 0.004099672194570303; that's 3.342097520828247 text and 0.0007575747440569103 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004426171833183616\n",
      "\n",
      "Total loss: 0.0034136527683585882; that's 0.002271974226459861 task and 0.0003558394673746079 recon and 3.9291951656341553 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003466747794300318\n",
      "\n",
      "Total loss: 0.0029091115575283766; that's 0.0015401121927425265 task and 0.00035616435343399644 recon and 5.064175128936768 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034994049044325946\n",
      "\n",
      "Total loss: 0.002928437665104866; that's 0.00166359543800354 task and 0.0003535120631568134 recon and 4.556650638580322 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034648981946520507\n",
      "\n",
      "Total recon loss: 0.004394576884806156; that's 3.5778205394744873 text and 0.00081675621913746 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004217595579102635\n",
      "\n",
      "Total loss: 0.0030615527648478746; that's 0.0018718825886026025 task and 0.00035193958319723606 recon and 4.188652515411377 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003468637294135988\n",
      "\n",
      "Total loss: 0.0038750777021050453; that's 0.002681252546608448 task and 0.00034794944804161787 recon and 4.229377746582031 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034429656690917907\n",
      "\n",
      "Total loss: 0.003544073086231947; that's 0.0021435392554849386 task and 0.00035971272154711187 recon and 5.20410680770874 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003512822741176933\n",
      "\n",
      "Total recon loss: 0.004326488822698593; that's 3.4817676544189453 text and 0.000844721042085439 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004043580272700638\n",
      "\n",
      "Total loss: 0.0035422369837760925; that's 0.0023548228200525045 task and 0.0003490250965114683 recon and 4.191946029663086 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0035314537631347775\n",
      "\n",
      "Total loss: 0.003420763649046421; that's 0.002288953633978963 task and 0.00034845914342440665 recon and 3.9167535305023193 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034559369436465205\n",
      "\n",
      "Total loss: 0.0037492578849196434; that's 0.0023365227971225977 task and 0.0003609820851124823 recon and 5.258764743804932 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034115075040608645\n",
      "\n",
      "Total recon loss: 0.004281697329133749; that's 3.3589963912963867 text and 0.0009227009722962976 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004104898893274367\n",
      "\n",
      "Total loss: 0.003403285052627325; that's 0.001971017336472869 task and 0.0003503604675643146 recon and 5.409536361694336 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003458658952731639\n",
      "\n",
      "Total loss: 0.003211669623851776; that's 0.0020113657228648663 task and 0.00034850012161768973 recon and 4.259018421173096 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034754799609072505\n",
      "\n",
      "Total loss: 0.003630725434049964; that's 0.002396809868514538 task and 0.00035024870885536075 recon and 4.418334007263184 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033949498645961286\n",
      "\n",
      "Total loss: 0.003414110979065299; that's 0.001902407850138843 task and 0.0003450986696407199 recon and 5.833022594451904 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0034242959250696003\n",
      "\n",
      "Total recon loss: 0.004803982097655535; that's 3.983503818511963 text and 0.0008204778423532844 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004146615331992507\n",
      "\n",
      "Total loss: 0.00311806658282876; that's 0.0019749412313103676 task and 0.0003464989713393152 recon and 3.9831318855285645 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003363288016989827\n",
      "\n",
      "Total loss: 0.0035849702544510365; that's 0.0022631529718637466 task and 0.00036272755824029446 recon and 4.795448303222656 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033927487139590084\n",
      "\n",
      "Total recon loss: 0.003423559246584773; that's 2.673626661300659 text and 0.0007499323692172766 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0038335199700668455\n",
      "\n",
      "Total loss: 0.0035247341729700565; that's 0.002172901062294841 task and 0.0003456423874013126 recon and 5.0309529304504395 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003387712659314275\n",
      "\n",
      "Total loss: 0.003618788905441761; that's 0.002270670374855399 task and 0.0003601669450290501 recon and 4.939758777618408 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003391095898114145\n",
      "\n",
      "Total loss: 0.002925971057265997; that's 0.0016833650879561901 task and 0.0003424971073400229 recon and 4.500543594360352 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033348333509638905\n",
      "\n",
      "Total recon loss: 0.004241708666086197; that's 3.4435250759124756 text and 0.0007981834933161736 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.004245931068435311\n",
      "\n",
      "Total loss: 0.0034157985355705023; that's 0.002283495618030429 task and 0.000356313306838274 recon and 3.879948377609253 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0032881611003540458\n",
      "\n",
      "Total loss: 0.002988803433254361; that's 0.0018302029930055141 task and 0.00035005222889594734 recon and 4.042741298675537 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003481462886556983\n",
      "\n",
      "Total loss: 0.0030976601410657167; that's 0.0021387760061770678 task and 0.0003401069843675941 recon and 3.093886613845825 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003340547285042703\n",
      "\n",
      "Total recon loss: 0.005344161298125982; that's 4.556093692779541 text and 0.0007880675257183611 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.0048345987638458605\n",
      "\n",
      "Total loss: 0.003256389405578375; that's 0.0018272630404680967 task and 0.00034104124642908573 recon and 5.440424919128418 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0032750107836909594\n",
      "\n",
      "Total loss: 0.003479116363450885; that's 0.002199314534664154 task and 0.00035069117438979447 recon and 4.645553112030029 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003386082537472248\n",
      "\n",
      "Total loss: 0.0034077251330018044; that's 0.0022583159152418375 task and 0.0003471980744507164 recon and 4.011056423187256 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0033459382061846554\n",
      "\n",
      "Total recon loss: 0.004756953101605177; that's 4.065254211425781 text and 0.0006916986894793808 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005032819951884449\n",
      "\n",
      "Total loss: 0.003734295256435871; that's 0.002517982153221965 task and 0.0003569373511709273 recon and 4.296879291534424 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003334301409777254\n",
      "\n",
      "Total loss: 0.0035963025875389576; that's 0.002584919799119234 task and 0.0003461771411821246 recon and 3.3260273933410645 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0032135790400207045\n",
      "\n",
      "Total loss: 0.003299369942396879; that's 0.002011619508266449 task and 0.00035753779229708016 recon and 4.651063919067383 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0032515812385827303\n",
      "\n",
      "Total recon loss: 0.004927645903080702; that's 4.220849514007568 text and 0.0007067961269058287 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005117365224286914\n",
      "\n",
      "Total loss: 0.0030678166076540947; that's 0.002013578312471509 task and 0.00036266824463382363 recon and 3.457850456237793 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003262786774430424\n",
      "\n",
      "Total loss: 0.003356311470270157; that's 0.0024635575246065855 task and 0.00035574473440647125 recon and 2.6850454807281494 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0031819937331601976\n",
      "\n",
      "Total loss: 0.002800275105983019; that's 0.0018367600860074162 task and 0.0003530470421537757 recon and 3.052339553833008 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003090931756887585\n",
      "\n",
      "Total recon loss: 0.004891702905297279; that's 4.0977253913879395 text and 0.0007939771749079227 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005117874497082084\n",
      "\n",
      "Total loss: 0.0031473126728087664; that's 0.002170391147956252 task and 0.0003459340368863195 recon and 3.154937982559204 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003115396921057254\n",
      "\n",
      "Total loss: 0.003337882226333022; that's 0.002121180761605501 task and 0.0003490382805466652 recon and 4.338315963745117 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003252551390323788\n",
      "\n",
      "Total loss: 0.003354010870680213; that's 0.002147867577150464 task and 0.00034080762998200953 recon and 4.3266777992248535 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003196118150372058\n",
      "\n",
      "Total recon loss: 0.006426742300391197; that's 5.624619007110596 text and 0.0008021232206374407 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005282208593562245\n",
      "\n",
      "Total loss: 0.0022936235181987286; that's 0.0014067240990698338 task and 0.0003454440739005804 recon and 2.7072768211364746 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0030331289116293193\n",
      "\n",
      "Total loss: 0.0029267636127769947; that's 0.001765495864674449 task and 0.00036036991514265537 recon and 4.004489898681641 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003050186629407108\n",
      "\n",
      "Total loss: 0.003409328870475292; that's 0.0020962173584848642 task and 0.00035857525654137135 recon and 4.772681713104248 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.003235844117589295\n",
      "\n",
      "Total loss: 0.003091220511123538; that's 0.0022115116007626057 task and 0.000349650887073949 recon and 2.6502907276153564 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0032317311549559234\n",
      "\n",
      "Total recon loss: 0.005392976570874453; that's 4.4835429191589355 text and 0.0009094331762753427 img\n",
      "\n",
      "\n",
      "Average total loss for task 2, last 100 batches: 0.005030446206219495\n",
      "\n",
      "Total loss: 0.0024509471841156483; that's 0.0016028749523684382 task and 0.0003396914107725024 recon and 2.5419037342071533 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.0029211683687753974\n",
      "\n",
      "Total loss: 0.0029764920473098755; that's 0.002055032877251506 task and 0.00034538115141913295 recon and 2.8803892135620117 total text\n",
      "\n",
      "\n",
      "Average total loss for task 0, last 100 batches: 0.002979598743841052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_batches = 10000*100#6250*32\n",
    "\n",
    "for b in range(total_batches):\n",
    "    secs_to_cool = monitor_stage(device)\n",
    "    if secs_to_cool > 0:\n",
    "        print(f\"Had to cool device for {secs_to_cool} seconds\\n\")\n",
    "        \n",
    "    triplet, _, ind = rb.random_draw()\n",
    "    func, opt, batch_size = triplet\n",
    "    \n",
    "    batch_num = batches[ind]\n",
    "    batches[ind] += 1\n",
    "\n",
    "    #reset_model = True #default option; only transfer memory within the task files\n",
    "    reset_model = (b % 3 == 2)\n",
    "\n",
    "    printing = ((batch_num % 100) == 99)\n",
    "    full_results = func(batch_size, brain, optimizer=opt, batch_num=batch_num, compute_grad=True, random_order=True, model_eval=False, reset_model=reset_model, printing=printing, training=True)\n",
    "    L = full_results[0] # no need to look into the detailed loss report\n",
    "    total_losses[ind] += L\n",
    "\n",
    "    if printing: # if this is a significant batch\n",
    "        avg_loss = total_losses[ind] / 100\n",
    "        total_losses[ind] = 0\n",
    "        print(f\"Average total loss for task {ind}, last 100 batches: {avg_loss}\\n\")\n",
    "        \n",
    "        if avg_loss < curr_mins[ind]:\n",
    "            curr_mins[ind] = avg_loss\n",
    "            torch.save(brain.state_dict(), f\"brain_checkpoints/enhanced_brain_arrow_task_v3_batch{b + 1}.pth\")\n",
    "            \n",
    "    if b < 10:\n",
    "        print(f\"batch {b}, task {ind}, task batch_num {batch_num}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb7c96-0ce6-4e96-99a6-0a2cddfd5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates: add sampling weights; add batch_num to print statement; split these two optimizers and optimize them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa482e-91be-4bae-ac3c-f75ae99e23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5913c4-87c5-4365-8376-3fddbef4efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_temps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82cb545-7d75-492b-84a2-7cd0f532f045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
