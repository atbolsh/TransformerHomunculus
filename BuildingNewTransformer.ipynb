{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4995639-4b98-4f52-8522-e413b7764213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer.custom_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e45ebe6-a3c8-4043-bdc5-d40b2a1175bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PatchEmbedding(768, 224, 14, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3066af2-48ea-4342-afde-aeedbac53aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = PE(torch.randn(2, 3, 224, 224).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82be2ba2-4cab-42a3-b27c-88818e7da7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e346c970-9381-4011-bd15-e4d0e3ea9d75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PatchEmbedding' object has no attribute 'pe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PE\u001b[38;5;241m.\u001b[39mpe\u001b[38;5;241m.\u001b[39mpe\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-florence/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PatchEmbedding' object has no attribute 'pe'"
     ]
    }
   ],
   "source": [
    "PE.pe.pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4c057d-61e3-42bf-a68d-fee1f1569afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PatchEmbedding' object has no attribute 'pe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PE\u001b[38;5;241m.\u001b[39mpe\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/llava-florence/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PatchEmbedding' object has no attribute 'pe'"
     ]
    }
   ],
   "source": [
    "PE.pe.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f8d5c57-eddb-4a18-bfe2-bcc2a23df0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = PositionalEncoding_2D(768, 16)\n",
    "pd.pe.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c0a12-f29e-4308-b04b-3b29ef719704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD. MOving on to the rest of this thing.\n",
    "# I checked the code for EncoderLayer and Encoder, and they *should* have worked, but didn't\n",
    "# Just in case this is something that's braindead-simple, I will rewrite the entire transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d2bf19-4a32-474e-bdec-f0688f829814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd84ede1-a569-403b-93d3-07708bb77908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function einops.einops.rearrange(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths) -> ~Tensor>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93cca930-a827-4911-83d3-348b5a0f5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 9 / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "520f7fb6-4b92-48af-861f-e2224135f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x^2 * y + (1 - y) * (1 - x)^2\n",
    "#2xy - 2(1 - y)*(1-x) = 0\n",
    "#xy = 1 - y - x + xy\n",
    "#1 - y - x = 0\n",
    "x = 1 - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45cee648-14ff-41b2-b9ff-de12ef71edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0339202880859375"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**2 * y + (1 - y) * (1 - x)*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c8c37c5-1721-415a-8b8e-f5e0e8ffa25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') #torch.device('cuda:1') # the big machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11bfb20c-4c05-4919-b1a8-e599e0906656",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DefaultAgentBrain().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5201914e-ae60-48a0-b56d-045d2588cdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.img_enc.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5b064-d215-495f-bbce-1109f0956290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda872b-da2e-417e-a03c-742d1e7051a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd016929-f86f-4fad-b61a-45b91b338703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.img_enc.load_state_dict(torch.load('brain_checkpoints/img_encoder_weights_DEBUG_MOVING.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f330dd28-8d38-4775-93cd-91fa30d7567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'layers': ModuleList(\n",
       "    (0-11): 12 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )},\n",
       " 'num_layers': 12,\n",
       " 'norm': None,\n",
       " 'enable_nested_tensor': True,\n",
       " 'use_nested_tensor': True,\n",
       " 'mask_check': True}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.img_enc.encoder.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ddcb089-5ebc-415f-af59-1dae8db5a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {'weight': Parameter containing:\n",
       "  tensor([[ 0.0200,  0.0164, -0.0175,  ..., -0.0136, -0.0154, -0.0214],\n",
       "          [ 0.0047,  0.0101, -0.0099,  ..., -0.0214,  0.0187,  0.0238],\n",
       "          [-0.0293,  0.0253,  0.0049,  ..., -0.0335,  0.0397,  0.0168],\n",
       "          ...,\n",
       "          [-0.0014, -0.0009,  0.0275,  ..., -0.0012, -0.0008,  0.0223],\n",
       "          [ 0.0331, -0.0405,  0.0010,  ..., -0.0073, -0.0167, -0.0290],\n",
       "          [ 0.0189,  0.0190,  0.0206,  ..., -0.0412, -0.0222,  0.0126]],\n",
       "         device='cuda:0', requires_grad=True),\n",
       "  'bias': Parameter containing:\n",
       "  tensor([ 0.0194, -0.0030, -0.0030,  ..., -0.0040,  0.0309, -0.0194],\n",
       "         device='cuda:0', requires_grad=True)},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {},\n",
       " 'in_features': 768,\n",
       " 'out_features': 2048}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.img_enc.encoder.layers[1].linear1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d702940-f138-46a2-81c3-d724252c263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, the layers are distinct after all. They are initialized to the same values, nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea15d5a9-7284-40c1-a60a-11601534e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH = MultiHead(768, 12).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e340acde-3549-4173-af4b-f8d93c71ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(4, 8, 768).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb33fb56-875f-4f6d-9ce0-666b923fc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = MH(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d17e5-425f-4794-a17f-67a2e1f96e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728e851-fb94-49cc-b9a6-dab4f622a6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d26c3-4e0f-4f5a-a23b-e4adcc4927f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30495d-7199-4257-81d7-0b72c60682bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46773bb-cf2a-4bb9-ab55-f6cd82b5df47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8c113-d3ee-464f-9e4e-ae965fc7192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8002b17-d275-4e66-bcd0-baec2a7c0b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
