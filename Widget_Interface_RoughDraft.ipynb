{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v1_batch33597.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v2_batch25999.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v3_batch19998.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_tutorialQA_v1_batch4665.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc83fba4620>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979bc2cd4cbd4dd2b8d457509a2a3a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ced2b256f147758d7099051e38b50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b45333cd4c479a95bf4bc54dd2e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337b029592fa4daf99b5f8daa4b107fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79473434da1e4bb1a6eeee1eca7c3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be33cbde6ac41679475f0fece8b500c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516db41e38643cb9b5e22def6e9cf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77c283ecd14b06ac1ab9f0f1ba3bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a4d39e1c6e47528588c51b2cf69a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509909f276664bffb25f26de14bf5e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8491f3-bdc7-4e05-abb9-d3fa4b830a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
