{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v1_batch33597.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v2_batch25999.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_canvases_v3_batch19998.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v3_batch2400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v3_batch8000.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5073ec5b80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b48c86f6548cb8c8f5e990f3b9201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babd3ad8a4304f589c4745793ceef7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c786d575bc224750acd242b0d0c4646f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94644fbf20e14b73b61146e33d24181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71a62f28326476c9bc1afd8fce720f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9cdee4b5064518b76157b656baea47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e642045296c4bc7be1ade3e5e54665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc92afb0234adcad04b6f6b720b73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ea5a4ab64c4951872053839db70323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Woah! What was that 3 images ago, again??', description='String:', placeholder='Type somethingâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d546dd2c15294f82bf8f2e63d8a9aa70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8491f3-bdc7-4e05-abb9-d3fa4b830a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
