{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f605b354290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d7a6b33f2c4cbe8309a14d4cf87c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be683f344eee45d0801f9a3a55e58d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240487ff46c941f8a1257e94b8913462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ed1f37f1204916865c1f788e2c02de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadee1be36144687934c82ebd11d7782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d7a5c0f0644de181379d9a6677ae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb8e5187c114bf0afe8730173976f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4291890ad36143a8b3cbf227c5684e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd5188baa6f4d0ab1550a79db7d3696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e263c26991432ab3ac322abcc5613d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ed1acbc-0080-4704-9720-f4240733e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.img_weight(torch.randn(8, 1024, 768, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee39b28-2d5e-4d18-97f2-e32acef0d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e59cb5e-b270-4a7d-bc3d-31977cf2a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9644e-01],\n",
       "         [3.4156e-03],\n",
       "         [1.0040e-04],\n",
       "         [4.6630e-05]],\n",
       "\n",
       "        [[9.9818e-01],\n",
       "         [1.7869e-03],\n",
       "         [1.5743e-05],\n",
       "         [1.2491e-05]],\n",
       "\n",
       "        [[9.9928e-01],\n",
       "         [6.6834e-04],\n",
       "         [3.5977e-05],\n",
       "         [1.2784e-05]],\n",
       "\n",
       "        [[9.9917e-01],\n",
       "         [7.9450e-04],\n",
       "         [2.2648e-05],\n",
       "         [1.6690e-05]],\n",
       "\n",
       "        [[9.9516e-01],\n",
       "         [4.6568e-03],\n",
       "         [1.3265e-04],\n",
       "         [5.1888e-05]],\n",
       "\n",
       "        [[9.9928e-01],\n",
       "         [6.6454e-04],\n",
       "         [2.2802e-05],\n",
       "         [2.7828e-05]],\n",
       "\n",
       "        [[9.9709e-01],\n",
       "         [2.8267e-03],\n",
       "         [4.6399e-05],\n",
       "         [3.2664e-05]],\n",
       "\n",
       "        [[9.9880e-01],\n",
       "         [1.0858e-03],\n",
       "         [5.2142e-05],\n",
       "         [6.3796e-05]]], device='cuda:1', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
