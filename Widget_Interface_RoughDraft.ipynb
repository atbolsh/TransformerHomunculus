{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f124f9f6870>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cd4cfecda2475ba957a257c2312337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f84be297454ffa84f61c0806da9281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86ed5b8c8b242f5a9ff16fe20ae9993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22672074f11c43cb8159dbfd2cf95c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420da77deae64eba9a93186d983d06cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3a819700f04fc7975e0514e99fd4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa700ce61ee4dc4a95770ec67166bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456d7c01bb5b4f4cae6956873bcc496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f79cd0ab214a6d86cfc1ba9994bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1535c36d934841639c1e83e6d7bd5bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ed1acbc-0080-4704-9720-f4240733e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.img_weight(torch.randn(8, 1024, 768, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee39b28-2d5e-4d18-97f2-e32acef0d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e59cb5e-b270-4a7d-bc3d-31977cf2a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9432e-01],\n",
       "         [5.6228e-03],\n",
       "         [3.8033e-05],\n",
       "         [1.9695e-05]],\n",
       "\n",
       "        [[9.9919e-01],\n",
       "         [7.7817e-04],\n",
       "         [1.6792e-05],\n",
       "         [1.0938e-05]],\n",
       "\n",
       "        [[9.9854e-01],\n",
       "         [1.4308e-03],\n",
       "         [1.5892e-05],\n",
       "         [9.0931e-06]],\n",
       "\n",
       "        [[9.9899e-01],\n",
       "         [9.6250e-04],\n",
       "         [3.6350e-05],\n",
       "         [1.5401e-05]],\n",
       "\n",
       "        [[9.9921e-01],\n",
       "         [7.5994e-04],\n",
       "         [1.7840e-05],\n",
       "         [1.0750e-05]],\n",
       "\n",
       "        [[9.9967e-01],\n",
       "         [2.8528e-04],\n",
       "         [2.8271e-05],\n",
       "         [1.5786e-05]],\n",
       "\n",
       "        [[9.9873e-01],\n",
       "         [1.2295e-03],\n",
       "         [2.1181e-05],\n",
       "         [1.9920e-05]],\n",
       "\n",
       "        [[9.9916e-01],\n",
       "         [8.0193e-04],\n",
       "         [2.0927e-05],\n",
       "         [1.4416e-05]]], device='cuda:1', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
