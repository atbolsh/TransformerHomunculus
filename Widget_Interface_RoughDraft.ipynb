{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_canvases_v1_batch33597.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f69b068d400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a972e00395415696e0b116bf04b01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b092dc4b1d45d78f5005b6f84b5c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b06deaaeb4ffda40e9534428a542f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b66093fdd2840d781d89727d52d50ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acdaa84980f402bb24f2f94741cd2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fa07f35e094febb13922cf7f9e43b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40df4ba148ce4064923a814869b1547b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56218e78b9c84acdb305df1c0087ae0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844266986793418b9f2b5a4667409ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bf9b069a434ea7876d02b28282eba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ed1acbc-0080-4704-9720-f4240733e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.img_weight(torch.randn(8, 1024, 768, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ee39b28-2d5e-4d18-97f2-e32acef0d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e59cb5e-b270-4a7d-bc3d-31977cf2a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.1552e-01],\n",
       "         [9.0332e-04],\n",
       "         [1.7566e-03],\n",
       "         [8.1818e-02]],\n",
       "\n",
       "        [[7.6054e-01],\n",
       "         [2.0806e-03],\n",
       "         [2.7113e-03],\n",
       "         [2.3467e-01]],\n",
       "\n",
       "        [[8.1494e-01],\n",
       "         [3.4287e-03],\n",
       "         [4.7371e-03],\n",
       "         [1.7690e-01]],\n",
       "\n",
       "        [[8.7918e-01],\n",
       "         [1.3425e-03],\n",
       "         [2.1982e-03],\n",
       "         [1.1728e-01]],\n",
       "\n",
       "        [[9.3100e-01],\n",
       "         [2.3404e-03],\n",
       "         [1.6377e-03],\n",
       "         [6.5022e-02]],\n",
       "\n",
       "        [[9.1832e-01],\n",
       "         [7.4185e-03],\n",
       "         [8.9850e-04],\n",
       "         [7.3362e-02]],\n",
       "\n",
       "        [[9.4387e-01],\n",
       "         [1.4033e-03],\n",
       "         [5.3150e-04],\n",
       "         [5.4197e-02]],\n",
       "\n",
       "        [[8.7231e-01],\n",
       "         [2.0032e-03],\n",
       "         [2.0629e-03],\n",
       "         [1.2362e-01]]], device='cuda:1', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8491f3-bdc7-4e05-abb9-d3fa4b830a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
