{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_canvases_v1_batch33597.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_canvases_v2_batch25999.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f61de783710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca74ddf34a88477ca92e94188789feb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be36b54e7a843b5bdbea1f64f9d4485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bac446633734a01979d9eac2feba089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79295d2fb3e41e19cdc98e324af1e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bc1371d26b43a2baeea94b71e37a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6bfc8862274f628c660491ba781536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9feddf67d248c9876f4b7e254ff9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df17dc3f7b943dcab50f1a7ddff7203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c4d3304674f32ba7675f185b99234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e2a7ff8cb946f684c8ba856ae4771e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8491f3-bdc7-4e05-abb9-d3fa4b830a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
