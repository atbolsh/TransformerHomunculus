{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "# -- that is the correct v3 version; the others come from the older tasks\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe78f36dd00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaba92650d94c1d84c02788bcedf019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a47bb5a70af490895aeea05f904feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d82df58e927412999ba3c264dee69f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216c6e7f88924ce897c329104c3b502b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f036bb97637949638bdfb645894ee0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df89fecb96764c16971387f3824c3a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa69204278f4ddab32fc3898f4033a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c70bc6b8ad4f74a59d63c605be6716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef703ae312649edacbc8378c903bdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c84d3f7ea94377a42fbb94df9624f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1acbc-0080-4704-9720-f4240733e8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee39b28-2d5e-4d18-97f2-e32acef0d9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59cb5e-b270-4a7d-bc3d-31977cf2a55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
