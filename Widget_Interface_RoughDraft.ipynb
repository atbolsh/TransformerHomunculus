{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e490d674-0492-4826-9e34-47944bb4e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-train.txt\n",
      "ðŸ”¥ text_pretraining_data/eng_sentences_pruned-eval.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FIRST 4 CELLS: specify frameworks, external imports, and the exact brain checkpoint used.\n",
    "# The other cells specify funcs for the widget, which is at the bottom\n",
    "# did you remember to change the 'device' in general_framework.py ?\n",
    "from tutorialQA_framework import *\n",
    "from tutorial1_framework import *\n",
    "from control_framework import *\n",
    "# add more here, or comment out\n",
    "\n",
    "device = torch.device('cuda:1') # let's use the alligator GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c995e3-9282-41fe-bcec-b779896818b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "from visual_transformer.enhanced_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e76133-fa47-472e-ba9f-1959fc6ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedAgentBrain()\n",
    "model.move_to(device) # special function that wraps 'to'. Dumb? yes. Needed? Also yes.\n",
    "#fname = 'brain_checkpoints/enhanced_brain_first_training_v2_batch160799.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_control_training_v2_batch55900.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v2_batch24800.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v3_batch155466.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v4_batch399.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_arrow_task_v5_batch133.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v2_batch29398.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_v3_batch35249.pth'\n",
    "#fname = 'brain_checkpoints/enhanced_brain_canvas_use_de_novo_v1_batch41200.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_v1_batch33400.pth'\n",
    "#fname = 'brain_checkpoints/frankenstein_transferred.pth'\n",
    "#fname = 'brain_checkpoints/super_brain_retraining_control_arrow_RESTART_v1_batch31799.pth'\n",
    "fname = 'brain_checkpoints/frankenstein_canvases_v1_batch33597.pth'\n",
    "\n",
    "# A little extra code to avoid weird error\n",
    "#model.memory.remember(torch.randn(16, 1, 768).to(device))\n",
    "#model.memory.remember(torch.randn(8, 1, 768).to(device))\n",
    "\n",
    "model.load_state_dict(torch.load(fname, weights_only=True, map_location=device))\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361e168-adf8-453b-ac4a-5825ef046c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other elements to use\n",
    "\n",
    "from game import *\n",
    "\n",
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings)\n",
    "\n",
    "####\n",
    "\n",
    "#For this time:\n",
    "def reset_G():\n",
    "    global G\n",
    "    G = discreteGame(G.random_bare_settings(gameSize=224, max_agent_offset=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b1ce00-688f-4a7b-aba0-4700d9341a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f241f74b110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reset_G()\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8492d-8c1e-4389-b871-31e9d30340b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis notebook is to show the EnhancedBrain input, output, and state of mind at once.\n",
    "# I'll start with a rough version and make it cleaner over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a6d7d3-7701-4ad6-aedf-9d1da7612150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f707fd-090f-42d3-92ed-b2ed1d87c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8946d87-c09e-4eba-897f-5f81c5211bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imshow_numpy(torch_img, imshow=False):\n",
    "    clean = torch_img.detach()[0].cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95dcd9-2119-47fb-a2b8-6713e5b28b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a55bbb-7aae-4acf-8232-baed2cc9f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EnhancedAgentBrain()\n",
    "#model.move_to(device)\n",
    "#2+2\n",
    "# commment to prevent output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dd7bcf-415e-4a8a-8078-7716f8ce17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "# tokenizer.save_model(\".\", \"tokenizer/eng_sentences_tokenizer_vc10000\")\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-vocab.json\",\n",
    "    \"./text_pretraining_tokenizer/eng_sentences_tokenizer_vc10000_v2-merges.txt\",\n",
    ")   \n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")   \n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b38a1d-479c-46d6-a3f7-b7e77181f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = widgets.Textarea(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ee3f89-d4b9-45f4-9e54-0ca7a6c40866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(game=None):\n",
    "    if game is None:\n",
    "        game = G\n",
    "    img = torch.FloatTensor(G.getData()).unsqueeze(0)\n",
    "    img = torch.permute(img, (0, 3, 1, 2)).contiguous().to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aed9588-a3c7-4fe3-8e5f-9fc29fe9a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a475188-3a0a-4aac-8a2e-36456c557cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this for other inputs\n",
    "# inp_tensor = get_image()\n",
    "inp_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab32506-2f60-4969-afdf-4fc35ccf8b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   32,   87,   34, 5411, 4226,    5,   32,   19,   87,   34,    2]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor(tokenizer.encode(\"<s>Hello World!</s>\").ids).unsqueeze(0).contiguous().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1930a98-8161-4c7a-b1c0-4dca5f2bf352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello World!</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tt[0][1:-1].cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88d78ff-8c78-4c99-88be-0a7340a40b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<s>Hello World!</s>\".find('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2da1651-dd24-4a73-a246-cbc3f5feb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def display_innards(b):\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    print(\"Canvases:\\n\")\n",
    "    if model.canvases.is_empty():\n",
    "        print(\"################\\nCanvases object is empty, nothing to show\\n################\")\n",
    "    else:\n",
    "        for i in range(model.canvases.num_canvases):\n",
    "            print(f\"##########\\nCanvas {i}:\\n\")\n",
    "            plt.imshow(pre_imshow_numpy(model.canvases.tw.L[i][:1]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b5eebc-b0a8-4252-bee8-eda5c4f1de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def forward_wrapper(b):\n",
    "    output.clear_output()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "    tt = torch.tensor(tokenizer.encode(T.value).ids).unsqueeze(0).contiguous().to(device)\n",
    "    _, recon = model(tt, local_tensor, create_context=True, ret_imgs=True)\n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b)\n",
    "\n",
    "# wrapper for extending input text without changing what is displayed, much\n",
    "\n",
    "temp=1.0\n",
    "temp_eps = 1e-4\n",
    "\n",
    "special_symbols = set([1, 3, 4, 108])\n",
    "symbol_action_map = { 1:1, 3:3, 4:4, 108:2}\n",
    "\n",
    "@output.capture()\n",
    "def extend_wrapper(b):\n",
    "    output.clear_output()\n",
    "    print(\"Game status:\\n\")\n",
    "    plt.imshow(G.getData())\n",
    "    plt.show()\n",
    "    if inp_tensor is None:\n",
    "        print(\"input tensor is None; using input from the game\\n\\n\")\n",
    "        local_tensor = get_image()\n",
    "    else:\n",
    "        print(\"using global variable inp_tensor as image input\\n\\n\")\n",
    "        local_tensor = inp_tensor\n",
    "\n",
    "    # in this case, we abridge the end-of-sentence token in order to continue the extension\n",
    "    inp_ids = tokenizer.encode(T.value).ids[:-1]\n",
    "    if len(inp_ids) > 32:\n",
    "        inp_ids = inp_ids[-32:] # the rest should be in memory\n",
    "    tt = torch.tensor(inp_ids).unsqueeze(0).contiguous().to(device)\n",
    "    logits, recon = model(tt, local_tensor, return_full=False, create_context=True, ret_imgs=True)\n",
    "\n",
    "    s = tt.size()\n",
    "    output_text = torch.zeros((s[0], s[1] +1), dtype = torch.long, device=device)\n",
    "    output_text[:, :-1] += tt\n",
    "    \n",
    "    preds = model.select(logits, temp, ret_all=False, temp_eps=temp_eps)\n",
    "    output_text[:, -1] += preds\n",
    "\n",
    "    predval = preds[0].item()\n",
    "    if predval in special_symbols:\n",
    "        action = symbol_action_map[predval]\n",
    "        print(f\"Detected special token {predval} which is action {action}\")\n",
    "        reward = G.actions[action]()\n",
    "        print(f\"Reward was {reward}\")\n",
    "    \n",
    "    T.value = tokenizer.decode(output_text[0][1:].cpu().numpy(), skip_special_tokens=False) # update the string, cut off start token\n",
    "    print(\"updated input string (also in text box):\\n\")\n",
    "    print(T.value)\n",
    "    \n",
    "    print(\"output image:\\n\")\n",
    "    plt.imshow(pre_imshow_numpy(recon[:1]))\n",
    "    plt.show()\n",
    "    display_innards(b) # including game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fcd589-cd91-488f-a7fd-74ca60b781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@output.capture()\n",
    "def soft_reset_wrapper(b):\n",
    "    print(\"soft reset (removing internal gradients)\\n\")\n",
    "    model.soft_reset()\n",
    "\n",
    "@output.capture()\n",
    "def reset_wrapper(b):\n",
    "    print(\"hard reset (clering memory and canvaases)\\n\")\n",
    "    model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7217f2f9-400e-49bc-ac6f-0852225ee038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32 # max len of the model input. Make this a text box or a selector or a knob.\n",
    "\n",
    "@output.capture()\n",
    "def generate_wrapper(b):\n",
    "    # while the string is not too long nor contains the stop codon\n",
    "    while (T.value.find('</s>') == -1) and (len(tokenizer.encode(T.value).ids[:-1]) < max_len):\n",
    "        extend_wrapper(b)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c779c1-427b-4e9f-8eeb-0c9ec08ac6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_clear_button_clicked(b):\n",
    "    output.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf60f2-d3b4-475c-b3d1-0ea1f2ea80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_G_wrapper(b):\n",
    "    reset_G()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6978ff-0314-4b5e-af6b-ad9a57deb475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedAgentBrain(\n",
       "  (img_enc): ImageTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): PatchEmbedding(\n",
       "        (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (1): PositionalEncoding_2D()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (img_dec): ImageTransformerDecoder(\n",
       "    (pe): PositionalEncoding_2D()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.01, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.01, inplace=False)\n",
       "          (dropout2): Dropout(p=0.01, inplace=False)\n",
       "          (dropout3): Dropout(p=0.01, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): PatchEmbeddingTranspose(\n",
       "        (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img_weight): VisionWeightedSum(\n",
       "    (pe): PositionalEncoding()\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (text_enc): SentenceTransformerEncoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(10000, 768, padding_idx=0)\n",
       "      (1): PositionalEncoding()\n",
       "      (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_dec): SentenceTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=3072, out_features=10000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dopamine): DopamineWrapper(\n",
       "    (dopamine): IntermediateTransformerScorer(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "        (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (memory): Memory()\n",
       "  (mem_enc): MemoryEncoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_prep): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fcs): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4256368e-b469-4e71-8f44-44339c70847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc7586d5b8644ea8c14cdfbe24bb928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset Game', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6015895fc64343e9a3d8088ca4543c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Forward', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e423df78587e425190b5941b76280495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Soft Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ec95f71b9f4216a6c379f8caf7196c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Reset', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d41a27f217349ff84f32fc51a799f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Extend by One Character', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c84255ace143d4a9c1d38e28800fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742f1c9aacef4c65b63dbbc3dbfb8489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Canvases', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec90716b65e4b988468d15b631bca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Output', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182f7bf98a24959b1c55451892f0702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello World', description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf014a8ab8f451783abc5e0ecb1d8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bGameReset = widgets.Button(description=\"Reset Game\")\n",
    "bForward = widgets.Button(description=\"Forward\")\n",
    "bSoftReset = widgets.Button(description=\"Soft Reset\")\n",
    "bReset = widgets.Button(description=\"Reset\")\n",
    "bExtend = widgets.Button(description=\"Extend by One Character\")\n",
    "bGenerate = widgets.Button(description=\"Generate Text\")\n",
    "bDisplayInnards = widgets.Button(description=\"Show Canvases\")\n",
    "bClear = widgets.Button(description=\"Clear Output\")\n",
    "\n",
    "display(bGameReset)\n",
    "display(bForward)\n",
    "display(bSoftReset)\n",
    "display(bReset)\n",
    "display(bExtend)\n",
    "display(bGenerate)\n",
    "display(bDisplayInnards)\n",
    "display(bClear)\n",
    "\n",
    "display(T) # expand me by hand; prettier that way\n",
    "\n",
    "bGameReset.on_click(reset_G_wrapper)\n",
    "bForward.on_click(forward_wrapper)\n",
    "bSoftReset.on_click(soft_reset_wrapper)\n",
    "bReset.on_click(reset_wrapper)\n",
    "bExtend.on_click(extend_wrapper)\n",
    "bGenerate.on_click(generate_wrapper)\n",
    "bDisplayInnards.on_click(display_innards)\n",
    "bClear.on_click(on_clear_button_clicked)\n",
    "\n",
    "display(output)\n",
    "display_innards(bDisplayInnards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ed1acbc-0080-4704-9720-f4240733e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.img_weight(torch.randn(8, 1024, 768, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee39b28-2d5e-4d18-97f2-e32acef0d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e59cb5e-b270-4a7d-bc3d-31977cf2a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.3126e-01],\n",
       "         [2.0689e-03],\n",
       "         [1.2505e-03],\n",
       "         [6.5424e-02]],\n",
       "\n",
       "        [[9.1072e-01],\n",
       "         [2.3120e-03],\n",
       "         [1.2019e-03],\n",
       "         [8.5767e-02]],\n",
       "\n",
       "        [[9.3401e-01],\n",
       "         [6.1866e-04],\n",
       "         [8.5893e-04],\n",
       "         [6.4513e-02]],\n",
       "\n",
       "        [[8.4890e-01],\n",
       "         [2.8055e-03],\n",
       "         [3.5197e-03],\n",
       "         [1.4478e-01]],\n",
       "\n",
       "        [[8.8454e-01],\n",
       "         [2.1526e-03],\n",
       "         [2.3878e-03],\n",
       "         [1.1092e-01]],\n",
       "\n",
       "        [[7.9057e-01],\n",
       "         [1.1125e-03],\n",
       "         [1.2258e-03],\n",
       "         [2.0709e-01]],\n",
       "\n",
       "        [[8.6077e-01],\n",
       "         [3.1369e-03],\n",
       "         [1.4055e-03],\n",
       "         [1.3469e-01]],\n",
       "\n",
       "        [[8.5306e-01],\n",
       "         [1.4300e-03],\n",
       "         [5.5902e-03],\n",
       "         [1.3992e-01]]], device='cuda:1', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513eec8-92db-4672-bdf4-9bde727c09a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8491f3-bdc7-4e05-abb9-d3fa4b830a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
