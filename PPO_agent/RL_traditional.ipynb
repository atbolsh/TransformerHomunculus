{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bdbd20-2df8-46c3-8500-c5f74ffeaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will only use a 'fake' brain for this one, only results are actions 0 - 4\n",
    "# This means I will start with an untrained text_dec and text_enc (though I will steal the img_enc from the old brain)\n",
    "# If this proves too hard, I will start with an easier task\n",
    "\n",
    "# This notebook will attempt actual PPO as a test study.\n",
    "# If I want this for the actual system, I may use a 'burn-in' phase which includes making the relevant actions have non-negligible probability,\n",
    "# and THEN launching the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04dcd88c-587c-4125-9ef4-c40cb39941b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db869825-5953-4bce-9dfa-e48f004a71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_settings = BIG_tool_use_advanced_2_5\n",
    "game_settings.gameSize = 224 # for compatibility with brain's expected size\n",
    "G = discreteGame(game_settings) # kind of a waste; will only call this object to generate random versions of itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a3dafb-614a-48bf-8c99-e188d28a581d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff27809f440>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(G.getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badc0e2d-1831-4d10-8b8f-e785a8701270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051aec13-5b41-47d2-8309-38aa36c66169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_transformer import *\n",
    "device = torch.device('cuda:0') # retrying on the 2080 after patching memory leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c144f17f-4ae9-488d-9778-bf15ec9d65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_display(torch_img):\n",
    "    clean = torch_img.detach().cpu()\n",
    "    right_order = torch.permute(clean, (1, 2, 0))\n",
    "    array = right_order.numpy()\n",
    "    plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb391465-6429-46a9-a79b-f252aba5e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, on to RL-specifics things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ff9d55-3cc1-4838-951a-b2fc26876c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 3: 3, 4: 4, 108: 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9150c21e-fed3-4841-9e2d-44ee984c050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis work for the full brain, but I am rewriting this for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a01b64-bd36-4d75-a3c9-c5c29e39a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 3, 4: 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_action_map = {1: 1, 2: 2, 3: 3, 4: 4}\n",
    "symbol_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca831ae8-3366-414d-8116-2821dd04be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I need to set up the brain itself\n",
    "old_brain = DefaultAgentBrain()\n",
    "old_brain.load_state_dict(torch.load('brain_checkpoints/brain_weights_tutorial1_v3_batch95000.pth', weights_only=True, map_location='cpu'))\n",
    "\n",
    "brain = DefaultAgentBrain(5)\n",
    "brain.img_enc = old_brain.img_enc\n",
    "brain.img_dec = old_brain.img_dec\n",
    "brain = brain.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a400a2b1-1dfb-40e1-aba2-7ed54568b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm starting to regret feeding the traces in at all\n",
    "# I should really just have a single 'prompt' like \"what is the best action here?\"\n",
    "# However, I don't feel like rewriting RL_helper just for this experiment, so this is what we are dealing with.\n",
    "# Hopefully PPO will still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1989aec-7459-4a2a-ac5d-862100324d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = GameOutputBuffer(brain, brain.evaluate_text, gamma=0.99, tau=0.97, default_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7077869c-4af8-485d-af11-d421049c61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70c35e56-8081-4ea5-998e-177530ecd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.fill(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b2269d-a6fb-45a8-870c-65504a7b3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30fb665-263d-4f05-8542-d0e5718150df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = buff.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f030f73-f8c3-4574-a432-46a87821b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530b7e7-ef3c-4285-a3ec-ef0ee902d4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd4c22-6024-4a73-9423-c93fef18cf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60190e2b-64dc-43ae-a9af-adc424f5be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb(num_buffers=64, batch_size=1):\n",
    "    bb = []\n",
    "    brain.eval()\n",
    "    for i in range(num_buffers):\n",
    "        #print(i)\n",
    "        # In this case, we are only training the 'dopamine' layer on the val training loop\n",
    "        buff = GameOutputBuffer(brain, brain.evaluate_text, gamma=0.99, tau=0.97, default_batch_size=batch_size)\n",
    "        buff.fill(G, num_games=batch_size)\n",
    "        buff.cpu()\n",
    "        bb.append(buff)\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0536c9f1-6b15-4009-81e3-576e6bd5be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = get_bb(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b066648-6d8a-42f7-8ca7-d0dd470ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce7dffba-e45d-4850-81a3-f68312d06fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only training the dopamine module here; img_enc is frozen, and hopefully it'll learn to largely ignore text_enc\n",
    "val_optimizer = optim.Adam(brain.dopamine.parameters(), lr=0.00001, eps=1e-9)\n",
    "val_epochs = 16 #16 # old is 80, but that's only sampling a few per turn; we're gonna go through the whole buffer-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9f9b784-2cd7-471b-a169-ca1867dcb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66e65b6-8ecf-4787-90f8-f5075aabebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e9ce6f-c3cc-4b45-9a65-0d078c00cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff = bb[0]\n",
    "#buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "710e1e26-15f3-4062-be28-d3575d6ba50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.to(device)\n",
    "#buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300ea774-9c4f-449b-8623-847a6a16f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7030e-2e0c-44dc-8320-534546ab2753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c923c05-d35b-4e6d-8e1c-ab7494b36794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cfaf00a-5b75-48fb-866f-54cb0c040f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_vals = buff.get_values(evaluation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9fc2443-a1c5-4b44-9040-c2eab408dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = mse_loss(new_vals, buff.returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a69ababa-7883-4223-8341-bf5612657144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a19ac016-ad5c-45bb-959b-9b6ef9c0fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a049f107-1484-4c60-aa43-e53817f40399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b37851e-cbce-4356-8084-5e1c625539cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "225d2999-c666-4962-912f-fda6a7be2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb[1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b484e94-5053-4ccf-a508-62826b306bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03bc5ca2-4ddb-4171-becf-b8e41d275042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb[1].returns.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad8b3cc7-8ed6-4a3f-a39c-125ee48121e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6456818580627441"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8416fcc7-20a5-44e2-ab9f-73847c06f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvidia_smi_spoof(device=device):\n",
    "    return torch.cuda.memory_allocated() / (1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5fd7889-cd61-4fd4-b050-39ad5fb83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_val_func(val_optimizer, epochs, buffer_buffer):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"==========Epoch {epoch}=====================\")\n",
    "        #print(nvidia_smi_spoof())\n",
    "        brain.train()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        train_loss = 0\n",
    "        random.shuffle(buffer_buffer)\n",
    "        i = 0\n",
    "        for buffer in buffer_buffer:\n",
    "            i += 1\n",
    "            #print(nvidia_smi_spoof())\n",
    "            buffer.to(device)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            val_optimizer.zero_grad()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            # call value func with dopamine gradients (and no others)\n",
    "            new_vals = buffer.get_values(evaluation = False, img_gradient = False, text_gradient = False)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss = mse_loss(new_vals, buffer.returns)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss.backward()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            val_optimizer.step()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            train_loss += loss.item()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            print(f\"episode {i}, val func loss {loss.item()}\\n\")\n",
    "            #print(nvidia_smi_spoof())\n",
    "            buffer.cpu()\n",
    "            #print(nvidia_smi_spoof())\n",
    "        val_optimizer.zero_grad()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        print(f\"Val func train loss in epoch {epoch}:{train_loss / (len(buffer_buffer))}\")\n",
    "#train_val_func(val_optimizer, val_epochs, buffer_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "854f5b40-eae7-42be-886a-c1c55e5f2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_func(val_optimizer, 1, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e81815f4-ed32-4c0b-9c04-e01e28ba8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brain.text_enc.embed[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f5e01bd-67e1-4642-90ee-94503af44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = bb[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60be3aca-754c-43de-b1cb-fa06723fbe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed_offset': 1,\n",
       " 'tau': 0.97,\n",
       " 'gamma': 0.99,\n",
       " 'default_batch_size': 1,\n",
       " 'contexts': None,\n",
       " 'logpas': tensor([[-1.3678, -1.5493, -2.5793, -1.1511]]),\n",
       " 'traces': tensor([[0, 1, 3, 4, 2]]),\n",
       " 'settings_buffer': [[<game.levels.skeleton.Settings at 0x7ff1b13916d0>,\n",
       "   <game.levels.skeleton.Settings at 0x7ff1b1391400>,\n",
       "   <game.levels.skeleton.Settings at 0x7ff1b1390b60>,\n",
       "   <game.levels.skeleton.Settings at 0x7ff1b1390830>,\n",
       "   <game.levels.skeleton.Settings at 0x7ff1b8d677d0>]],\n",
       " 'games': [<game.discreteEngine.discreteGame at 0x7ff1b1393dd0>],\n",
       " 'entropies': tensor([[1.5309, 1.5081, 1.5177, 1.5152]]),\n",
       " 'gaes': tensor([[2.1568, 1.9536, 2.2717, 1.0638]]),\n",
       " 'terminated': tensor([[False, False, False, False,  True]]),\n",
       " 'past_terminated': tensor([[False, False, False, False, False]]),\n",
       " 'values': tensor([[0.0000, 0.2836, 0.0562, 1.3196, 2.4075]]),\n",
       " 'rewards': tensor([[0., 0., 0., 0., 0.]]),\n",
       " 'returns': tensor([[0., 0., 0., 0., 0.]]),\n",
       " 'policy_model': DefaultAgentBrain(\n",
       "   (img_enc): ImageTransformerEncoder(\n",
       "     (embed): Sequential(\n",
       "       (0): PatchEmbedding(\n",
       "         (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "       )\n",
       "       (1): PositionalEncoding_2D()\n",
       "       (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (pe): PositionalEncoding_2D()\n",
       "     (encoder): TransformerEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x TransformerEncoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (img_dec): ImageTransformerDecoder(\n",
       "     (pe): PositionalEncoding_2D()\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-2): 3 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.01, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.01, inplace=False)\n",
       "           (dropout2): Dropout(p=0.01, inplace=False)\n",
       "           (dropout3): Dropout(p=0.01, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (linear_layer): Sequential(\n",
       "       (0): Dropout(p=0.1, inplace=False)\n",
       "       (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): PatchEmbeddingTranspose(\n",
       "         (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (text_enc): SentenceTransformerEncoder(\n",
       "     (embed): Sequential(\n",
       "       (0): Embedding(5, 768, padding_idx=0)\n",
       "       (1): PositionalEncoding()\n",
       "       (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): TransformerEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-7): 8 x TransformerEncoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (text_dec): SentenceTransformerDecoder(\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-3): 4 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "           (dropout3): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (fc): Sequential(\n",
       "       (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "       (4): Linear(in_features=3072, out_features=5, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (dopamine): IntermediateTransformerScorer(\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-3): 4 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "           (dropout3): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (fc): Sequential(\n",
       "       (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "       (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'value_model': <bound method DefaultAgentBrain.evaluate_text of DefaultAgentBrain(\n",
       "   (img_enc): ImageTransformerEncoder(\n",
       "     (embed): Sequential(\n",
       "       (0): PatchEmbedding(\n",
       "         (linear_project): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "       )\n",
       "       (1): PositionalEncoding_2D()\n",
       "       (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (pe): PositionalEncoding_2D()\n",
       "     (encoder): TransformerEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x TransformerEncoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (img_dec): ImageTransformerDecoder(\n",
       "     (pe): PositionalEncoding_2D()\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-2): 3 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.01, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.01, inplace=False)\n",
       "           (dropout2): Dropout(p=0.01, inplace=False)\n",
       "           (dropout3): Dropout(p=0.01, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (linear_layer): Sequential(\n",
       "       (0): Dropout(p=0.1, inplace=False)\n",
       "       (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): PatchEmbeddingTranspose(\n",
       "         (linear_project): ConvTranspose2d(768, 3, kernel_size=(14, 14), stride=(14, 14))\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (text_enc): SentenceTransformerEncoder(\n",
       "     (embed): Sequential(\n",
       "       (0): Embedding(5, 768, padding_idx=0)\n",
       "       (1): PositionalEncoding()\n",
       "       (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): TransformerEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-7): 8 x TransformerEncoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (text_dec): SentenceTransformerDecoder(\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-3): 4 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "           (dropout3): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (fc): Sequential(\n",
       "       (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "       (4): Linear(in_features=3072, out_features=5, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (dopamine): IntermediateTransformerScorer(\n",
       "     (decoder): TransformerDecoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-3): 4 x TransformerDecoderLayer(\n",
       "           (self_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (multihead_attn): MultiheadAttention(\n",
       "             (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "           (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout1): Dropout(p=0.1, inplace=False)\n",
       "           (dropout2): Dropout(p=0.1, inplace=False)\n",
       "           (dropout3): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (fc): Sequential(\n",
       "       (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "       (2): LeakyReLU(negative_slope=0.01)\n",
       "       (3): Dropout(p=0.1, inplace=False)\n",
       "       (4): Linear(in_features=3072, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " )>,\n",
       " 'reward_func': None,\n",
       " 'discounts': tensor([1.0000, 0.9897, 0.9795, 0.9694, 0.9594, 0.9495, 0.9397, 0.9300, 0.9204,\n",
       "         0.9109, 0.9015, 0.8923, 0.8831, 0.8739, 0.8649, 0.8560, 0.8472, 0.8385,\n",
       "         0.8298, 0.8213, 0.8128, 0.8044, 0.7961, 0.7879, 0.7798, 0.7717, 0.7638,\n",
       "         0.7559, 0.7481, 0.7404, 0.7328, 0.7252, 0.7177]),\n",
       " 'tau_discounts': tensor([1.0000, 0.9591, 0.9198, 0.8822, 0.8461, 0.8115, 0.7783, 0.7464, 0.7159,\n",
       "         0.6866, 0.6585, 0.6316, 0.6057, 0.5810, 0.5572, 0.5344, 0.5125, 0.4916,\n",
       "         0.4714, 0.4522, 0.4337, 0.4159, 0.3989, 0.3826, 0.3669, 0.3519, 0.3375,\n",
       "         0.3237, 0.3105, 0.2978, 0.2856, 0.2739, 0.2627]),\n",
       " 'tensor_keys': ['contexts',\n",
       "  'logpas',\n",
       "  'traces',\n",
       "  'entropies',\n",
       "  'gaes',\n",
       "  'terminated',\n",
       "  'past_terminated',\n",
       "  'values',\n",
       "  'rewards',\n",
       "  'returns',\n",
       "  'discounts',\n",
       "  'tau_discounts']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0e0794a-a18f-46a0-9f6a-54e3268638c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts\n",
      "\n",
      "\n",
      "\n",
      "logpas\n",
      "tensor([[-1.3678, -1.5493, -2.5793, -1.1511]])\n",
      "\n",
      "\n",
      "\n",
      "traces\n",
      "tensor([[0, 1, 3, 4, 2]])\n",
      "\n",
      "\n",
      "\n",
      "entropies\n",
      "tensor([[1.5309, 1.5081, 1.5177, 1.5152]])\n",
      "\n",
      "\n",
      "\n",
      "gaes\n",
      "tensor([[2.1568, 1.9536, 2.2717, 1.0638]])\n",
      "\n",
      "\n",
      "\n",
      "terminated\n",
      "tensor([[False, False, False, False,  True]])\n",
      "\n",
      "\n",
      "\n",
      "past_terminated\n",
      "tensor([[False, False, False, False, False]])\n",
      "\n",
      "\n",
      "\n",
      "values\n",
      "tensor([[0.0000, 0.2836, 0.0562, 1.3196, 2.4075]])\n",
      "\n",
      "\n",
      "\n",
      "rewards\n",
      "tensor([[0., 0., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "\n",
      "returns\n",
      "tensor([[0., 0., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "\n",
      "discounts\n",
      "tensor([1.0000, 0.9897, 0.9795, 0.9694, 0.9594, 0.9495, 0.9397, 0.9300, 0.9204,\n",
      "        0.9109, 0.9015, 0.8923, 0.8831, 0.8739, 0.8649, 0.8560, 0.8472, 0.8385,\n",
      "        0.8298, 0.8213, 0.8128, 0.8044, 0.7961, 0.7879, 0.7798, 0.7717, 0.7638,\n",
      "        0.7559, 0.7481, 0.7404, 0.7328, 0.7252, 0.7177])\n",
      "\n",
      "\n",
      "\n",
      "tau_discounts\n",
      "tensor([1.0000, 0.9591, 0.9198, 0.8822, 0.8461, 0.8115, 0.7783, 0.7464, 0.7159,\n",
      "        0.6866, 0.6585, 0.6316, 0.6057, 0.5810, 0.5572, 0.5344, 0.5125, 0.4916,\n",
      "        0.4714, 0.4522, 0.4337, 0.4159, 0.3989, 0.3826, 0.3669, 0.3519, 0.3375,\n",
      "        0.3237, 0.3105, 0.2978, 0.2856, 0.2739, 0.2627])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in buff.tensor_keys:\n",
    "    print(key)\n",
    "    if buff.__dict__[key] is not None:\n",
    "        print(buff.__dict__[key])\n",
    "        if buff.__dict__[key].grad is not None:\n",
    "            print(\"found a suspect\")\n",
    "            print(buff.__dict__[key].grad)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c3f9b32-4c0e-458b-a903-736d24cf8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.dopamine.fc[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04b44217-7e5e-4fc6-a4e7-03aca41c79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.text_enc.embed[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db64755c-2601-4911-b466-a4e9be6b5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.text_dec.fc[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d021df7-4309-44f5-97df-89f55a65c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok. I think there's some noise in the values returned by nvidia-smi and the cuda func (slow deallocation?)\n",
    "# but it really looks like I squashed both sources of memory leaks.\n",
    "# Let's try this again, but with big-boy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05e7ea1d-b436-409a-bcda-7846d97c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recalculate gaes after burn-in, to get any progress on the policy network at all\n",
    "def restore_coherence(buffer_buffer):\n",
    "    for buffer in buffer_buffer:\n",
    "        buffer=buffer.cuda()\n",
    "        buffer.values = buffer.get_values() # retrained val func\n",
    "        buffer.gaes = buffer.get_gaes()\n",
    "        buffer = buffer.cpu()\n",
    "    return buffer_buffer\n",
    "#buffer_buffer = restore_coherence(buffer_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d007dd1f-fb0f-4e73-a4c1-7a4cb4f61e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(list(brain.text_enc.parameters()) + list(brain.text_dec.parameters()), lr=0.00001, eps=1e-9)\n",
    "policy_epochs = 4\n",
    "epochs = policy_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f45ac1a-261b-4980-b4a7-030345cbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(policy_optimizer, epochs, buffer_buffer, policy_clip_range=0.1, entropy_loss_weight=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"==========Epoch {epoch}=====================\")\n",
    "        #print(nvidia_smi_spoof())\n",
    "        brain.train()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        train_loss = 0\n",
    "        random.shuffle(buffer_buffer)\n",
    "        i = 0\n",
    "        for buffer in buffer_buffer:\n",
    "            i += 1\n",
    "            buffer.to(device)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_optimizer.zero_grad()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            logpas, entropies = buffer.get_probabilities_and_entropies(evaluation=False)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            ratios = (logpas - buffer.logpas).exp()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            pi_obj = buffer.gaes * ratios\n",
    "            #print(nvidia_smi_spoof())\n",
    "            pi_obj_clipped = buffer.gaes * ratios.clamp(1.0 - policy_clip_range,\n",
    "                                                       1.0 + policy_clip_range)\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_loss = -torch.min(pi_obj, pi_obj_clipped).mean()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            entropy_loss = -entropies.mean() * entropy_loss_weight\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss = policy_loss + entropy_loss\n",
    "            #print(nvidia_smi_spoof())\n",
    "            loss.backward()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            policy_optimizer.step()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            train_loss += loss.item()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            buffer.cpu()\n",
    "            #print(nvidia_smi_spoof())\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"episode {i}, policy loss {loss.item()}\\n\")\n",
    "        del loss, logpas, entropies, ratios, pi_obj, pi_obj_clipped, policy_loss, entropy_loss\n",
    "        policy_optimizer.zero_grad()\n",
    "        #print(nvidia_smi_spoof())\n",
    "        print(f\"Policy train loss in epoch {epoch}:{train_loss / (len(buffer_buffer))}\")\n",
    "#train_policy(policy_optimizer, policy_epochs, buffer_buffer, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f46f9c5a-9bc3-4de3-bd4b-fe18cd667995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60eb9fa9-46f1-4782-93ff-eda1a59aa32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_policy(policy_optimizer, 1, bb[:4])#, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "497e45a3-0370-4279-a3f5-d36eaf431e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_policy(policy_optimizer, 2, bb[4:8])#, policy_clip_range, entropy_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "812b0e2b-c32b-4605-810c-16acea4b68bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6456818580627441"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy_optimizer.zero_grad()\n",
    "#torch.cuda.empty_cache()\n",
    "nvidia_smi_spoof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95dfe71b-cc9e-43dc-b11d-4491b7c39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for b in bb:\n",
    "    #print(b.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293b064-6de3-4be7-a1db-074ae3c393de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb5806e5-ba1e-4d9e-acf6-3d7fbec2aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_return(bb):\n",
    "    \"\"\"The average return (at the end of the seeds alone) from a buffer-buffer\"\"\"\n",
    "    s = torch.zeros(bb[0].returns[:, 0].size(), device = bb[0].returns[:, 0].device)\n",
    "    for b in bb:\n",
    "        s += bb[0].returns[:, 0]\n",
    "    return torch.sum(s).item()/(len(bb) * bb[0].returns.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "700176bf-191e-4502-b15c-73a125c33eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no policy optimization on first round, only subsequent\n",
    "def run_round(round_num, policy_optimizer, val_optimizer, num_buffers=64, batch_size=6, policy_epochs=4, val_epochs=16, policy_clip_range=0.5, entropy_loss_weight=1e-3):\n",
    "    # First, get some samples\n",
    "    brain.eval()\n",
    "#    get_value.eval()\n",
    "    buffer_buffer = get_bb(num_buffers, batch_size) # run the inference side\n",
    "    print(f\"Return before training was {average_return(buffer_buffer)}\")\n",
    "    if round_num > 0:\n",
    "        print(\"\\n~~~~~~~POLICY loop~~~~~~~\\n\")\n",
    "        train_policy(policy_optimizer, policy_epochs, buffer_buffer, policy_clip_range, entropy_loss_weight)\n",
    "    print(\"\\n~~~~~~~~VALUE loop~~~~~~~~~~~\\n\")\n",
    "    train_val_func(val_optimizer, val_epochs, buffer_buffer)\n",
    "    del buffer_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc6d12-3ee2-460d-ab49-19b853173d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************ROUND 0 ***************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "policy_epochs=4\n",
    "val_epochs=16\n",
    "num_buffers=8 # keep it simpler\n",
    "batch_size=12\n",
    "num_rounds = 150*10 # give it more of a chance to learn policy, which can only change a little over each round.\n",
    "policy_clip_range=0.5\n",
    "entropy_loss_weight=5e-3\n",
    "for i in range(num_rounds):\n",
    "    start = time.time()\n",
    "    print(f\"**********************ROUND {i} ***************************\\n\")\n",
    "    run_round(i, policy_optimizer, val_optimizer, num_buffers, batch_size, policy_epochs, val_epochs, policy_clip_range, entropy_loss_weight)\n",
    "    torch.save(brain.state_dict(), f'brain_checkpoints/brain_EXPERIMENTAL_5output_weights_RL_v4_round{i + 1}.pth')\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"***********************TIME WAS {elapsed / 60} min*****************************\\n\")\n",
    "    # I think the entropy was too low last time, let's see if this fixes the issue.\n",
    "    if i > 40:\n",
    "        entropy_loss_weight = max(entropy_loss_weight / 2, 1e-4)#1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f507f-c1c1-442d-94e2-d31681c7c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having the '2' value available at all for the trace is a problem. It's cutting these short\n",
    "# Also, it needs more steps to find the gold.\n",
    "# THe next reasonable task is to fix these two issues, then rerun this training sesssion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
